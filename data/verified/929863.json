{
  "id": "929863",
  "title": "LLM에 대해서는 이렇게 생각하면 편함",
  "category": "",
  "author": "ㅇㅇ",
  "date": "2026.01.10 04:11:56",
  "views": 0,
  "likes": 0,
  "comments": 0,
  "content": "\n\t\t\t\t\t\t\t<p>네 무의식의 확장처럼 생각하셈</p><p><br></p><p>사람이 어떤 생각을 떠올릴 때 1) 먼저 생각이 떠오르고, 2) 그 뒤에 그게 정말 맞는 생각인지 검증하잖음</p><p><br></p><p>즉 네가 어떤 생각을 떠올리는지 자체는 네가 컨트롤할 수 있는 부분이 아니라는 거임. 마치 아침에 저절로 눈이 떠지는 것처럼 너의 존재 방식의 일부일 뿐임.</p><p><br></p><p>이걸 확률적 프로세스라고 부르고 말고는 얘기가 좀 딥해지니깐 이건 넘어가고</p><p><br></p><p>문제는 그렇다면 네 사고의 범위에는 한계가 있다는 거고, 이때 LLM이 유용하게 쓰일 수 있는 거지</p><p><br></p><p>LLM이 내뱉는 텍스트를 네 특정 사고 패턴을 활성화하는 자극처럼 생각해 보셈. \"전지전능한 LLM이 내게 진리를 알려주는구나\"와 같이 LLM을 타자화하지 말고, LLM이 내뱉은 말이 방금 네가 너 스스로 떠올린 아이디어라고 생각해 보는 거임.</p><p><br></p><p>그럼 LLM이 네 말을 반박하는 건? 너 혼자 '아니 그건 아니지 않나?' 하고 의심하는 거.</p><p><br></p><p>LLM이 네 말을 긍정하는 건? 너 혼자 '이거 좀 말되는 것 같은데?' 하고 몰두한 것.</p><p><br></p><p>LLM이 너 보고 상위 0.1%라고 하는 건? 너 스스로 '캬 난 역시 상위 0.1%야' 하고 있는 꼴.</p><p><br></p><p>이런 식으로 생각하면 LLM을 어떻게 활용해야 할지도 명확해질 거임.</p><p><br></p><p>물론 LLM이 겨우 이것보다는 좀 더 유용하긴 함. LLM은 어쨌든 인터넷 텍스트와 강화학습을 바탕으로 학습한 물건이고, 기본적으로 너보다 아는 게 많음.</p><p><br></p><p>문제는 그 LLM의 지식이라는 게 두루뭉술한 확률 분포로 존재할 뿐 원본 데이터를 있는 그대로 반영하는 게 아니며, 애초에 그 원본 데이터조차도 100% 신뢰할 수 없다는 거. 그걸 전제로 하고 LLM의 말을 받아들일지 말지 판단해야 함. 이 역시 '내가 이 분야에 대해 제대로 알고 있는 게 맞나?' 하고 자기검증하는 과정과 유사함</p>\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t",
  "contentText": "네 무의식의 확장처럼 생각하셈사람이 어떤 생각을 떠올릴 때 1) 먼저 생각이 떠오르고, 2) 그 뒤에 그게 정말 맞는 생각인지 검증하잖음즉 네가 어떤 생각을 떠올리는지 자체는 네가 컨트롤할 수 있는 부분이 아니라는 거임. 마치 아침에 저절로 눈이 떠지는 것처럼 너의 존재 방식의 일부일 뿐임.이걸 확률적 프로세스라고 부르고 말고는 얘기가 좀 딥해지니깐 이건 넘어가고문제는 그렇다면 네 사고의 범위에는 한계가 있다는 거고, 이때 LLM이 유용하게 쓰일 수 있는 거지LLM이 내뱉는 텍스트를 네 특정 사고 패턴을 활성화하는 자극처럼 생각해 보셈. \"전지전능한 LLM이 내게 진리를 알려주는구나\"와 같이 LLM을 타자화하지 말고, LLM이 내뱉은 말이 방금 네가 너 스스로 떠올린 아이디어라고 생각해 보는 거임.그럼 LLM이 네 말을 반박하는 건? 너 혼자 '아니 그건 아니지 않나?' 하고 의심하는 거.LLM이 네 말을 긍정하는 건? 너 혼자 '이거 좀 말되는 것 같은데?' 하고 몰두한 것.LLM이 너 보고 상위 0.1%라고 하는 건? 너 스스로 '캬 난 역시 상위 0.1%야' 하고 있는 꼴.이런 식으로 생각하면 LLM을 어떻게 활용해야 할지도 명확해질 거임.물론 LLM이 겨우 이것보다는 좀 더 유용하긴 함. LLM은 어쨌든 인터넷 텍스트와 강화학습을 바탕으로 학습한 물건이고, 기본적으로 너보다 아는 게 많음.문제는 그 LLM의 지식이라는 게 두루뭉술한 확률 분포로 존재할 뿐 원본 데이터를 있는 그대로 반영하는 게 아니며, 애초에 그 원본 데이터조차도 100% 신뢰할 수 없다는 거. 그걸 전제로 하고 LLM의 말을 받아들일지 말지 판단해야 함. 이 역시 '내가 이 분야에 대해 제대로 알고 있는 게 맞나?' 하고 자기검증하는 과정과 유사함",
  "images": [],
  "url": "https://gall.dcinside.com/mgallery/board/view/?id=thesingularity&no=929863",
  "crawledAt": "2026-01-09T19:45:09.133Z",
  "verification": {
    "postId": "929863",
    "verifiedAt": "2026-01-09T21:44:55.484Z",
    "systemMode": "online",
    "header": "[🟢 Online Mode | 26.01.10_06:44:14]",
    "claims": [
      {
        "id": "claim_1",
        "text": "LLM은 어쨌든 인터넷 텍스트와 강화학습을 바탕으로 학습한 물건이고",
        "type": "technical_spec",
        "entities": [
          "LLM"
        ],
        "verified": true,
        "confidence": 1,
        "notes": "해당 주장은 기술적으로 매우 정확함. 주요 LLM들은 Common Crawl, Wikipedia, Stack Overflow 등 인터넷상의 방대한 텍스트를 학습하며, 모델의 유용성과 안전성을 높이기 위해 PPO(Proximal Policy Optimization)와 같은 강화학습 알고리즘을 사용한 RLHF 과정을 거침. (현대 LLM(GPT, Llama, Claude 등)의 개발 프로세스는 '대규모 인터넷 데이터셋을 활용한 사전 학습(Pre-training)'과 '인간 피드백 기반 강화학습(RLHF)'을 핵심 축으로 삼고 있음이 학술 논문 및 기술 명세서를 통해 공인됨.)",
        "sources": [
          {
            "url": "https://arxiv.org/abs/2203.02155",
            "title": "Training language models to follow instructions with human feedback (InstructGPT)",
            "tier": "S",
            "domain": "arxiv.org",
            "icon": "🏛️",
            "publishDate": "2022-03-04"
          },
          {
            "url": "https://arxiv.org/abs/2005.14165",
            "title": "Language Models are Few-Shot Learners (GPT-3)",
            "tier": "S",
            "domain": "arxiv.org",
            "icon": "🏛️",
            "publishDate": "2020-05-22"
          },
          {
            "url": "https://ai.meta.com/blog/meta-llama-3/",
            "title": "Introducing Meta Llama 3: The most capable openly available LLM to date",
            "tier": "A",
            "domain": "ai.meta.com",
            "icon": "🛡️",
            "publishDate": "2024-04-18"
          }
        ],
        "strategy": {
          "keywords": [
            "LLM",
            "LLM training methods internet text reinforcement learning",
            "LLM RLHF pre-training process"
          ],
          "focus": "technical_spec",
          "academicRequired": true,
          "domainFilters": [
            "site:arxiv.org",
            "site:openai.com",
            "site:anthropic.com",
            "site:huggingface.co"
          ]
        }
      },
      {
        "id": "claim_2",
        "text": "LLM의 지식이라는 게 두루뭉술한 확률 분포로 존재할 뿐 원본 데이터를 있는 그대로 반영하는 게 아니며",
        "type": "technical_spec",
        "entities": [
          "LLM"
        ],
        "verified": true,
        "confidence": 1,
        "notes": "LLM은 원본 데이터를 데이터베이스처럼 저장하는 것이 아니라, 학습 과정에서 데이터의 패턴을 신경망 가중치(Weights)에 '손실 압축(Lossy Compression)' 형태로 내재화합니다. 이를 '파라미터적 지식(Parametric Knowledge)'이라 하며, 추론 시에는 이 가중치를 기반으로 다음에 올 토큰의 확률 분포를 계산하여 텍스트를 생성합니다. 따라서 특정 데이터를 있는 그대로 복사해오는 것이 아니라 확률적으로 재구성하는 방식이므로 주장은 기술적으로 정확합니다. (LLM의 기본 아키텍처인 Transformer와 학습 목적 함수(Cross-Entropy Loss)는 데이터를 직접 저장하는 것이 아니라 데이터 간의 통계적 확률 분포를 학습하도록 설계되어 있음.)",
        "sources": [
          {
            "url": "https://arxiv.org/abs/1706.03762",
            "title": "Attention Is All You Need",
            "tier": "S",
            "domain": "arxiv.org",
            "icon": "🏛️",
            "publishDate": "2017-06-12"
          },
          {
            "url": "https://arxiv.org/abs/2001.08361",
            "title": "Scaling Laws for Neural Language Models",
            "tier": "S",
            "domain": "arxiv.org",
            "icon": "🏛️",
            "publishDate": "2020-01-23"
          },
          {
            "url": "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf",
            "title": "A Neural Probabilistic Language Model",
            "tier": "C",
            "domain": "www.jmlr.org",
            "icon": "",
            "publishDate": "2003-03-01"
          }
        ],
        "strategy": {
          "keywords": [
            "LLM",
            "how LLMs store information probability distribution",
            "LLM knowledge representation vs raw data storage"
          ],
          "focus": "technical_spec",
          "academicRequired": true,
          "domainFilters": [
            "site:arxiv.org",
            "site:openai.com",
            "site:anthropic.com",
            "site:huggingface.co"
          ]
        }
      },
      {
        "id": "claim_3",
        "text": "애초에 그 원본 데이터조차도 100% 신뢰할 수 없다는 거",
        "type": "research",
        "entities": [
          "LLM training data"
        ],
        "verified": true,
        "confidence": 1,
        "notes": "주장은 기술적으로 매우 정확함. LLM의 주 학습원인 인터넷 텍스트는 증오 표현, 편향된 정보, 사실적 오류, 기계 생성 텍스트(Spam)를 다량 포함하고 있음. 이를 해결하기 위해 연구자들은 휴리스틱 필터링, 분류기 기반 정제 등 복잡한 전처리 과정을 거치지만, 그럼에도 불구하고 원본 데이터의 100% 신뢰성을 확보하는 것은 불가능하다는 것이 학계의 공통된 견해임. (LLM 학습의 근간이 되는 웹 스케일 데이터셋(Common Crawl, C4 등)의 저품질, 노이즈, 허위 정보 포함 문제는 수많은 AI 학술 논문 및 기술 보고서를 통해 입증된 사실임.)",
        "sources": [
          {
            "url": "https://arxiv.org/abs/2104.08758",
            "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus",
            "tier": "S",
            "domain": "arxiv.org",
            "icon": "🏛️",
            "publishDate": "2021-04-19"
          },
          {
            "url": "https://arxiv.org/abs/2105.02732",
            "title": "What's in the Box? A Preliminary Analysis of Undesirable Content in the Common Crawl Data",
            "tier": "S",
            "domain": "arxiv.org",
            "icon": "🏛️",
            "publishDate": "2021-05-06"
          },
          {
            "url": "https://arxiv.org/abs/2101.00027",
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
            "tier": "S",
            "domain": "arxiv.org",
            "icon": "🏛️",
            "publishDate": "2020-12-31"
          }
        ],
        "strategy": {
          "keywords": [
            "LLM training data",
            "reliability of internet text for LLM training",
            "quality issues in LLM training datasets like Common Crawl"
          ],
          "focus": "research",
          "academicRequired": true,
          "domainFilters": [
            "site:arxiv.org",
            "site:openai.com",
            "site:anthropic.com",
            "site:huggingface.co"
          ]
        }
      }
    ],
    "summary": {
      "totalClaims": 3,
      "verifiedClaims": 3,
      "overallScore": 1,
      "sourceTierDistribution": {
        "S": 7,
        "A": 1,
        "B": 0,
        "C": 1
      }
    },
    "allSources": [
      {
        "url": "https://arxiv.org/abs/2203.02155",
        "title": "Training language models to follow instructions with human feedback (InstructGPT)",
        "tier": "S",
        "domain": "arxiv.org",
        "icon": "🏛️",
        "publishDate": "2022-03-04"
      },
      {
        "url": "https://arxiv.org/abs/2005.14165",
        "title": "Language Models are Few-Shot Learners (GPT-3)",
        "tier": "S",
        "domain": "arxiv.org",
        "icon": "🏛️",
        "publishDate": "2020-05-22"
      },
      {
        "url": "https://ai.meta.com/blog/meta-llama-3/",
        "title": "Introducing Meta Llama 3: The most capable openly available LLM to date",
        "tier": "A",
        "domain": "ai.meta.com",
        "icon": "🛡️",
        "publishDate": "2024-04-18"
      },
      {
        "url": "https://arxiv.org/abs/1706.03762",
        "title": "Attention Is All You Need",
        "tier": "S",
        "domain": "arxiv.org",
        "icon": "🏛️",
        "publishDate": "2017-06-12"
      },
      {
        "url": "https://arxiv.org/abs/2001.08361",
        "title": "Scaling Laws for Neural Language Models",
        "tier": "S",
        "domain": "arxiv.org",
        "icon": "🏛️",
        "publishDate": "2020-01-23"
      },
      {
        "url": "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf",
        "title": "A Neural Probabilistic Language Model",
        "tier": "C",
        "domain": "www.jmlr.org",
        "icon": "",
        "publishDate": "2003-03-01"
      },
      {
        "url": "https://arxiv.org/abs/2104.08758",
        "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus",
        "tier": "S",
        "domain": "arxiv.org",
        "icon": "🏛️",
        "publishDate": "2021-04-19"
      },
      {
        "url": "https://arxiv.org/abs/2105.02732",
        "title": "What's in the Box? A Preliminary Analysis of Undesirable Content in the Common Crawl Data",
        "tier": "S",
        "domain": "arxiv.org",
        "icon": "🏛️",
        "publishDate": "2021-05-06"
      },
      {
        "url": "https://arxiv.org/abs/2101.00027",
        "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
        "tier": "S",
        "domain": "arxiv.org",
        "icon": "🏛️",
        "publishDate": "2020-12-31"
      }
    ],
    "recommendation": "publish",
    "verificationSummary": "## 검증 요약\n- 총 주장: 3개\n- 검증 완료: 3개\n- 전체 점수: 100%\n\n## 출처 신뢰도 분포\n- 🏛️ Tier S (학술): 7개\n- 🛡️ Tier A (공식): 1개\n- ⚠️ Tier B (주의): 0개\n- Tier C (일반): 1개"
  },
  "translation": {
    "title_en": "A Useful Mental Model for Understanding LLMs",
    "title_ko": "LLM에 대해서는 이렇게 생각하면 편함",
    "content_en": "Think of a Large Language Model (LLM) as an extension of your own unconscious mind. When a human has a thought, the process generally follows two stages: 1) the thought emerges spontaneously, and 2) you subsequently verify whether that thought is actually correct. In other words, you do not have direct control over which specific thoughts surface in your mind. It is simply a part of your mode of existence, much like waking up automatically in the morning. Whether this should be defined as a \"probabilistic process\" is a complex topic, so we will set that aside for now.\n\nThe issue is that your inherent range of thought is limited, and this is where an LLM becomes highly useful. You should view the text generated by an LLM as a stimulus that activates specific patterns in your own thinking. Rather than \"othering\" the LLM—viewing it as an omnipotent entity revealing the truth—try treating its output as if it were an idea you just conceived yourself.\n\nFrom this perspective, how should you interpret the interaction? \n- If the LLM contradicts you, view it as your own internal skepticism: \"Wait, is that not right?\"\n- If the LLM agrees with you, view it as you becoming absorbed in a plausible idea: \"This seems to make sense.\"\n- If the LLM tells you that you are in the top 0.1%, view it as your own self-congratulation: \"Wow, I really am in the top 0.1%.\" (Note: The original text uses the Korean exclamation \"캬,\" which expresses intense self-satisfaction or admiration).\n\nApproaching it this way clarifies exactly how to utilize LLMs. Of course, LLMs are slightly more capable than this simple metaphor suggests. They are built upon vast internet text and Reinforcement Learning (RL), and they generally possess more information than any single individual.\n\nThe problem, however, is that this knowledge exists only as a vague probability distribution rather than a perfect reflection of original data. Furthermore, the source data itself is not 100% reliable. You must judge whether to accept the LLM's output based on these premises. This process is strikingly similar to the act of self-verification: \"Do I actually have a proper understanding of this field?\"",
    "content_ko": "네 무의식의 확장처럼 생각하셈사람이 어떤 생각을 떠올릴 때 1) 먼저 생각이 떠오르고, 2) 그 뒤에 그게 정말 맞는 생각인지 검증하잖음즉 네가 어떤 생각을 떠올리는지 자체는 네가 컨트롤할 수 있는 부분이 아니라는 거임. 마치 아침에 저절로 눈이 떠지는 것처럼 너의 존재 방식의 일부일 뿐임.이걸 확률적 프로세스라고 부르고 말고는 얘기가 좀 딥해지니깐 이건 넘어가고문제는 그렇다면 네 사고의 범위에는 한계가 있다는 거고, 이때 LLM이 유용하게 쓰일 수 있는 거지LLM이 내뱉는 텍스트를 네 특정 사고 패턴을 활성화하는 자극처럼 생각해 보셈. \"전지전능한 LLM이 내게 진리를 알려주는구나\"와 같이 LLM을 타자화하지 말고, LLM이 내뱉은 말이 방금 네가 너 스스로 떠올린 아이디어라고 생각해 보는 거임.그럼 LLM이 네 말을 반박하는 건? 너 혼자 '아니 그건 아니지 않나?' 하고 의심하는 거.LLM이 네 말을 긍정하는 건? 너 혼자 '이거 좀 말되는 것 같은데?' 하고 몰두한 것.LLM이 너 보고 상위 0.1%라고 하는 건? 너 스스로 '캬 난 역시 상위 0.1%야' 하고 있는 꼴.이런 식으로 생각하면 LLM을 어떻게 활용해야 할지도 명확해질 거임.물론 LLM이 겨우 이것보다는 좀 더 유용하긴 함. LLM은 어쨌든 인터넷 텍스트와 강화학습을 바탕으로 학습한 물건이고, 기본적으로 너보다 아는 게 많음.문제는 그 LLM의 지식이라는 게 두루뭉술한 확률 분포로 존재할 뿐 원본 데이터를 있는 그대로 반영하는 게 아니며, 애초에 그 원본 데이터조차도 100% 신뢰할 수 없다는 거. 그걸 전제로 하고 LLM의 말을 받아들일지 말지 판단해야 함. 이 역시 '내가 이 분야에 대해 제대로 알고 있는 게 맞나?' 하고 자기검증하는 과정과 유사함",
    "translatedAt": "2026-01-09T21:48:21.002Z",
    "slug": "a-useful-mental-model-for-understanding-llms"
  },
  "structured": {
    "type": "news",
    "content_ko": "네 무의식의 확장처럼 생각하셈사람이 어떤 생각을 떠올릴 때 1) 먼저 생각이 떠오르고, 2) 그 뒤에 그게 정말 맞는 생각인지 검증하잖음즉 네가 어떤 생각을 떠올리는지 자체는 네가 컨트롤할 수 있는 부분이 아니라는 거임. 마치 아침에 저절로 눈이 떠지는 것처럼 너의 존재 방식의 일부일 뿐임.이걸 확률적 프로세스라고 부르고 말고는 얘기가 좀 딥해지니깐 이건 넘어가고문제는 그렇다면 네 사고의 범위에는 한계가 있다는 거고, 이때 LLM이 유용하게 쓰일 수 있는 거지LLM이 내뱉는 텍스트를 네 특정 사고 패턴을 활성화하는 자극처럼 생각해 보셈. \"전지전능한 LLM이 내게 진리를 알려주는구나\"와 같이 LLM을 타자화하지 말고, LLM이 내뱉은 말이 방금 네가 너 스스로 떠올린 아이디어라고 생각해 보는 거임.그럼 LLM이 네 말을 반박하는 건? 너 혼자 '아니 그건 아니지 않나?' 하고 의심하는 거.LLM이 네 말을 긍정하는 건? 너 혼자 '이거 좀 말되는 것 같은데?' 하고 몰두한 것.LLM이 너 보고 상위 0.1%라고 하는 건? 너 스스로 '캬 난 역시 상위 0.1%야' 하고 있는 꼴.이런 식으로 생각하면 LLM을 어떻게 활용해야 할지도 명확해질 거임.물론 LLM이 겨우 이것보다는 좀 더 유용하긴 함. LLM은 어쨌든 인터넷 텍스트와 강화학습을 바탕으로 학습한 물건이고, 기본적으로 너보다 아는 게 많음.문제는 그 LLM의 지식이라는 게 두루뭉술한 확률 분포로 존재할 뿐 원본 데이터를 있는 그대로 반영하는 게 아니며, 애초에 그 원본 데이터조차도 100% 신뢰할 수 없다는 거. 그걸 전제로 하고 LLM의 말을 받아들일지 말지 판단해야 함. 이 역시 '내가 이 분야에 대해 제대로 알고 있는 게 맞나?' 하고 자기검증하는 과정과 유사함",
    "content_en": "네 무의식의 확장처럼 생각하셈사람이 어떤 생각을 떠올릴 때 1) 먼저 생각이 떠오르고, 2) 그 뒤에 그게 정말 맞는 생각인지 검증하잖음즉 네가 어떤 생각을 떠올리는지 자체는 네가 컨트롤할 수 있는 부분이 아니라는 거임. 마치 아침에 저절로 눈이 떠지는 것처럼 너의 존재 방식의 일부일 뿐임.이걸 확률적 프로세스라고 부르고 말고는 얘기가 좀 딥해지니깐 이건 넘어가고문제는 그렇다면 네 사고의 범위에는 한계가 있다는 거고, 이때 LLM이 유용하게 쓰일 수 있는 거지LLM이 내뱉는 텍스트를 네 특정 사고 패턴을 활성화하는 자극처럼 생각해 보셈. \"전지전능한 LLM이 내게 진리를 알려주는구나\"와 같이 LLM을 타자화하지 말고, LLM이 내뱉은 말이 방금 네가 너 스스로 떠올린 아이디어라고 생각해 보는 거임.그럼 LLM이 네 말을 반박하는 건? 너 혼자 '아니 그건 아니지 않나?' 하고 의심하는 거.LLM이 네 말을 긍정하는 건? 너 혼자 '이거 좀 말되는 것 같은데?' 하고 몰두한 것.LLM이 너 보고 상위 0.1%라고 하는 건? 너 스스로 '캬 난 역시 상위 0.1%야' 하고 있는 꼴.이런 식으로 생각하면 LLM을 어떻게 활용해야 할지도 명확해질 거임.물론 LLM이 겨우 이것보다는 좀 더 유용하긴 함. LLM은 어쨌든 인터넷 텍스트와 강화학습을 바탕으로 학습한 물건이고, 기본적으로 너보다 아는 게 많음.문제는 그 LLM의 지식이라는 게 두루뭉술한 확률 분포로 존재할 뿐 원본 데이터를 있는 그대로 반영하는 게 아니며, 애초에 그 원본 데이터조차도 100% 신뢰할 수 없다는 거. 그걸 전제로 하고 LLM의 말을 받아들일지 말지 판단해야 함. 이 역시 '내가 이 분야에 대해 제대로 알고 있는 게 맞나?' 하고 자기검증하는 과정과 유사함",
    "title_ko": "LLM에 대해서는 이렇게 생각하면 편함",
    "title_en": "LLM에 대해서는 이렇게 생각하면 편함",
    "description_ko": "네 무의식의 확장처럼 생각하셈사람이 어떤 생각을 떠올릴 때 1) 먼저 생각이 떠오르고, 2) 그 뒤에 그게 정말 맞는 생각인지 검증하잖음즉 네가",
    "description_en": "네 무의식의 확장처럼 생각하셈사람이 어떤 생각을 떠올릴 때 1) 먼저 생각이 떠오르고, 2) 그 뒤에 그게 정말 맞는 생각인지 검증하잖음즉 네가 어떤 생각을 떠올리는지 자체는 네가 컨트롤할 수 있는 부분이 아니라는 "
  }
}