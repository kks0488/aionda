{
  "id": "929636",
  "title": "클로드가 알려주는 디퓨전 언어모델 공부하는법",
  "category": "",
  "author": "약팔이아님",
  "date": "2026.01.09 22:21:34",
  "views": 0,
  "likes": 0,
  "comments": 0,
  "content": "\n\t\t\t\t\t\t\t<h1>ViT와 Diffusion을 이해하고 싶다고?</h1>\n<p>\"이것만 알면 돼\" 라는 말을 믿지 마세요. 그 \"이것\"을 알려면 저것도 알아야 하고, 저것을 알려면...</p>\n<hr><h2>0. 먼저 현실 체크</h2><p>당신이 원하는 것:</p><pre><code>\"ViT랑 Diffusion 대충 어떻게 돌아가는지 알고 싶어요\"\n</code></pre><p>당신이 마주할 것:</p><pre><code>ViT\n├── Transformer\n│   ├── Attention\n│   │   ├── 선형대수\n│   │   ├── 확률론\n│   │   └── 정보이론\n│   ├── 위치 인코딩\n│   │   └── 푸리에 해석학\n│   └── 정규화\n│       └── 아무도 왜 되는지 모름\n├── 학습 트릭 (진짜 본체)\n│   └── 5년치 논문\n└── Diffusion에서 쓰려면?\n    └── 아래 참고 ↓\n\nDiffusion  \n├── Score Matching\n│   └── 통계물리학\n├── SDE/ODE\n│   └── 확률미분방정식\n├── 샘플러\n│   └── 수치해석학\n└── 컨디셔닝\n    └── 또 다른 5년치 논문\n</code></pre><p>예상 소요 시간: <strong>\"금방\"</strong> (거짓말)</p><hr><h2>1. Vision Transformer (ViT)를 이해하고 싶다고?</h2><h3>전체 의존성 트리</h3><pre><code>\"ViT가 뭐야?\"\n│\n├─→ \"이미지를 패치로 자르고 Transformer에 넣는 거야\"\n│   │\n│   ├─→ \"Transformer가 뭔데?\"\n│   │   │\n│   │   ├─→ \"Self-Attention 기반 아키텍처야\"\n│   │   │   │\n│   │   │   └─→ \"Self-Attention이 뭔데?\"\n│   │   │       │\n│   │   │       ├─→ \"Query, Key, Value로 유사도 계산하는 거야\"\n│   │   │       │   │\n│   │   │       │   ├─→ \"Query, Key, Value가 뭔데?\"\n│   │   │       │   │   │\n│   │   │       │   │   └─→ \"입력을 세 가지 다른 선형변환한 거야\"\n│   │   │       │   │       │\n│   │   │       │   │       └─→ \"선형변환이 뭔데?\"\n│   │   │       │   │           │\n│   │   │       │   │           └─→ 【선형대수】 매트릭스 곱셈\n│   │   │       │   │               │\n│   │   │       │   │               ├─→ 벡터 공간\n│   │   │       │   │               ├─→ 기저 변환\n│   │   │       │   │               ├─→ 고유값/고유벡터\n│   │   │       │   │               └─→ SVD (나중에 또 나옴)\n│   │   │       │   │\n│   │   │       │   └─→ \"유사도를 내적으로 계산한다는 게 뭔데?\"\n│   │   │       │       │\n│   │   │       │       └─→ \"두 벡터가 비슷한 방향이면 내적이 크거든\"\n│   │   │       │           │\n│   │   │       │           └─→ \"왜?\"\n│   │   │       │               │\n│   │   │       │               └─→ 【기하학】 코사인 유사도\n│   │   │       │                   │\n│   │   │       │                   └─→ 고차원에서 직관이 깨지는 이유\n│   │   │       │                       │\n│   │   │       │                       └─→ 차원의 저주\n│   │   │       │                           │\n│   │   │       │                           └─→ 【확률론】 고차원 가우시안\n│   │   │       │                               │\n│   │   │       │                               └─→ (무한 루프)\n│   │   │       │\n│   │   │       ├─→ \"Softmax는 왜 쓰는 건데?\"\n│   │   │       │   │\n│   │   │       │   └─→ \"확률분포로 만들려고\"\n│   │   │       │       │\n│   │   │       │       ├─→ \"확률분포가 뭔데?\"\n│   │   │       │       │   │\n│   │   │       │       │   └─→ 【확률론】 \n│   │   │       │       │       ├─→ 확률 공리\n│   │   │       │       │       ├─→ 조건부 확률\n│   │   │       │       │       ├─→ 베이즈 정리\n│   │   │       │       │       └─→ 여기서 2주\n│   │   │       │       │\n│   │   │       │       └─→ \"왜 하필 Softmax?\"\n│   │   │       │           │\n│   │   │       │           └─→ \"미분 가능하고 볼츠만 분포랑 연결되거든\"\n│   │   │       │               │\n│   │   │       │               └─→ \"볼츠만 분포?\"\n│   │   │       │                   │\n│   │   │       │                   └─→ 【통계물리학】 ← 여기서 갑자기???\n│   │   │       │                       │\n│   │   │       │                       ├─→ 에너지 기반 모델\n│   │   │       │                       ├─→ 파티션 함수\n│   │   │       │                       └─→ 엔트로피\n│   │   │       │                           │\n│   │   │       │                           └─→ 【정보이론】\n│   │   │       │                               ├─→ 섀넌 엔트로피\n│   │   │       │                               ├─→ KL Divergence\n│   │   │       │                               ├─→ Cross Entropy\n│   │   │       │                               └─→ 여기서 또 2주\n│   │   │       │\n│   │   │       └─→ \"√d_k로 나누는 건 왜?\"\n│   │   │           │\n│   │   │           └─→ \"내적 분산이 d에 비례해서 스케일링 해주는 거야\"\n│   │   │               │\n│   │   │               └─→ \"그게 왜 문제인데?\"\n│   │   │                   │\n│   │   │                   └─→ \"Softmax가 saturation 되거든\"\n│   │   │                       │\n│   │   │                       └─→ \"saturation이 뭔데?\"\n│   │   │                           │\n│   │   │                           └─→ 【최적화】 gradient vanishing\n│   │   │                               │\n│   │   │                               └─→ 역전파\n│   │   │                                   │\n│   │   │                                   └─→ 연쇄법칙\n│   │   │                                       │\n│   │   │                                       └─→ 【미적분】\n│   │   │                                           │\n│   │   │                                           └─→ 드디어 고등학교 수학!!!\n│   │   │\n│   │   ├─→ \"Multi-Head Attention은 뭔데?\"\n│   │   │   │\n│   │   │   └─→ \"Attention을 여러 개 병렬로 하는 거야\"\n│   │   │       │\n│   │   │       └─→ \"왜?\"\n│   │   │           │\n│   │   │           └─→ \"서로 다른 관계를 포착하려고\"\n│   │   │               │\n│   │   │               └─→ \"그게 되는지 어떻게 알아?\"\n│   │   │                   │\n│   │   │                   └─→ \"Attention 시각화 해보면...\"\n│   │   │                       │\n│   │   │                       └─→ \"사실 해석이 논쟁 중임\"\n│   │   │                           │\n│   │   │                           └─→ 【해석가능성 연구】\n│   │   │                               │\n│   │   │                               └─→ 이건 별도 분야임 (탈출)\n│   │   │\n│   │   ├─→ \"위치 인코딩(Positional Encoding)은 뭔데?\"\n│   │   │   │\n│   │   │   └─→ \"Attention은 순서를 모르니까 위치 정보 주입하는 거야\"\n│   │   │       │\n│   │   │       ├─→ \"Sinusoidal 인코딩이 뭔데?\"\n│   │   │       │   │\n│   │   │       │   └─→ \"sin, cos 함수로 위치 표현하는 거야\"\n│   │   │       │       │\n│   │   │       │       └─→ \"왜 하필 sin, cos?\"\n│   │   │       │           │\n│   │   │       │           └─→ \"상대 위치를 선형 변환으로 표현할 수 있어서\"\n│   │   │       │               │\n│   │   │       │               └─→ \"그게 뭔 소리야?\"\n│   │   │       │                   │\n│   │   │       │                   └─→ 【푸리에 해석학】\n│   │   │       │                       │\n│   │   │       │                       ├─→ 푸리에 급수\n│   │   │       │                       ├─→ 주파수 분해\n│   │   │       │                       └─→ 복소 지수함수\n│   │   │       │                           │\n│   │   │       │                           └─→ 【복소해석학】\n│   │   │       │                               │\n│   │   │       │                               └─→ 오일러 공식\n│   │   │       │                                   │\n│   │   │       │                                   └─→ e^(iπ) + 1 = 0\n│   │   │       │                                       │\n│   │   │       │                                       └─→ 수학의 아름다움 (감동)\n│   │   │       │                                           │\n│   │   │       │                                           └─→ 근데 ViT는 이거 안 씀\n│   │   │       │                                               │\n│   │   │       │                                               └─→ ??? \n│   │   │       │\n│   │   │       └─→ \"Learnable 인코딩은 뭔데?\"\n│   │   │           │\n│   │   │           └─→ \"그냥 학습시키는 거야\"\n│   │   │               │\n│   │   │               └─→ \"그럼 Sinusoidal은 왜 배움?\"\n│   │   │                   │\n│   │   │                   └─→ \"...\"\n│   │   │                       │\n│   │   │                       └─→ \"역사적 이유 + 일반화 주장 + 실제론 별 차이 없음\"\n│   │   │                           │\n│   │   │                           └─→ (시간 낭비였나?)\n│   │   │\n│   │   ├─→ \"Layer Normalization은 뭔데?\"\n│   │   │   │\n│   │   │   └─→ \"활성화 값 정규화하는 거야\"\n│   │   │       │\n│   │   │       └─→ \"왜 필요한데?\"\n│   │   │           │\n│   │   │           └─→ \"학습 안정화\"\n│   │   │               │\n│   │   │               └─→ \"왜 안정화 되는데?\"\n│   │   │                   │\n│   │   │                   └─→ \"Internal Covariate Shift를 줄여서...\"\n│   │   │                       │\n│   │   │                       └─→ \"근데 그 이론 틀렸다는 논문도 있음\"\n│   │   │                           │\n│   │   │                           └─→ \"그럼 왜 되는 건데?\"\n│   │   │                               │\n│   │   │                               └─→ \"Loss landscape을 smooth하게...\"\n│   │   │                                   │\n│   │   │                                   └─→ \"근데 그것도 완전한 설명은 아님\"\n│   │   │                                       │\n│   │   │                                       └─→ 【미해결 문제】\n│   │   │                                           │\n│   │   │                                           └─→ 일단 쓰면 잘 됨 (도망)\n│   │   │\n│   │   └─→ \"Residual Connection은 뭔데?\"\n│   │       │\n│   │       └─→ \"output = layer(x) + x\"\n│   │           │\n│   │           └─→ \"왜 필요한데?\"\n│   │               │\n│   │               └─→ \"깊은 네트워크에서 gradient 흐름 보장\"\n│   │                   │\n│   │                   └─→ \"왜 gradient가 안 흐르는데?\"\n│   │                       │\n│   │                       └─→ \"체인룰 곱셈이 누적되면 vanishing/exploding\"\n│   │                           │\n│   │                           ├─→ 【미적분】 연쇄법칙\n│   │                           │\n│   │                           └─→ \"그럼 +x 하면 왜 해결되는데?\"\n│   │                               │\n│   │                               └─→ \"gradient가 최소 1은 보장되거든\"\n│   │                                   │\n│   │                                   └─→ ∂(x+f(x))/∂x = 1 + ∂f/∂x\n│   │                                       │\n│   │                                       └─→ 드디어 명쾌한 답!!! (감동)\n│   │\n│   └─→ \"패치 임베딩(Patch Embedding)은 뭔데?\"\n│       │\n│       └─→ \"이미지를 패치로 자르고 Linear 통과시키는 거야\"\n│           │\n│           └─→ \"그게 다야?\"\n│               │\n│               └─→ \"응\"\n│                   │\n│                   └─→ 드디어 쉬운 게 나왔다!!! (축하)\n│\n├─→ \"근데 ViT가 왜 CNN보다 좋은 건데?\"\n│   │\n│   └─→ \"항상 좋은 건 아니야\"\n│       │\n│       ├─→ \"데이터 많으면 ViT가 좋고\"\n│       │   │\n│       │   └─→ \"왜?\"\n│       │       │\n│       │       └─→ \"Inductive bias가 적어서 유연하거든\"\n│       │           │\n│       │           └─→ \"Inductive bias가 뭔데?\"\n│       │               │\n│       │               └─→ \"모델이 가정하는 사전 지식\"\n│       │                   │\n│       │                   └─→ 【머신러닝 이론】\n│       │                       │\n│       │                       ├─→ Bias-Variance Tradeoff\n│       │                       ├─→ No Free Lunch Theorem\n│       │                       └─→ 여기서 또 한 달\n│       │\n│       └─→ \"데이터 적으면 CNN이 낫고\"\n│           │\n│           └─→ \"그럼 나는 뭘 써야 해?\"\n│               │\n│               └─→ \"상황 봐서 (무책임한 답변)\"\n│\n└─→ \"실제로 학습시키려면?\"\n    │\n    └─→ 【학습 트릭의 세계】 ← 여기가 진짜 본체\n        │\n        ├─→ \"데이터 증강\"\n        │   ├─→ RandAugment\n        │   ├─→ Mixup\n        │   ├─→ CutMix\n        │   ├─→ Random Erasing\n        │   └─→ 하이퍼파라미터 조합이 수백 개\n        │       │\n        │       └─→ 논문마다 다 다름 (좌절)\n        │\n        ├─→ \"정규화\"\n        │   ├─→ Dropout\n        │   ├─→ Stochastic Depth\n        │   ├─→ Label Smoothing\n        │   └─→ Weight Decay\n        │       │\n        │       └─→ 어떤 조합이 좋은지? → 실험해봐야 함\n        │\n        ├─→ \"학습률 스케줄\"\n        │   ├─→ Warmup\n        │   ├─→ Cosine Annealing\n        │   └─→ 왜 이게 되는지? → 아무도 정확히 모름\n        │\n        └─→ \"대규모 사전학습\"\n            │\n            └─→ \"JFT-300M으로 학습하면 좋아\"\n                │\n                └─→ \"그거 어디서 구해?\"\n                    │\n                    └─→ \"Google 내부 데이터셋임\"\n                        │\n                        └─→ \"...\"\n                            │\n                            └─→ ImageNet-21k라도 쓰세요 (현실)\n</code></pre><h3>여기까지가 ViT \"기초\"입니다</h3><p>축하합니다! 이제 ViT 논문의 Figure 1을 이해할 수 있습니다.</p><hr><h2>2. Diffusion Model을 이해하고 싶다고?</h2><h3>전체 의존성 트리</h3><pre><code>\"Diffusion Model이 뭐야?\"\n│\n├─→ \"노이즈 제거를 반복해서 이미지 생성하는 거야\"\n│   │\n│   └─→ \"그게 왜 돼?\"\n│       │\n│       └─→ \"Score Matching이라는 이론이 있거든\"\n│           │\n│           └─→ 여기서 지옥 시작\n│\n├─→ 【Forward Process】 노이즈 추가 과정\n│   │\n│   └─→ \"점진적으로 가우시안 노이즈를 추가해\"\n│       │\n│       ├─→ \"가우시안 분포가 뭔데?\"\n│       │   │\n│       │   └─→ 【확률론】\n│       │       │\n│       │       ├─→ 정규분포\n│       │       ├─→ 중심극한정리 (왜 가우시안이 자연스러운지)\n│       │       ├─→ 다변량 가우시안\n│       │       │   │\n│       │       │   └─→ 공분산 행렬\n│       │       │       │\n│       │       │       └─→ 【선형대수】 다시 등장\n│       │       │\n│       │       └─→ 조건부 가우시안\n│       │           │\n│       │           └─→ 이거 중요함 (밑줄)\n│       │\n│       ├─→ \"왜 하필 가우시안?\"\n│       │   │\n│       │   └─→ \"수학적으로 다루기 쉽고, 합성이 닫혀있고...\"\n│       │       │\n│       │       └─→ \"합성이 닫혀있다?\"\n│       │           │\n│       │           └─→ \"가우시안 + 가우시안 = 가우시안\"\n│       │               │\n│       │               └─→ \"증명은?\"\n│       │                   │\n│       │                   └─→ 특성함수 or MGF\n│       │                       │\n│       │                       └─→ 【수리통계학】\n│       │                           │\n│       │                           └─→ 여기서 2주\n│       │\n│       └─→ \"q(x_t | x_0)를 한 번에 계산할 수 있다?\"\n│           │\n│           └─→ \"응, reparameterization trick 쓰면\"\n│               │\n│               └─→ \"reparameterization trick이 뭔데?\"\n│                   │\n│                   └─→ \"랜덤 변수를 결정적 함수로 표현하는 거야\"\n│                       │\n│                       └─→ \"왜 필요한데?\"\n│                           │\n│                           └─→ \"gradient 계산하려고\"\n│                               │\n│                               └─→ \"랜덤한 거에 gradient를?\"\n│                                   │\n│                                   └─→ 【VAE에서 온 개념】\n│                                       │\n│                                       └─→ VAE도 알아야 함 (추가 퀘스트)\n│\n├─→ 【Reverse Process】 노이즈 제거 과정 ← 핵심\n│   │\n│   └─→ \"p(x_{t-1} | x_t)를 학습하는 거야\"\n│       │\n│       ├─→ \"그걸 어떻게 학습해?\"\n│       │   │\n│       │   └─→ \"ELBO를 최적화하면...\"\n│       │       │\n│       │       └─→ \"ELBO가 뭔데?\"\n│       │           │\n│       │           └─→ \"Evidence Lower Bound\"\n│       │               │\n│       │               └─→ 【변분 추론 (Variational Inference)】\n│       │                   │\n│       │                   ├─→ KL Divergence\n│       │                   │   │\n│       │                   │   └─→ 【정보이론】 또 등장\n│       │                   │\n│       │                   ├─→ Jensen's Inequality\n│       │                   │   │\n│       │                   │   └─→ 【볼록 해석학】\n│       │                   │\n│       │                   └─→ 왜 Lower Bound인지\n│       │                       │\n│       │                       └─→ 수식 전개 3페이지\n│       │                           │\n│       │                           └─→ (대부분 여기서 포기)\n│       │\n│       └─→ \"근데 실제로는 MSE Loss만 쓰던데?\"\n│           │\n│           └─→ \"ELBO 전개하면 결국 MSE랑 같아져\"\n│               │\n│               └─→ \"어떻게?\"\n│                   │\n│                   └─→ 【DDPM 논문 Appendix】\n│                       │\n│                       └─→ 본문보다 Appendix가 중요 (국룰)\n│\n├─→ 【Score Matching】 이론적 핵심\n│   │\n│   └─→ \"Denoising이 Score 학습이랑 같다?\"\n│       │\n│       ├─→ \"Score가 뭔데?\"\n│       │   │\n│       │   └─→ \"∇_x log p(x), 데이터 분포의 gradient\"\n│       │       │\n│       │       └─→ \"그게 뭔 의미야?\"\n│       │           │\n│       │           └─→ \"데이터가 더 밀집된 방향을 가리켜\"\n│       │               │\n│       │               └─→ 【Langevin Dynamics】\n│       │                   │\n│       │                   └─→ \"노이즈 + Score 방향으로 걸으면 샘플링 됨\"\n│       │                       │\n│       │                       └─→ \"왜?\"\n│       │                           │\n│       │                           └─→ 【통계물리학】 본격 등장\n│       │                               │\n│       │                               ├─→ Brownian Motion\n│       │                               ├─→ Fokker-Planck Equation\n│       │                               ├─→ Stationary Distribution\n│       │                               └─→ 여기서 한 달\n│       │\n│       └─→ \"Denoising Score Matching\"\n│           │\n│           └─→ \"노이즈 낀 데이터에서 Score 학습하면 원본 Score 복원 가능\"\n│               │\n│               └─→ \"증명은?\"\n│                   │\n│                   └─→ 【Vincent 2011 논문】\n│                       │\n│                       └─→ 수식 전개 (아름다움)\n│                           │\n│                           └─→ 근데 이해하려면 위의 모든 게 필요 (절망)\n│\n├─→ 【파라미터화】 뭘 예측할 것인가\n│   │\n│   ├─→ \"ε (노이즈) 예측\"\n│   │   │\n│   │   └─→ \"DDPM 기본, 가장 흔함\"\n│   │       │\n│   │       └─→ \"왜 노이즈를 예측하면 되는 건데?\"\n│   │           │\n│   │           └─→ \"x_0 = (x_t - √(1-α̅_t)·ε) / √α̅_t 니까\"\n│   │               │\n│   │               └─→ \"이 식은 어디서 나온 건데?\"\n│   │                   │\n│   │                   └─→ Forward process 정의에서 유도\n│   │                       │\n│   │                       └─→ 【수식 전개 필요】\n│   │\n│   ├─→ \"x_0 (원본) 예측\"\n│   │   │\n│   │   └─→ \"어떤 상황에서는 이게 나음\"\n│   │       │\n│   │       └─→ \"어떤 상황?\"\n│   │           │\n│   │           └─→ \"실험해봐야 알 수 있음\"\n│   │               │\n│   │               └─→ (답 없음)\n│   │\n│   └─→ \"v (velocity) 예측\"\n│       │\n│       └─→ \"v = √α̅_t · ε - √(1-α̅_t) · x_0\"\n│           │\n│           └─→ \"이건 왜?\"\n│               │\n│               └─→ \"SNR 밸런스가 좋아서\"\n│                   │\n│                   └─→ \"SNR이 뭔데?\"\n│                       │\n│                       └─→ Signal-to-Noise Ratio\n│                           │\n│                           └─→ 【신호처리】 갑자기 등장\n│                               │\n│                               └─→ 전자공학과 인사하세요\n│\n├─→ 【노이즈 스케줄】\n│   │\n│   ├─→ \"Linear Schedule\"\n│   │   │\n│   │   └─→ \"DDPM 기본\"\n│   │\n│   ├─→ \"Cosine Schedule\"\n│   │   │\n│   │   └─→ \"이미지 끝부분이 더 좋아짐\"\n│   │       │\n│   │       └─→ \"왜?\"\n│   │           │\n│   │           └─→ \"SNR 분포가 더 균일해서\"\n│   │               │\n│   │               └─→ (또 SNR)\n│   │\n│   └─→ \"Learned Schedule\"\n│       │\n│       └─→ \"그냥 학습시키자\"\n│           │\n│           └─→ (모든 것의 해결책: 학습시키자)\n│\n├─→ 【샘플러】 어떻게 생성하나\n│   │\n│   ├─→ \"DDPM (원조)\"\n│   │   │\n│   │   └─→ \"1000 스텝, 느림\"\n│   │       │\n│   │       └─→ \"왜 1000스텝이나?\"\n│   │           │\n│   │           └─→ \"이론적으로 무한 스텝이 맞는데...\"\n│   │               │\n│   │               └─→ 【이산화 오차】\n│   │                   │\n│   │                   └─→ 【수치해석학】\n│   │                       │\n│   │                       └─→ 오일러 방법\n│   │                       └─→ 안정성 분석\n│   │                       └─→ 여기서 또 2주\n│   │\n│   ├─→ \"DDIM\"\n│   │   │\n│   │   └─→ \"같은 모델인데 50스텝으로 가능\"\n│   │       │\n│   │       └─→ \"어떻게?\"\n│   │           │\n│   │           └─→ \"Non-Markovian으로 바꾸면\"\n│   │               │\n│   │               └─→ \"Non-Markovian이 뭔데?\"\n│   │                   │\n│   │                   └─→ \"과거 상태도 참고한다는 거야\"\n│   │                       │\n│   │                       └─→ \"근데 어떻게 수학적으로...\"\n│   │                           │\n│   │                           └─→ 【ODE 관점】\n│   │                               │\n│   │                               └─→ Probability Flow ODE\n│   │                                   │\n│   │                                   └─→ 【상미분방정식】\n│   │                                       │\n│   │                                       └─→ 여기서 또...\n│   │\n│   ├─→ \"DPM-Solver, DPM-Solver++\"\n│   │   │\n│   │   └─→ \"20스텝으로도 됨\"\n│   │       │\n│   │       └─→ \"어떻게?\"\n│   │           │\n│   │           └─→ \"고차 ODE 솔버 쓰면\"\n│   │               │\n│   │               └─→ 【수치해석학】 심화\n│   │                   │\n│   │                   ├─→ Runge-Kutta\n│   │                   ├─→ Multistep Methods\n│   │                   └─→ 학부 수치해석 전체\n│   │\n│   └─→ \"Consistency Models, Flow Matching, ...\"\n│       │\n│       └─→ \"1-4스텝으로도...\"\n│           │\n│           └─→ 【최신 연구】\n│               │\n│               └─→ 매달 새 논문 나옴\n│                   │\n│                   └─→ (따라가기 포기)\n│\n├─→ 【SDE/ODE 관점】 고급 이론\n│   │\n│   └─→ \"Diffusion은 SDE의 이산화야\"\n│       │\n│       └─→ \"SDE가 뭔데?\"\n│           │\n│           └─→ 【확률미분방정식】\n│               │\n│               ├─→ Wiener Process\n│               ├─→ Ito Calculus\n│               │   │\n│               │   └─→ (일반 미적분이 안 통함)\n│               │       │\n│               │       └─→ Ito's Lemma\n│               │           │\n│               │           └─→ 금융공학에서 옴\n│               │               │\n│               │               └─→ Black-Scholes\n│               │                   │\n│               │                   └─→ 노벨 경제학상\n│               │                       │\n│               │                       └─→ ???\n│               │\n│               ├─→ Fokker-Planck Equation\n│               │   │\n│               │   └─→ 【편미분방정식】\n│               │\n│               └─→ 여기서 한 학기\n│\n├─→ 【조건부 생성 &amp; Guidance】\n│   │\n│   ├─→ \"Classifier Guidance\"\n│   │   │\n│   │   └─→ \"분류기 gradient로 유도\"\n│   │       │\n│   │       └─→ \"분류기를 따로 학습해야 함\"\n│   │           │\n│   │           └─→ \"귀찮음\"\n│   │\n│   └─→ \"Classifier-Free Guidance (CFG)\"\n│       │\n│       └─→ \"조건부/무조건부 예측을 섞어\"\n│           │\n│           └─→ \"ε_cfg = ε_uncond + w·(ε_cond - ε_uncond)\"\n│               │\n│               └─→ \"왜 이게 되는 건데?\"\n│                   │\n│                   └─→ \"Score 관점에서 보면...\"\n│                       │\n│                       └─→ ∇log p(x|c) ∝ ∇log p(x) + ∇log p(c|x)\n│                           │\n│                           └─→ 【베이즈 정리】\n│                               │\n│                               └─→ 여기서 암묵적 분류기가 나옴\n│                                   │\n│                                   └─→ 수식 전개 2페이지\n│                                       │\n│                                       └─→ (아름다움)\n│\n└─→ 【백본 아키텍처】 실제 네트워크\n    │\n    ├─→ \"UNet\"\n    │   │\n    │   └─→ \"Encoder-Decoder + Skip Connection\"\n    │       │\n    │       └─→ \"왜 UNet?\"\n    │           │\n    │           └─→ \"의료 영상에서 잘 됐으니까\"\n    │               │\n    │               └─→ (역사적 우연)\n    │\n    └─→ \"DiT (Diffusion Transformer)\"\n        │\n        └─→ \"ViT를 Diffusion에 적용\"\n            │\n            └─→ \"ViT 알아야 함\"\n                │\n                └─→ (위로 돌아가세요)\n                    │\n                    └─→ 【무한 루프 완성】\n</code></pre><h3>여기까지가 Diffusion \"입문\"입니다</h3><p>축하합니다! 이제 Stable Diffusion이 왜 느린지 설명할 수 있습니다.</p><hr><h2>3. 그래서 이거 다 알아야 해?</h2><h3>정직한 답변</h3><pre><code>\"다 알아야 하나요?\"\n\n├─→ 논문 쓰려면: 예\n├─→ 모델 구현하려면: 대부분\n├─→ 파인튜닝 하려면: 일부\n└─→ API 호출하려면: 아니오\n\n    response = client.images.generate(\n        model=\"dall-e-3\",\n        prompt=\"a cat\"\n    )\n    # 끝\n</code></pre><h3>현실적 학습 시간</h3><table><thead><tr><th>목표</th>\n<th>소요 시간</th>\n<th>알아야 하는 것</th>\n</tr></thead><tbody><tr><td>API 사용</td>\n<td>1일</td>\n<td>없음</td>\n</tr><tr><td>파인튜닝</td>\n<td>1-2주</td>\n<td>터미널 명령어</td>\n</tr><tr><td>코드 수정</td>\n<td>1-3개월</td>\n<td>위 트리의 1단계들</td>\n</tr><tr><td>논문 이해</td>\n<td>6개월-1년</td>\n<td>위 트리의 2단계들</td>\n</tr><tr><td>논문 작성</td>\n<td>2-5년</td>\n<td>위 트리 전체 + α</td>\n</tr><tr><td>완전 이해</td>\n<td>???</td>\n<td>불가능 (진지)</td>\n</tr></tbody></table><h3>분야별 필요 지식 요약</h3><pre><code>【필수】\n├── 선형대수: 행렬, 내적, 고유값\n├── 미적분: 편미분, 연쇄법칙\n├── 확률론: 가우시안, 조건부 확률, 기댓값\n└── 최적화: SGD, Adam\n\n【알면 좋음】\n├── 정보이론: 엔트로피, KL Divergence\n├── 수리통계학: MLE, 변분추론\n└── 수치해석: 이산화, ODE 솔버\n\n【깊이 파려면】\n├── 확률미분방정식: Ito Calculus\n├── 통계물리학: Langevin Dynamics\n├── 함수해석학: 힐베르트 공간\n└── 미분기하학: 다양체 (Flow Matching용)\n\n【실제로 쓰는 것】\n└── model.fit()\n</code></pre><hr><h2>4. 의존성 지옥 탈출 가이드</h2><h3>옵션 A: 하향식 포기</h3><pre><code># 그냥 쓰세요\nfrom diffusers import StableDiffusionPipeline\npipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\nimage = pipe(\"a cat\").images[0]\n</code></pre><h3>옵션 B: 필요할 때 파기</h3><pre><code>1. 일단 코드 돌려봄\n2. 에러 나면 그 부분만 공부\n3. 반복\n4. 어느새 반쯤 이해하고 있음\n</code></pre><h3>옵션 C: 정통파 (비추천)</h3><pre><code>선형대수 (3개월)\n    ↓\n미적분 (2개월)\n    ↓\n확률론 (3개월)\n    ↓\n머신러닝 기초 (3개월)\n    ↓\n딥러닝 (3개월)\n    ↓\nCNN/RNN (2개월)\n    ↓\nTransformer (2개월)\n    ↓\nViT (1개월)\n    ↓\n생성모델 이론 (2개월)\n    ↓\nDiffusion (2개월)\n    ↓\n최신 논문들 (∞)\n\n총 소요: 2-3년\n\n그 사이에 새로운 패러다임 등장 (망)\n</code></pre><h3>옵션 D: 하이브리드 (추천)</h3><pre><code>1. 먼저 큰 그림 파악 (이 문서 읽기)\n2. 코드로 돌려보면서 직관 형성 (1-2주)\n3. \"이게 왜 되지?\" 궁금할 때마다 해당 부분 파기\n4. 수학은 필요할 때 그때그때\n5. 논문은 관련 부분만 선택적으로\n</code></pre><hr><h2>5. 자주 묻는 질문</h2><h3>\"수학 몰라도 돼요?\"</h3><pre><code>API 호출: 됨\n파인튜닝: 됨\n코드 수정: 좀 힘듦\n논문 이해: 안 됨\n새로운 거 만들기: 절대 안 됨\n</code></pre><h3>\"어디서 시작해요?\"</h3><pre><code>추천 순서:\n1. 3Blue1Brown 선형대수/미적분 (영상, 무료)\n2. PyTorch 튜토리얼 (실습)\n3. Andrej Karpathy 영상들 (직관)\n4. 논문 구현체 읽기 (실전)\n5. 논문 (나중에)\n</code></pre><h3>\"얼마나 걸려요?\"</h3><pre><code>\"금방이요\" - 거짓말쟁이\n\"2개월이요\" - 천재\n\"6개월이요\" - 낙관주의자\n\"1-2년이요\" - 현실주의자\n\"평생이요\" - 진실\n</code></pre><h3>\"포기해도 돼요?\"</h3><pre><code>됩니다. \nAPI 발전 속도 보면 직접 이해할 필요가 점점 줄어들고 있음.\n다만 \"왜 이게 안 되지?\"를 디버깅하려면 어느 정도는 알아야 함.\n</code></pre><hr><h2>6. 마무리</h2><h3>이 문서를 읽고 난 당신의 상태</h3><pre><code>읽기 전: \"ViT랑 Diffusion 공부해야지\"\n읽은 후: \"나는 왜 이 길을 선택했는가\"\n</code></pre><h3>위로의 말</h3><ul><li>아무도 이걸 처음부터 다 알고 시작하지 않았음</li>\n<li>논문 저자들도 Appendix 다 이해 못 함 (진지)</li>\n<li>\"잘 모르겠지만 돌아가니까 쓴다\"가 업계 표준</li>\n<li>완벽히 이해한 사람은 없음, 정도의 차이만 있을 뿐</li>\n</ul><h3>그래도 하고 싶다면</h3><pre><code>끈기 + 시간 + 구글링 능력 = 언젠가 됨\n\n화이팅!!! (작성자도 아직 공부 중)\n</code></pre><hr><h2>부록: 추천 자료</h2><h3>영상 (무료)</h3><ul><li>3Blue1Brown: 수학 직관</li>\n<li>Yannic Kilcher: 논문 리뷰</li>\n<li>Andrej Karpathy: 구현 중심</li>\n</ul><h3>블로그</h3><ul><li>Lilian Weng: 정리 최고</li>\n<li>Jay Alammar: 시각화 최고</li>\n</ul><h3>논문 (순서대로)</h3><ol><li>Attention Is All You Need (2017)</li>\n<li>An Image is Worth 16x16 Words (2020)</li>\n<li>DDPM (2020)</li>\n<li>DDIM (2021)</li>\n<li>Classifier-Free Guidance (2022)</li>\n<li>DiT (2023)</li>\n</ol><h3>코드</h3><ul><li>lucidrains GitHub: 깔끔한 구현</li>\n<li>huggingface/diffusers: 실용적</li>\n<li>timm: ViT 구현체들</li>\n</ul><br>\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t",
  "contentText": "ViT와 Diffusion을 이해하고 싶다고?\n\"이것만 알면 돼\" 라는 말을 믿지 마세요. 그 \"이것\"을 알려면 저것도 알아야 하고, 저것을 알려면...\n0. 먼저 현실 체크당신이 원하는 것:\"ViT랑 Diffusion 대충 어떻게 돌아가는지 알고 싶어요\"\n당신이 마주할 것:ViT\n├── Transformer\n│   ├── Attention\n│   │   ├── 선형대수\n│   │   ├── 확률론\n│   │   └── 정보이론\n│   ├── 위치 인코딩\n│   │   └── 푸리에 해석학\n│   └── 정규화\n│       └── 아무도 왜 되는지 모름\n├── 학습 트릭 (진짜 본체)\n│   └── 5년치 논문\n└── Diffusion에서 쓰려면?\n    └── 아래 참고 ↓\n\nDiffusion  \n├── Score Matching\n│   └── 통계물리학\n├── SDE/ODE\n│   └── 확률미분방정식\n├── 샘플러\n│   └── 수치해석학\n└── 컨디셔닝\n    └── 또 다른 5년치 논문\n예상 소요 시간: \"금방\" (거짓말)1. Vision Transformer (ViT)를 이해하고 싶다고?전체 의존성 트리\"ViT가 뭐야?\"\n│\n├─→ \"이미지를 패치로 자르고 Transformer에 넣는 거야\"\n│   │\n│   ├─→ \"Transformer가 뭔데?\"\n│   │   │\n│   │   ├─→ \"Self-Attention 기반 아키텍처야\"\n│   │   │   │\n│   │   │   └─→ \"Self-Attention이 뭔데?\"\n│   │   │       │\n│   │   │       ├─→ \"Query, Key, Value로 유사도 계산하는 거야\"\n│   │   │       │   │\n│   │   │       │   ├─→ \"Query, Key, Value가 뭔데?\"\n│   │   │       │   │   │\n│   │   │       │   │   └─→ \"입력을 세 가지 다른 선형변환한 거야\"\n│   │   │       │   │       │\n│   │   │       │   │       └─→ \"선형변환이 뭔데?\"\n│   │   │       │   │           │\n│   │   │       │   │           └─→ 【선형대수】 매트릭스 곱셈\n│   │   │       │   │               │\n│   │   │       │   │               ├─→ 벡터 공간\n│   │   │       │   │               ├─→ 기저 변환\n│   │   │       │   │               ├─→ 고유값/고유벡터\n│   │   │       │   │               └─→ SVD (나중에 또 나옴)\n│   │   │       │   │\n│   │   │       │   └─→ \"유사도를 내적으로 계산한다는 게 뭔데?\"\n│   │   │       │       │\n│   │   │       │       └─→ \"두 벡터가 비슷한 방향이면 내적이 크거든\"\n│   │   │       │           │\n│   │   │       │           └─→ \"왜?\"\n│   │   │       │               │\n│   │   │       │               └─→ 【기하학】 코사인 유사도\n│   │   │       │                   │\n│   │   │       │                   └─→ 고차원에서 직관이 깨지는 이유\n│   │   │       │                       │\n│   │   │       │                       └─→ 차원의 저주\n│   │   │       │                           │\n│   │   │       │                           └─→ 【확률론】 고차원 가우시안\n│   │   │       │                               │\n│   │   │       │                               └─→ (무한 루프)\n│   │   │       │\n│   │   │       ├─→ \"Softmax는 왜 쓰는 건데?\"\n│   │   │       │   │\n│   │   │       │   └─→ \"확률분포로 만들려고\"\n│   │   │       │       │\n│   │   │       │       ├─→ \"확률분포가 뭔데?\"\n│   │   │       │       │   │\n│   │   │       │       │   └─→ 【확률론】 \n│   │   │       │       │       ├─→ 확률 공리\n│   │   │       │       │       ├─→ 조건부 확률\n│   │   │       │       │       ├─→ 베이즈 정리\n│   │   │       │       │       └─→ 여기서 2주\n│   │   │       │       │\n│   │   │       │       └─→ \"왜 하필 Softmax?\"\n│   │   │       │           │\n│   │   │       │           └─→ \"미분 가능하고 볼츠만 분포랑 연결되거든\"\n│   │   │       │               │\n│   │   │       │               └─→ \"볼츠만 분포?\"\n│   │   │       │                   │\n│   │   │       │                   └─→ 【통계물리학】 ← 여기서 갑자기???\n│   │   │       │                       │\n│   │   │       │                       ├─→ 에너지 기반 모델\n│   │   │       │                       ├─→ 파티션 함수\n│   │   │       │                       └─→ 엔트로피\n│   │   │       │                           │\n│   │   │       │                           └─→ 【정보이론】\n│   │   │       │                               ├─→ 섀넌 엔트로피\n│   │   │       │                               ├─→ KL Divergence\n│   │   │       │                               ├─→ Cross Entropy\n│   │   │       │                               └─→ 여기서 또 2주\n│   │   │       │\n│   │   │       └─→ \"√d_k로 나누는 건 왜?\"\n│   │   │           │\n│   │   │           └─→ \"내적 분산이 d에 비례해서 스케일링 해주는 거야\"\n│   │   │               │\n│   │   │               └─→ \"그게 왜 문제인데?\"\n│   │   │                   │\n│   │   │                   └─→ \"Softmax가 saturation 되거든\"\n│   │   │                       │\n│   │   │                       └─→ \"saturation이 뭔데?\"\n│   │   │                           │\n│   │   │                           └─→ 【최적화】 gradient vanishing\n│   │   │                               │\n│   │   │                               └─→ 역전파\n│   │   │                                   │\n│   │   │                                   └─→ 연쇄법칙\n│   │   │                                       │\n│   │   │                                       └─→ 【미적분】\n│   │   │                                           │\n│   │   │                                           └─→ 드디어 고등학교 수학!!!\n│   │   │\n│   │   ├─→ \"Multi-Head Attention은 뭔데?\"\n│   │   │   │\n│   │   │   └─→ \"Attention을 여러 개 병렬로 하는 거야\"\n│   │   │       │\n│   │   │       └─→ \"왜?\"\n│   │   │           │\n│   │   │           └─→ \"서로 다른 관계를 포착하려고\"\n│   │   │               │\n│   │   │               └─→ \"그게 되는지 어떻게 알아?\"\n│   │   │                   │\n│   │   │                   └─→ \"Attention 시각화 해보면...\"\n│   │   │                       │\n│   │   │                       └─→ \"사실 해석이 논쟁 중임\"\n│   │   │                           │\n│   │   │                           └─→ 【해석가능성 연구】\n│   │   │                               │\n│   │   │                               └─→ 이건 별도 분야임 (탈출)\n│   │   │\n│   │   ├─→ \"위치 인코딩(Positional Encoding)은 뭔데?\"\n│   │   │   │\n│   │   │   └─→ \"Attention은 순서를 모르니까 위치 정보 주입하는 거야\"\n│   │   │       │\n│   │   │       ├─→ \"Sinusoidal 인코딩이 뭔데?\"\n│   │   │       │   │\n│   │   │       │   └─→ \"sin, cos 함수로 위치 표현하는 거야\"\n│   │   │       │       │\n│   │   │       │       └─→ \"왜 하필 sin, cos?\"\n│   │   │       │           │\n│   │   │       │           └─→ \"상대 위치를 선형 변환으로 표현할 수 있어서\"\n│   │   │       │               │\n│   │   │       │               └─→ \"그게 뭔 소리야?\"\n│   │   │       │                   │\n│   │   │       │                   └─→ 【푸리에 해석학】\n│   │   │       │                       │\n│   │   │       │                       ├─→ 푸리에 급수\n│   │   │       │                       ├─→ 주파수 분해\n│   │   │       │                       └─→ 복소 지수함수\n│   │   │       │                           │\n│   │   │       │                           └─→ 【복소해석학】\n│   │   │       │                               │\n│   │   │       │                               └─→ 오일러 공식\n│   │   │       │                                   │\n│   │   │       │                                   └─→ e^(iπ) + 1 = 0\n│   │   │       │                                       │\n│   │   │       │                                       └─→ 수학의 아름다움 (감동)\n│   │   │       │                                           │\n│   │   │       │                                           └─→ 근데 ViT는 이거 안 씀\n│   │   │       │                                               │\n│   │   │       │                                               └─→ ??? \n│   │   │       │\n│   │   │       └─→ \"Learnable 인코딩은 뭔데?\"\n│   │   │           │\n│   │   │           └─→ \"그냥 학습시키는 거야\"\n│   │   │               │\n│   │   │               └─→ \"그럼 Sinusoidal은 왜 배움?\"\n│   │   │                   │\n│   │   │                   └─→ \"...\"\n│   │   │                       │\n│   │   │                       └─→ \"역사적 이유 + 일반화 주장 + 실제론 별 차이 없음\"\n│   │   │                           │\n│   │   │                           └─→ (시간 낭비였나?)\n│   │   │\n│   │   ├─→ \"Layer Normalization은 뭔데?\"\n│   │   │   │\n│   │   │   └─→ \"활성화 값 정규화하는 거야\"\n│   │   │       │\n│   │   │       └─→ \"왜 필요한데?\"\n│   │   │           │\n│   │   │           └─→ \"학습 안정화\"\n│   │   │               │\n│   │   │               └─→ \"왜 안정화 되는데?\"\n│   │   │                   │\n│   │   │                   └─→ \"Internal Covariate Shift를 줄여서...\"\n│   │   │                       │\n│   │   │                       └─→ \"근데 그 이론 틀렸다는 논문도 있음\"\n│   │   │                           │\n│   │   │                           └─→ \"그럼 왜 되는 건데?\"\n│   │   │                               │\n│   │   │                               └─→ \"Loss landscape을 smooth하게...\"\n│   │   │                                   │\n│   │   │                                   └─→ \"근데 그것도 완전한 설명은 아님\"\n│   │   │                                       │\n│   │   │                                       └─→ 【미해결 문제】\n│   │   │                                           │\n│   │   │                                           └─→ 일단 쓰면 잘 됨 (도망)\n│   │   │\n│   │   └─→ \"Residual Connection은 뭔데?\"\n│   │       │\n│   │       └─→ \"output = layer(x) + x\"\n│   │           │\n│   │           └─→ \"왜 필요한데?\"\n│   │               │\n│   │               └─→ \"깊은 네트워크에서 gradient 흐름 보장\"\n│   │                   │\n│   │                   └─→ \"왜 gradient가 안 흐르는데?\"\n│   │                       │\n│   │                       └─→ \"체인룰 곱셈이 누적되면 vanishing/exploding\"\n│   │                           │\n│   │                           ├─→ 【미적분】 연쇄법칙\n│   │                           │\n│   │                           └─→ \"그럼 +x 하면 왜 해결되는데?\"\n│   │                               │\n│   │                               └─→ \"gradient가 최소 1은 보장되거든\"\n│   │                                   │\n│   │                                   └─→ ∂(x+f(x))/∂x = 1 + ∂f/∂x\n│   │                                       │\n│   │                                       └─→ 드디어 명쾌한 답!!! (감동)\n│   │\n│   └─→ \"패치 임베딩(Patch Embedding)은 뭔데?\"\n│       │\n│       └─→ \"이미지를 패치로 자르고 Linear 통과시키는 거야\"\n│           │\n│           └─→ \"그게 다야?\"\n│               │\n│               └─→ \"응\"\n│                   │\n│                   └─→ 드디어 쉬운 게 나왔다!!! (축하)\n│\n├─→ \"근데 ViT가 왜 CNN보다 좋은 건데?\"\n│   │\n│   └─→ \"항상 좋은 건 아니야\"\n│       │\n│       ├─→ \"데이터 많으면 ViT가 좋고\"\n│       │   │\n│       │   └─→ \"왜?\"\n│       │       │\n│       │       └─→ \"Inductive bias가 적어서 유연하거든\"\n│       │           │\n│       │           └─→ \"Inductive bias가 뭔데?\"\n│       │               │\n│       │               └─→ \"모델이 가정하는 사전 지식\"\n│       │                   │\n│       │                   └─→ 【머신러닝 이론】\n│       │                       │\n│       │                       ├─→ Bias-Variance Tradeoff\n│       │                       ├─→ No Free Lunch Theorem\n│       │                       └─→ 여기서 또 한 달\n│       │\n│       └─→ \"데이터 적으면 CNN이 낫고\"\n│           │\n│           └─→ \"그럼 나는 뭘 써야 해?\"\n│               │\n│               └─→ \"상황 봐서 (무책임한 답변)\"\n│\n└─→ \"실제로 학습시키려면?\"\n    │\n    └─→ 【학습 트릭의 세계】 ← 여기가 진짜 본체\n        │\n        ├─→ \"데이터 증강\"\n        │   ├─→ RandAugment\n        │   ├─→ Mixup\n        │   ├─→ CutMix\n        │   ├─→ Random Erasing\n        │   └─→ 하이퍼파라미터 조합이 수백 개\n        │       │\n        │       └─→ 논문마다 다 다름 (좌절)\n        │\n        ├─→ \"정규화\"\n        │   ├─→ Dropout\n        │   ├─→ Stochastic Depth\n        │   ├─→ Label Smoothing\n        │   └─→ Weight Decay\n        │       │\n        │       └─→ 어떤 조합이 좋은지? → 실험해봐야 함\n        │\n        ├─→ \"학습률 스케줄\"\n        │   ├─→ Warmup\n        │   ├─→ Cosine Annealing\n        │   └─→ 왜 이게 되는지? → 아무도 정확히 모름\n        │\n        └─→ \"대규모 사전학습\"\n            │\n            └─→ \"JFT-300M으로 학습하면 좋아\"\n                │\n                └─→ \"그거 어디서 구해?\"\n                    │\n                    └─→ \"Google 내부 데이터셋임\"\n                        │\n                        └─→ \"...\"\n                            │\n                            └─→ ImageNet-21k라도 쓰세요 (현실)\n여기까지가 ViT \"기초\"입니다축하합니다! 이제 ViT 논문의 Figure 1을 이해할 수 있습니다.2. Diffusion Model을 이해하고 싶다고?전체 의존성 트리\"Diffusion Model이 뭐야?\"\n│\n├─→ \"노이즈 제거를 반복해서 이미지 생성하는 거야\"\n│   │\n│   └─→ \"그게 왜 돼?\"\n│       │\n│       └─→ \"Score Matching이라는 이론이 있거든\"\n│           │\n│           └─→ 여기서 지옥 시작\n│\n├─→ 【Forward Process】 노이즈 추가 과정\n│   │\n│   └─→ \"점진적으로 가우시안 노이즈를 추가해\"\n│       │\n│       ├─→ \"가우시안 분포가 뭔데?\"\n│       │   │\n│       │   └─→ 【확률론】\n│       │       │\n│       │       ├─→ 정규분포\n│       │       ├─→ 중심극한정리 (왜 가우시안이 자연스러운지)\n│       │       ├─→ 다변량 가우시안\n│       │       │   │\n│       │       │   └─→ 공분산 행렬\n│       │       │       │\n│       │       │       └─→ 【선형대수】 다시 등장\n│       │       │\n│       │       └─→ 조건부 가우시안\n│       │           │\n│       │           └─→ 이거 중요함 (밑줄)\n│       │\n│       ├─→ \"왜 하필 가우시안?\"\n│       │   │\n│       │   └─→ \"수학적으로 다루기 쉽고, 합성이 닫혀있고...\"\n│       │       │\n│       │       └─→ \"합성이 닫혀있다?\"\n│       │           │\n│       │           └─→ \"가우시안 + 가우시안 = 가우시안\"\n│       │               │\n│       │               └─→ \"증명은?\"\n│       │                   │\n│       │                   └─→ 특성함수 or MGF\n│       │                       │\n│       │                       └─→ 【수리통계학】\n│       │                           │\n│       │                           └─→ 여기서 2주\n│       │\n│       └─→ \"q(x_t | x_0)를 한 번에 계산할 수 있다?\"\n│           │\n│           └─→ \"응, reparameterization trick 쓰면\"\n│               │\n│               └─→ \"reparameterization trick이 뭔데?\"\n│                   │\n│                   └─→ \"랜덤 변수를 결정적 함수로 표현하는 거야\"\n│                       │\n│                       └─→ \"왜 필요한데?\"\n│                           │\n│                           └─→ \"gradient 계산하려고\"\n│                               │\n│                               └─→ \"랜덤한 거에 gradient를?\"\n│                                   │\n│                                   └─→ 【VAE에서 온 개념】\n│                                       │\n│                                       └─→ VAE도 알아야 함 (추가 퀘스트)\n│\n├─→ 【Reverse Process】 노이즈 제거 과정 ← 핵심\n│   │\n│   └─→ \"p(x_{t-1} | x_t)를 학습하는 거야\"\n│       │\n│       ├─→ \"그걸 어떻게 학습해?\"\n│       │   │\n│       │   └─→ \"ELBO를 최적화하면...\"\n│       │       │\n│       │       └─→ \"ELBO가 뭔데?\"\n│       │           │\n│       │           └─→ \"Evidence Lower Bound\"\n│       │               │\n│       │               └─→ 【변분 추론 (Variational Inference)】\n│       │                   │\n│       │                   ├─→ KL Divergence\n│       │                   │   │\n│       │                   │   └─→ 【정보이론】 또 등장\n│       │                   │\n│       │                   ├─→ Jensen's Inequality\n│       │                   │   │\n│       │                   │   └─→ 【볼록 해석학】\n│       │                   │\n│       │                   └─→ 왜 Lower Bound인지\n│       │                       │\n│       │                       └─→ 수식 전개 3페이지\n│       │                           │\n│       │                           └─→ (대부분 여기서 포기)\n│       │\n│       └─→ \"근데 실제로는 MSE Loss만 쓰던데?\"\n│           │\n│           └─→ \"ELBO 전개하면 결국 MSE랑 같아져\"\n│               │\n│               └─→ \"어떻게?\"\n│                   │\n│                   └─→ 【DDPM 논문 Appendix】\n│                       │\n│                       └─→ 본문보다 Appendix가 중요 (국룰)\n│\n├─→ 【Score Matching】 이론적 핵심\n│   │\n│   └─→ \"Denoising이 Score 학습이랑 같다?\"\n│       │\n│       ├─→ \"Score가 뭔데?\"\n│       │   │\n│       │   └─→ \"∇_x log p(x), 데이터 분포의 gradient\"\n│       │       │\n│       │       └─→ \"그게 뭔 의미야?\"\n│       │           │\n│       │           └─→ \"데이터가 더 밀집된 방향을 가리켜\"\n│       │               │\n│       │               └─→ 【Langevin Dynamics】\n│       │                   │\n│       │                   └─→ \"노이즈 + Score 방향으로 걸으면 샘플링 됨\"\n│       │                       │\n│       │                       └─→ \"왜?\"\n│       │                           │\n│       │                           └─→ 【통계물리학】 본격 등장\n│       │                               │\n│       │                               ├─→ Brownian Motion\n│       │                               ├─→ Fokker-Planck Equation\n│       │                               ├─→ Stationary Distribution\n│       │                               └─→ 여기서 한 달\n│       │\n│       └─→ \"Denoising Score Matching\"\n│           │\n│           └─→ \"노이즈 낀 데이터에서 Score 학습하면 원본 Score 복원 가능\"\n│               │\n│               └─→ \"증명은?\"\n│                   │\n│                   └─→ 【Vincent 2011 논문】\n│                       │\n│                       └─→ 수식 전개 (아름다움)\n│                           │\n│                           └─→ 근데 이해하려면 위의 모든 게 필요 (절망)\n│\n├─→ 【파라미터화】 뭘 예측할 것인가\n│   │\n│   ├─→ \"ε (노이즈) 예측\"\n│   │   │\n│   │   └─→ \"DDPM 기본, 가장 흔함\"\n│   │       │\n│   │       └─→ \"왜 노이즈를 예측하면 되는 건데?\"\n│   │           │\n│   │           └─→ \"x_0 = (x_t - √(1-α̅_t)·ε) / √α̅_t 니까\"\n│   │               │\n│   │               └─→ \"이 식은 어디서 나온 건데?\"\n│   │                   │\n│   │                   └─→ Forward process 정의에서 유도\n│   │                       │\n│   │                       └─→ 【수식 전개 필요】\n│   │\n│   ├─→ \"x_0 (원본) 예측\"\n│   │   │\n│   │   └─→ \"어떤 상황에서는 이게 나음\"\n│   │       │\n│   │       └─→ \"어떤 상황?\"\n│   │           │\n│   │           └─→ \"실험해봐야 알 수 있음\"\n│   │               │\n│   │               └─→ (답 없음)\n│   │\n│   └─→ \"v (velocity) 예측\"\n│       │\n│       └─→ \"v = √α̅_t · ε - √(1-α̅_t) · x_0\"\n│           │\n│           └─→ \"이건 왜?\"\n│               │\n│               └─→ \"SNR 밸런스가 좋아서\"\n│                   │\n│                   └─→ \"SNR이 뭔데?\"\n│                       │\n│                       └─→ Signal-to-Noise Ratio\n│                           │\n│                           └─→ 【신호처리】 갑자기 등장\n│                               │\n│                               └─→ 전자공학과 인사하세요\n│\n├─→ 【노이즈 스케줄】\n│   │\n│   ├─→ \"Linear Schedule\"\n│   │   │\n│   │   └─→ \"DDPM 기본\"\n│   │\n│   ├─→ \"Cosine Schedule\"\n│   │   │\n│   │   └─→ \"이미지 끝부분이 더 좋아짐\"\n│   │       │\n│   │       └─→ \"왜?\"\n│   │           │\n│   │           └─→ \"SNR 분포가 더 균일해서\"\n│   │               │\n│   │               └─→ (또 SNR)\n│   │\n│   └─→ \"Learned Schedule\"\n│       │\n│       └─→ \"그냥 학습시키자\"\n│           │\n│           └─→ (모든 것의 해결책: 학습시키자)\n│\n├─→ 【샘플러】 어떻게 생성하나\n│   │\n│   ├─→ \"DDPM (원조)\"\n│   │   │\n│   │   └─→ \"1000 스텝, 느림\"\n│   │       │\n│   │       └─→ \"왜 1000스텝이나?\"\n│   │           │\n│   │           └─→ \"이론적으로 무한 스텝이 맞는데...\"\n│   │               │\n│   │               └─→ 【이산화 오차】\n│   │                   │\n│   │                   └─→ 【수치해석학】\n│   │                       │\n│   │                       └─→ 오일러 방법\n│   │                       └─→ 안정성 분석\n│   │                       └─→ 여기서 또 2주\n│   │\n│   ├─→ \"DDIM\"\n│   │   │\n│   │   └─→ \"같은 모델인데 50스텝으로 가능\"\n│   │       │\n│   │       └─→ \"어떻게?\"\n│   │           │\n│   │           └─→ \"Non-Markovian으로 바꾸면\"\n│   │               │\n│   │               └─→ \"Non-Markovian이 뭔데?\"\n│   │                   │\n│   │                   └─→ \"과거 상태도 참고한다는 거야\"\n│   │                       │\n│   │                       └─→ \"근데 어떻게 수학적으로...\"\n│   │                           │\n│   │                           └─→ 【ODE 관점】\n│   │                               │\n│   │                               └─→ Probability Flow ODE\n│   │                                   │\n│   │                                   └─→ 【상미분방정식】\n│   │                                       │\n│   │                                       └─→ 여기서 또...\n│   │\n│   ├─→ \"DPM-Solver, DPM-Solver++\"\n│   │   │\n│   │   └─→ \"20스텝으로도 됨\"\n│   │       │\n│   │       └─→ \"어떻게?\"\n│   │           │\n│   │           └─→ \"고차 ODE 솔버 쓰면\"\n│   │               │\n│   │               └─→ 【수치해석학】 심화\n│   │                   │\n│   │                   ├─→ Runge-Kutta\n│   │                   ├─→ Multistep Methods\n│   │                   └─→ 학부 수치해석 전체\n│   │\n│   └─→ \"Consistency Models, Flow Matching, ...\"\n│       │\n│       └─→ \"1-4스텝으로도...\"\n│           │\n│           └─→ 【최신 연구】\n│               │\n│               └─→ 매달 새 논문 나옴\n│                   │\n│                   └─→ (따라가기 포기)\n│\n├─→ 【SDE/ODE 관점】 고급 이론\n│   │\n│   └─→ \"Diffusion은 SDE의 이산화야\"\n│       │\n│       └─→ \"SDE가 뭔데?\"\n│           │\n│           └─→ 【확률미분방정식】\n│               │\n│               ├─→ Wiener Process\n│               ├─→ Ito Calculus\n│               │   │\n│               │   └─→ (일반 미적분이 안 통함)\n│               │       │\n│               │       └─→ Ito's Lemma\n│               │           │\n│               │           └─→ 금융공학에서 옴\n│               │               │\n│               │               └─→ Black-Scholes\n│               │                   │\n│               │                   └─→ 노벨 경제학상\n│               │                       │\n│               │                       └─→ ???\n│               │\n│               ├─→ Fokker-Planck Equation\n│               │   │\n│               │   └─→ 【편미분방정식】\n│               │\n│               └─→ 여기서 한 학기\n│\n├─→ 【조건부 생성 & Guidance】\n│   │\n│   ├─→ \"Classifier Guidance\"\n│   │   │\n│   │   └─→ \"분류기 gradient로 유도\"\n│   │       │\n│   │       └─→ \"분류기를 따로 학습해야 함\"\n│   │           │\n│   │           └─→ \"귀찮음\"\n│   │\n│   └─→ \"Classifier-Free Guidance (CFG)\"\n│       │\n│       └─→ \"조건부/무조건부 예측을 섞어\"\n│           │\n│           └─→ \"ε_cfg = ε_uncond + w·(ε_cond - ε_uncond)\"\n│               │\n│               └─→ \"왜 이게 되는 건데?\"\n│                   │\n│                   └─→ \"Score 관점에서 보면...\"\n│                       │\n│                       └─→ ∇log p(x|c) ∝ ∇log p(x) + ∇log p(c|x)\n│                           │\n│                           └─→ 【베이즈 정리】\n│                               │\n│                               └─→ 여기서 암묵적 분류기가 나옴\n│                                   │\n│                                   └─→ 수식 전개 2페이지\n│                                       │\n│                                       └─→ (아름다움)\n│\n└─→ 【백본 아키텍처】 실제 네트워크\n    │\n    ├─→ \"UNet\"\n    │   │\n    │   └─→ \"Encoder-Decoder + Skip Connection\"\n    │       │\n    │       └─→ \"왜 UNet?\"\n    │           │\n    │           └─→ \"의료 영상에서 잘 됐으니까\"\n    │               │\n    │               └─→ (역사적 우연)\n    │\n    └─→ \"DiT (Diffusion Transformer)\"\n        │\n        └─→ \"ViT를 Diffusion에 적용\"\n            │\n            └─→ \"ViT 알아야 함\"\n                │\n                └─→ (위로 돌아가세요)\n                    │\n                    └─→ 【무한 루프 완성】\n여기까지가 Diffusion \"입문\"입니다축하합니다! 이제 Stable Diffusion이 왜 느린지 설명할 수 있습니다.3. 그래서 이거 다 알아야 해?정직한 답변\"다 알아야 하나요?\"\n\n├─→ 논문 쓰려면: 예\n├─→ 모델 구현하려면: 대부분\n├─→ 파인튜닝 하려면: 일부\n└─→ API 호출하려면: 아니오\n\n    response = client.images.generate(\n        model=\"dall-e-3\",\n        prompt=\"a cat\"\n    )\n    # 끝\n현실적 학습 시간목표\n소요 시간\n알아야 하는 것\nAPI 사용\n1일\n없음\n파인튜닝\n1-2주\n터미널 명령어\n코드 수정\n1-3개월\n위 트리의 1단계들\n논문 이해\n6개월-1년\n위 트리의 2단계들\n논문 작성\n2-5년\n위 트리 전체 + α\n완전 이해\n???\n불가능 (진지)\n분야별 필요 지식 요약【필수】\n├── 선형대수: 행렬, 내적, 고유값\n├── 미적분: 편미분, 연쇄법칙\n├── 확률론: 가우시안, 조건부 확률, 기댓값\n└── 최적화: SGD, Adam\n\n【알면 좋음】\n├── 정보이론: 엔트로피, KL Divergence\n├── 수리통계학: MLE, 변분추론\n└── 수치해석: 이산화, ODE 솔버\n\n【깊이 파려면】\n├── 확률미분방정식: Ito Calculus\n├── 통계물리학: Langevin Dynamics\n├── 함수해석학: 힐베르트 공간\n└── 미분기하학: 다양체 (Flow Matching용)\n\n【실제로 쓰는 것】\n└── model.fit()\n4. 의존성 지옥 탈출 가이드옵션 A: 하향식 포기# 그냥 쓰세요\nfrom diffusers import StableDiffusionPipeline\npipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\nimage = pipe(\"a cat\").images[0]\n옵션 B: 필요할 때 파기1. 일단 코드 돌려봄\n2. 에러 나면 그 부분만 공부\n3. 반복\n4. 어느새 반쯤 이해하고 있음\n옵션 C: 정통파 (비추천)선형대수 (3개월)\n    ↓\n미적분 (2개월)\n    ↓\n확률론 (3개월)\n    ↓\n머신러닝 기초 (3개월)\n    ↓\n딥러닝 (3개월)\n    ↓\nCNN/RNN (2개월)\n    ↓\nTransformer (2개월)\n    ↓\nViT (1개월)\n    ↓\n생성모델 이론 (2개월)\n    ↓\nDiffusion (2개월)\n    ↓\n최신 논문들 (∞)\n\n총 소요: 2-3년\n\n그 사이에 새로운 패러다임 등장 (망)\n옵션 D: 하이브리드 (추천)1. 먼저 큰 그림 파악 (이 문서 읽기)\n2. 코드로 돌려보면서 직관 형성 (1-2주)\n3. \"이게 왜 되지?\" 궁금할 때마다 해당 부분 파기\n4. 수학은 필요할 때 그때그때\n5. 논문은 관련 부분만 선택적으로\n5. 자주 묻는 질문\"수학 몰라도 돼요?\"API 호출: 됨\n파인튜닝: 됨\n코드 수정: 좀 힘듦\n논문 이해: 안 됨\n새로운 거 만들기: 절대 안 됨\n\"어디서 시작해요?\"추천 순서:\n1. 3Blue1Brown 선형대수/미적분 (영상, 무료)\n2. PyTorch 튜토리얼 (실습)\n3. Andrej Karpathy 영상들 (직관)\n4. 논문 구현체 읽기 (실전)\n5. 논문 (나중에)\n\"얼마나 걸려요?\"\"금방이요\" - 거짓말쟁이\n\"2개월이요\" - 천재\n\"6개월이요\" - 낙관주의자\n\"1-2년이요\" - 현실주의자\n\"평생이요\" - 진실\n\"포기해도 돼요?\"됩니다. \nAPI 발전 속도 보면 직접 이해할 필요가 점점 줄어들고 있음.\n다만 \"왜 이게 안 되지?\"를 디버깅하려면 어느 정도는 알아야 함.\n6. 마무리이 문서를 읽고 난 당신의 상태읽기 전: \"ViT랑 Diffusion 공부해야지\"\n읽은 후: \"나는 왜 이 길을 선택했는가\"\n위로의 말아무도 이걸 처음부터 다 알고 시작하지 않았음\n논문 저자들도 Appendix 다 이해 못 함 (진지)\n\"잘 모르겠지만 돌아가니까 쓴다\"가 업계 표준\n완벽히 이해한 사람은 없음, 정도의 차이만 있을 뿐\n그래도 하고 싶다면끈기 + 시간 + 구글링 능력 = 언젠가 됨\n\n화이팅!!! (작성자도 아직 공부 중)\n부록: 추천 자료영상 (무료)3Blue1Brown: 수학 직관\nYannic Kilcher: 논문 리뷰\nAndrej Karpathy: 구현 중심\n블로그Lilian Weng: 정리 최고\nJay Alammar: 시각화 최고\n논문 (순서대로)Attention Is All You Need (2017)\nAn Image is Worth 16x16 Words (2020)\nDDPM (2020)\nDDIM (2021)\nClassifier-Free Guidance (2022)\nDiT (2023)\n코드lucidrains GitHub: 깔끔한 구현\nhuggingface/diffusers: 실용적\ntimm: ViT 구현체들",
  "images": [],
  "url": "https://gall.dcinside.com/mgallery/board/view/?id=thesingularity&no=929636",
  "crawledAt": "2026-01-09T21:38:58.694Z"
}