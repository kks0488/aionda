export type EvergreenSeed = {
  id: string;
  title: string;
  description: string;
  keyInsights: string[];
  researchQuestions: string[];
};

export const EVERGREEN_QUEUE: EvergreenSeed[] = [
  {
    id: 'llm-provider-monitoring',
    title: 'AI 모델/정책 업데이트 모니터링 셋업',
    description: 'OpenAI/Google/Anthropic 등 제공자의 릴리즈 노트·정책 변화를 놓치지 않기 위한 RSS/Changelog/알림 체계를 만든다.',
    keyInsights: [
      '최신성은 “운”이 아니라 시스템이다. RSS/Changelog/깃헙 릴리즈를 한 곳에 모아야 누락이 줄어든다.',
      '모델 버전/가격/제한이 바뀌면 제품 품질과 비용 구조가 같이 흔들리므로, 모니터링은 운영(SRE) 항목에 가깝다.',
    ],
    researchQuestions: [
      '주요 LLM 제공자(OpenAI/Google/Anthropic)의 공식 업데이트 채널(RSS/Changelog/Release notes)은 어디인가?',
      '정책/가격/레이트리밋 변경을 “감지 → 공유 → 롤백/대응”까지 연결하는 운영 루프는 어떻게 설계하는가?',
      '기업 환경에서 외부 벤더 변경 공지를 추적할 때 필요한 기록(감사 로그/변경 이력/영향 분석)은 무엇인가?',
    ],
  },
  {
    id: 'korea-tech-rss-stack',
    title: '한국 AI/개발 블로그 RSS 구독 리스트',
    description: '국내 테크/AI 글을 빠르게 따라잡기 위해 “구독 가능한 RSS/Atom” 중심으로 리스트와 셋업 방법을 정리한다.',
    keyInsights: [
      '국내는 커뮤니티 속도는 빠르지만, 기술 블로그는 “정확한 원문”이 된다. 둘을 같이 보아야 한다.',
      'RSS는 알고리즘 피드보다 “누락”이 적고, 검색형 콘텐츠 아이디어를 안정적으로 공급한다.',
    ],
    researchQuestions: [
      '국내 주요 기술 블로그(NAVER D2, Kakao Tech, Toss Tech 등)에서 제공하는 공식 RSS/Atom URL은 무엇인가?',
      'RSS 리더/슬랙/디스코드/이메일로 팀 단위로 공유하는 추천 워크플로우는 무엇인가?',
      'RSS를 주제별로 분류(LLM/보안/인프라/데이터)할 때 운영이 쉬운 태깅 규칙은 무엇인가?',
    ],
  },
  {
    id: 'llm-selection-checklist',
    title: 'LLM 선택 체크리스트 (GPT/Gemini/Claude)',
    description: '“최신 모델”보다 중요한 건 제품 요구사항이다. 비용·지연·정확·보안·도구콜 기준으로 선택 프레임을 만든다.',
    keyInsights: [
      '한 모델로 끝내지 말고, 난이도/리스크/지연시간에 따라 라우팅하는 편이 품질과 비용이 같이 좋아지는 경우가 많다.',
      '비교는 성능표보다 “실패 모드(환각/거절/지연/비용 폭발)”를 기준으로 해야 실전에 남는다.',
    ],
    researchQuestions: [
      '주요 제공자별 API 기능 차이(툴 호출, 멀티모달, 구조화 출력, 캐시/배치 등)는 무엇인가?',
      '가격/레이트리밋/정책(콘텐츠, 데이터 보관 등)에서 제품/세일즈에 영향을 주는 핵심 항목은 무엇인가?',
      '기업 환경에서 모델 선택 시 보안/컴플라이언스 관점의 필수 체크리스트는 무엇인가?',
    ],
  },
  {
    id: 'prompt-futureproofing',
    title: '프롬프트 “버전 앵커링”을 줄이는 법',
    description: '글/답변이 구형 모델명에 고정돼 낡아 보이는 문제를 줄이기 위한 프롬프트·템플릿·검증 패턴을 정리한다.',
    keyInsights: [
      '모델명은 기능 요구사항의 대체물이 아니다. 프롬프트는 “무엇을 해야 하는지”로 써야 교체가 쉽다.',
      '출처 없는 버전 언급은 최신성을 해치고, 사실성 리스크를 키운다. 필요하면 “시점(As-of)”를 먼저 박아야 한다.',
    ],
    researchQuestions: [
      'LLM 시스템에서 프롬프트를 “모델 독립”으로 설계하기 위한 대표 패턴(요구사항 기반, 정책 분리, 라우팅)은 무엇인가?',
      '모델 버전/가격이 바뀌어도 품질을 유지하기 위한 평가(Evals)·회귀 테스트 구성은 어떻게 하는가?',
      '자동 글 생성 파이프라인에서 “구형 모델명/버전”이 불필요하게 반복되는 경우를 감지하는 정적 규칙은 무엇인가?',
    ],
  },
  {
    id: 'community-to-trusted-playbook',
    title: '커뮤니티 신호를 “근거”로 바꾸는 플레이북',
    description: 'DC/커뮤니티에서 본 신호를 공식·신뢰 출처로 교차검증해 기사/리서치로 바꾸는 절차를 체크리스트로 만든다.',
    keyInsights: [
      '커뮤니티는 “빠르지만 불완전”하다. 핵심은 속도가 아니라, 빠른 신호를 빠른 검증으로 바꾸는 능력이다.',
      '출처 구조를 강제(원문 1 + 신뢰 1+)하면, 자동화가 “똥글 생성기”로 흐르는 것을 막을 수 있다.',
    ],
    researchQuestions: [
      '커뮤니티 기반 토픽을 기사화할 때 필수로 확인해야 하는 “1차 출처” 유형은 무엇인가?',
      '교차검증을 빠르게 하기 위한 검색/필터링/요약 파이프라인(RSS, changelog, GitHub releases)은 어떻게 설계하는가?',
      '출처가 불충분할 때 “글을 버리는 기준”과 “보류로 전환하는 기준”은 어떻게 정의하는가?',
    ],
  },
  {
    id: 'mcp-intro',
    title: 'MCP(Model Context Protocol) 입문 가이드',
    description: 'MCP가 무엇인지, 왜 “에이전트 통합 표준”으로 부상하는지, 도입 시 무엇부터 점검해야 하는지 정리한다.',
    keyInsights: [
      '도구 호출(tool calling)·컨텍스트 주입의 “연결 규격”을 표준화하면, 모델/클라이언트 교체 비용이 줄어든다.',
      '보안·권한·감사 로그가 같이 설계되지 않으면 MCP 도입이 오히려 공격 표면을 넓힐 수 있다.',
    ],
    researchQuestions: [
      'Model Context Protocol(MCP)의 목표와 핵심 구성요소(서버/클라이언트/툴/리소스)는 무엇인가?',
      'MCP에서 권한/승인/감사 로그(guardrails)를 어떻게 설계하는 것이 권장되는가?',
      '기존 “모델별 플러그인/툴콜” 방식 대비 MCP 도입의 운영상 장단점은 무엇인가?',
    ],
  },
  {
    id: 'prompt-injection-defense',
    title: '프롬프트 인젝션 방어 체크리스트',
    description: '현실에서 자주 발생하는 프롬프트 인젝션/데이터 유출 패턴을 정리하고, 제품·에이전트 레벨에서의 방어책을 제시한다.',
    keyInsights: [
      '인젝션은 “모델이 속아서”라기보다, 입력 경계/권한 경계가 없는 시스템 설계 문제로 번지는 경우가 많다.',
      '툴콜·브라우징·파일 접근이 붙는 순간, 방어는 프롬프트가 아니라 정책/권한/검증 레이어로 이동한다.',
    ],
    researchQuestions: [
      '프롬프트 인젝션의 대표적 공격 유형(지시 탈취, 데이터 유출, 정책 우회)은 무엇인가?',
      '주요 LLM 제공자/보안 가이드에서 권장하는 완화 전략(allowlist, sandbox, output validation)은 무엇인가?',
      '툴 사용 에이전트에서 데이터 유출을 막기 위한 권한 모델/검증 체계는 어떻게 설계해야 하는가?',
    ],
  },
  {
    id: 'rag-pipeline-basics',
    title: 'RAG 파이프라인 구성 가이드',
    description: '인덱싱→검색→생성까지의 기본 구성 요소를 정리하고, “정확도”를 떨어뜨리는 흔한 실패 패턴을 피하는 방법을 다룬다.',
    keyInsights: [
      'RAG 품질은 모델 성능보다 “문서 분할·메타데이터·검색 쿼리” 설계에 더 크게 좌우된다.',
      '권한/버전/변경이력 같은 운영 메타데이터가 없으면, 답변은 맞아도 서비스는 위험해진다.',
    ],
    researchQuestions: [
      'RAG의 핵심 구성요소(ingestion, chunking, embedding, retrieval, generation)에서 흔한 실패 원인은 무엇인가?',
      '검색 단계에서 precision/recall을 개선하기 위한 대표 기법(하이브리드 검색, reranking)은 무엇인가?',
      '운영 환경에서 RAG 문서 업데이트/삭제/권한 변경을 안전하게 반영하는 방법은 무엇인가?',
    ],
  },
  {
    id: 'rag-evals',
    title: 'RAG 평가(Evals) 실전 가이드',
    description: 'RAG는 “그럴듯함”이 아니라 재현 가능한 평가로 관리해야 한다. 실무에서 통하는 평가 지표와 운영 루프를 정리한다.',
    keyInsights: [
      '정답률만 보면 실패한다. 근거 적합성(grounding)·출처 일치·회수율을 분리해 봐야 한다.',
      '평가는 데이터셋이 아니라 “업무 질문”에서 시작해야 유지된다.',
    ],
    researchQuestions: [
      'RAG 평가에서 자주 쓰이는 지표(groundedness, faithfulness, retrieval recall 등)는 무엇인가?',
      '자동 평가(LLM-as-judge)에서 편향/누수/과적합 위험을 줄이는 방법은 무엇인가?',
      '프로덕션에서 RAG 품질을 모니터링/회귀 테스트하는 운영 패턴은 무엇인가?',
    ],
  },
  {
    id: 'agent-memory',
    title: 'LLM 에이전트 메모리 설계 가이드',
    description: '에이전트의 “기억”은 대화 저장이 아니라 제품 기능이다. 단기/장기/업무 메모리를 어떻게 나눠 설계할지 정리한다.',
    keyInsights: [
      '무제한 저장은 리스크다. 저장 범위·보존 기간·삭제 정책을 먼저 정의해야 한다.',
      '메모리는 모델이 아니라 “검색/요약/적재/회수” 파이프라인의 문제다.',
    ],
    researchQuestions: [
      '에이전트 메모리의 대표 패턴(요약 메모리, 벡터 메모리, 규칙 기반 메모리)은 무엇인가?',
      '개인정보/민감정보가 포함될 수 있는 메모리를 안전하게 저장/삭제하기 위한 원칙은 무엇인가?',
      '메모리 회수(retrieval)가 오류를 만드는 대표 원인과 완화 방법은 무엇인가?',
    ],
  },
  {
    id: 'tool-calling-guardrails',
    title: '툴콜링(도구 호출) 권한 모델 정리',
    description: '에이전트가 외부 API·DB·파일을 만질수록 “무엇을 할 수 있는가”를 코드로 제한해야 한다. 권한 모델을 실무 관점에서 정리한다.',
    keyInsights: [
      '툴콜은 “기능”이 아니라 “권한”이다. 호출 가능 도구를 최소화하고 목적별로 분리해야 한다.',
      '출력 검증(JSON schema 등) 없이 자동 실행을 붙이면 작은 오류가 큰 사고로 이어진다.',
    ],
    researchQuestions: [
      '툴콜 에이전트에서 권한을 최소화(least privilege)하기 위한 대표 설계는 무엇인가?',
      '출력 포맷 강제/검증(JSON schema, validators)로 안전성을 높이는 방법은 무엇인가?',
      '감사 로그/리플레이(재현) 체계를 구축할 때 필수로 남겨야 하는 메타데이터는 무엇인가?',
    ],
  },
  {
    id: 'llm-cost-optimization',
    title: 'LLM 비용 최적화 가이드',
    description: '토큰 비용은 곧 제품 마진이다. 캐시·배치·요약·라우팅으로 비용을 낮추는 실전 전략을 정리한다.',
    keyInsights: [
      '비용 최적화는 “덜 쓰기”가 아니라 “같은 품질을 더 싸게”다. 품질 지표와 함께 움직여야 한다.',
      '라우팅(간단 질의→저가 모델, 복잡 질의→고성능 모델)은 가장 큰 레버지만 실패하면 품질이 무너진다.',
    ],
    researchQuestions: [
      'LLM 비용을 줄이는 대표 전략(캐시, 배치, 요약, 라우팅)의 효과/리스크는 무엇인가?',
      '프롬프트 길이/컨텍스트 설계가 비용과 품질에 미치는 영향은 무엇인가?',
      '프로덕션에서 비용과 품질을 동시에 모니터링하는 지표 설계는 어떻게 해야 하는가?',
    ],
  },
  {
    id: 'model-routing',
    title: '모델 라우팅 전략: 비용과 성능을 섞는 법',
    description: '한 모델로 끝내지 않는 시대다. 업무 난이도·리스크·지연시간에 따라 모델을 분기하는 라우팅 전략을 정리한다.',
    keyInsights: [
      '라우터는 “분류기”다. 분류 실패가 곧 품질/비용 폭발로 이어진다.',
      '에러 버짓과 사용자 기대치가 다르므로, 기능별 SLA를 먼저 정의해야 한다.',
    ],
    researchQuestions: [
      '모델 라우팅(다중 모델 오케스트레이션)에서 일반적으로 쓰이는 분기 기준은 무엇인가?',
      '라우팅의 대표 실패 모드(난이도 오판, 긴급도 오판, 정책 위반)를 줄이는 방법은 무엇인가?',
      '기업 환경에서 모델 라우팅을 적용할 때 거버넌스/감사 요구사항은 무엇인가?',
    ],
  },
  {
    id: 'finetune-vs-rag',
    title: '파인튜닝 vs RAG 선택 가이드',
    description: '“학습할 것인가, 검색할 것인가”는 제품 전략이다. 비용·품질·운영 복잡도를 기준으로 선택 프레임을 제시한다.',
    keyInsights: [
      'RAG는 최신성/출처를 얻고, 파인튜닝은 문체/행동을 고정한다. 목적이 다르다.',
      '둘을 섞을 때는 평가 루프가 없으면 재현성이 무너진다.',
    ],
    researchQuestions: [
      '파인튜닝과 RAG가 각각 강점을 가지는 문제 유형은 무엇인가?',
      '파인튜닝 데이터 품질/보안/라이선스 측면에서 주의해야 할 점은 무엇인가?',
      '하이브리드(RAG+파인튜닝) 설계에서 흔한 함정과 운영 방법은 무엇인가?',
    ],
  },
  {
    id: 'hallucination-playbook',
    title: 'LLM 환각 줄이기 실전 플레이북',
    description: '환각은 “모델 문제”로만 보면 해결이 늦다. 제품·프롬프트·검증 레이어로 나눠 줄이는 방법을 정리한다.',
    keyInsights: [
      '환각은 입력 불충분/검색 실패/출력 포맷 실패 등 시스템 결함이 겹쳐서 발생한다.',
      '“모른다”를 말하게 만들려면, 실패를 허용하는 UX와 정책이 먼저다.',
    ],
    researchQuestions: [
      'LLM 환각을 줄이기 위한 대표 접근(검색 기반 grounding, self-check, verification)은 무엇인가?',
      '출력 포맷 강제/검증이 환각을 줄이는 데 어떤 역할을 하는가?',
      '프로덕션에서 환각을 측정하고 회귀를 막는 운영 지표는 무엇인가?',
    ],
  },
  {
    id: 'on-device-ai-npu',
    title: '온디바이스 AI/NPU 기초 정리',
    description: '온디바이스 AI는 개인정보·지연시간·비용 구조를 바꾼다. NPU의 역할과 제품 설계 포인트를 정리한다.',
    keyInsights: [
      '온디바이스는 “빠름”만이 아니라 “데이터 경계”를 바꾼다. 어떤 데이터가 기기 밖으로 나가면 안 되는지부터 정의해야 한다.',
      '모델 최적화(양자화/프루닝/캐시) 없이는 배터리·발열·성능을 동시에 만족시키기 어렵다.',
    ],
    researchQuestions: [
      'NPU/가속기의 역할과 GPU/CPU 대비 장단점은 무엇인가?',
      '온디바이스 모델 배포에서 대표 최적화(양자화, distillation)와 트레이드오프는 무엇인가?',
      '온디바이스/클라우드 하이브리드 제품 설계에서 흔한 설계 패턴은 무엇인가?',
    ],
  },
  {
    id: 'agent-observability',
    title: '에이전트 관측성(Observability) 구축 가이드',
    description: '에이전트는 “왜 그렇게 했는지”가 중요하다. 로그/트레이스/리플레이를 어떻게 설계할지 정리한다.',
    keyInsights: [
      '관측성이 없으면, 동일한 버그를 같은 비용으로 계속 재현하게 된다.',
      '프롬프트/툴콜/검색 결과/결정 기준을 같이 저장해야 디버깅이 된다.',
    ],
    researchQuestions: [
      '에이전트 시스템에서 필수로 수집해야 하는 관측성 데이터(프롬프트, 툴콜, 검색 결과 등)는 무엇인가?',
      '개인정보/보안 이슈를 고려한 로그 마스킹/보관 정책은 어떻게 설계해야 하는가?',
      '리플레이(재현) 가능한 로그를 남기기 위한 최소 메타데이터는 무엇인가?',
    ],
  },
  {
    id: 'structured-output',
    title: 'LLM 구조화 출력(JSON) 강제 가이드',
    description: '모델이 “그럴듯한 글”이 아니라 “검증 가능한 출력”을 내게 하려면 포맷을 강제해야 한다. JSON 스키마/검증을 중심으로 정리한다.',
    keyInsights: [
      '구조화 출력은 품질 개선보다 “안전한 자동화”를 위해 필요하다.',
      '스키마 검증 실패를 복구하는 재시도 전략이 없으면, 자동화 파이프라인이 쉽게 깨진다.',
    ],
    researchQuestions: [
      'LLM 출력 포맷을 구조화(JSON schema 등)하는 대표 접근은 무엇인가?',
      '스키마 검증 실패 시 재시도/수정 루프를 설계하는 방법은 무엇인가?',
      '구조화 출력이 보안(인젝션/권한)과 운영(관측성/테스트)에 주는 효과는 무엇인가?',
    ],
  },
  {
    id: 'rag-access-control',
    title: '권한 기반 RAG(문서 권한 유지) 정리',
    description: 'RAG에서 가장 위험한 문제는 “정답률”이 아니라 권한 없는 문서가 섞이는 것이다. 권한 기반 검색 설계를 정리한다.',
    keyInsights: [
      '검색은 권한을 “상속”해야 한다. 생성 단계에서 마스킹하는 방식은 늦다.',
      '권한/테넌시를 벡터 인덱스 구조에 반영하지 않으면 운영 난이도가 급상승한다.',
    ],
    researchQuestions: [
      '권한 기반 검색(ABAC/RBAC)을 RAG 파이프라인에 적용하는 대표 패턴은 무엇인가?',
      '테넌트 분리(멀티테넌시)에서 인덱스/쿼리 설계의 트레이드오프는 무엇인가?',
      '권한 누수를 탐지하기 위한 테스트/감사(샘플링, 리플레이) 방법은 무엇인가?',
    ],
  },
  {
    id: 'vector-db-choosing',
    title: '벡터DB 선택 기준(정확도·비용·운영)',
    description: 'RAG에 벡터DB를 붙이기 전, 무엇을 기준으로 비교해야 하는지 정리한다. 성능보다 운영(백업/권한/모니터링)을 먼저 본다.',
    keyInsights: [
      '“검색 품질”은 DB 이름이 아니라 인덱싱/스키마/쿼리 설계에서 갈린다.',
      '운영 요구(백업, 삭제, 권한, 비용 예측)가 충족되지 않으면 결국 교체한다.',
    ],
    researchQuestions: [
      '벡터 검색 시스템에서 인덱스 타입/파라미터가 성능에 미치는 영향은 무엇인가?',
      '운영 관점에서 반드시 확인해야 할 체크리스트(백업, 삭제, 권한, 모니터링)는 무엇인가?',
      '하이브리드 검색(키워드+벡터)과 reranking 조합은 어떤 상황에서 유리한가?',
    ],
  },
  {
    id: 'reranking-playbook',
    title: 'Retrieval Reranking 적용 플레이북',
    description: 'RAG에서 “찾았는데도 틀린다”는 대부분 reranking으로 개선된다. 적용 포인트와 실패 패턴을 정리한다.',
    keyInsights: [
      'reranking은 품질을 올리지만 지연시간과 비용을 늘린다. 적용 지점이 중요하다.',
      '평가 데이터가 없으면 reranker는 “좋아진 것처럼 보이는” 착시를 만든다.',
    ],
    researchQuestions: [
      'reranking 모델의 역할과 1차 검색 대비 기대 효과는 무엇인가?',
      'reranking을 도입할 때 지연시간/비용을 관리하는 전략(캐시, 후보 수 제한)은 무엇인가?',
      'reranking 품질을 검증하기 위한 평가 셋/지표는 어떻게 구성하는가?',
    ],
  },
  {
    id: 'eval-dataset',
    title: 'LLM 평가 데이터셋 만드는 법(사내 QA→Evals)',
    description: '벤치마크는 남의 일이다. 사내 질문/CS/운영 로그를 “평가 자산”으로 바꾸는 방법을 정리한다.',
    keyInsights: [
      '평가셋은 정답보다 “의도/채점 기준”이 핵심이다.',
      '업데이트마다 회귀가 생기므로, 최소한의 자동화된 회귀 테스트가 필요하다.',
    ],
    researchQuestions: [
      '실무 평가셋을 구성할 때 필요한 필드(의도, 정답, 채점 기준, 난이도)는 무엇인가?',
      'LLM-as-judge를 쓰는 경우 신뢰도를 높이기 위한 가이드라인은 무엇인가?',
      '프로덕션 업데이트에서 회귀 테스트를 운영하는 대표 패턴은 무엇인가?',
    ],
  },
  {
    id: 'open-source-serving',
    title: '오픈소스 LLM 배포(Serving) 가이드',
    description: '오픈소스 모델을 직접 띄우는 순간, 성능과 비용은 엔지니어링 문제가 된다. 추론 서버 선택과 운영 포인트를 정리한다.',
    keyInsights: [
      '서빙은 모델만이 아니라 배치/캐시/토크나이저/스트리밍까지 포함한 시스템이다.',
      '모델 교체가 잦으면, 인터페이스 표준화(OpenAI-compatible 등)가 운영 비용을 줄인다.',
    ],
    researchQuestions: [
      '오픈소스 LLM 서빙에서 널리 쓰이는 추론 서버/런타임의 특징은 무엇인가?',
      '배치 처리, KV 캐시, 스트리밍이 지연시간/비용에 미치는 영향은 무엇인가?',
      '운영 관점(롤백, 버전 관리, 모니터링)에서 필수로 준비할 항목은 무엇인가?',
    ],
  },
  {
    id: 'gpu-inference-optimization',
    title: 'GPU 추론 최적화 기초: 지연·비용 줄이기',
    description: 'LLM 서비스는 결국 GPU 시간 싸움이다. 추론 최적화의 기본(배치·캐시·양자화)을 정리한다.',
    keyInsights: [
      '지연시간은 “모델”뿐 아니라 토크나이저/네트워크/큐잉에서 크게 튄다.',
      '양자화는 비용을 낮추지만, 품질 저하를 평가로 통제해야 한다.',
    ],
    researchQuestions: [
      'LLM 추론 지연시간을 구성하는 주요 요소(프리필/디코딩, 배치, 큐잉)는 무엇인가?',
      'KV 캐시 및 배치 전략이 처리량/지연시간에 미치는 영향은 무엇인가?',
      '양자화/프루닝/디스틸레이션의 대표 트레이드오프는 무엇인가?',
    ],
  },
  {
    id: 'governance-enterprise',
    title: '기업용 AI 거버넌스(정책/감사) 체크리스트',
    description: 'AI를 “썼다”가 아니라 “운영한다”로 넘어가면 거버넌스가 필요하다. 정책·감사·승인 프로세스를 정리한다.',
    keyInsights: [
      '거버넌스는 혁신을 막는 문서가 아니라, 실패 비용을 낮추는 장치다.',
      '권한/로그/삭제 요청이 제품 기능으로 들어가지 않으면 규정 대응이 어려워진다.',
    ],
    researchQuestions: [
      '기업 환경에서 AI 거버넌스에 포함되는 대표 항목(데이터, 보안, 책임, 감사)은 무엇인가?',
      '모델/프롬프트 변경에 대한 승인·롤백·감사 체계를 어떻게 구축하는가?',
      '사용자 데이터 처리(보관/삭제/옵트아웃) 관점에서 필요한 기능은 무엇인가?',
    ],
  },
  {
    id: 'agent-testing',
    title: '에이전트 테스트 하네스 구축 가이드',
    description: '에이전트는 테스트 없이는 운영이 불가능하다. 시나리오 테스트, 회귀, 시뮬레이션을 어떻게 구축할지 정리한다.',
    keyInsights: [
      '테스트는 “정답”보다 “실패 패턴”을 재현하는 데 초점을 둬야 한다.',
      '툴콜/검색/메모리까지 포함한 통합 테스트가 없으면 배포할수록 불안해진다.',
    ],
    researchQuestions: [
      '에이전트 테스트에서 시나리오/상태/툴 모킹을 설계하는 대표 방법은 무엇인가?',
      '회귀 테스트를 자동화하기 위한 데이터셋/리플레이 접근은 무엇인가?',
      '에이전트 평가에서 흔히 쓰이는 지표(성공률, 비용, 지연, 안전)를 어떻게 정의하는가?',
    ],
  },
  {
    id: 'prompt-template-versioning',
    title: '프롬프트 템플릿 버전 관리 방법',
    description: '프롬프트는 코드다. 변경 이력·실험·롤백·환경별 설정을 어떻게 관리하는지 정리한다.',
    keyInsights: [
      '“프롬프트 파일 하나”로는 운영이 안 된다. 버전/실험/가드레일이 필요하다.',
      '템플릿과 정책(금지/허용)은 분리해야 유지 보수가 된다.',
    ],
    researchQuestions: [
      '프롬프트를 코드처럼 버전 관리하기 위한 대표 패턴(템플릿, 변수, 환경 분리)은 무엇인가?',
      'A/B 테스트와 롤백을 포함한 프롬프트 실험 운영 방법은 무엇인가?',
      '안전 정책(금지/허용)과 프롬프트 텍스트를 분리하는 이유와 방법은 무엇인가?',
    ],
  },
  {
    id: 'ai-monitoring-metrics',
    title: 'LLM/에이전트 운영 지표(모니터링) 정리',
    description: 'DAU가 아니라 “품질과 비용”이 생존을 가른다. 프로덕션에서 꼭 봐야 하는 운영 지표를 정리한다.',
    keyInsights: [
      '품질 지표(정확도/근거/안전)와 비용 지표(토큰/지연/실패율)를 같이 봐야 한다.',
      '지표가 없으면 최적화가 아니라 “감”으로 운영하게 된다.',
    ],
    researchQuestions: [
      'LLM 서비스에서 일반적으로 모니터링하는 핵심 지표(지연, 비용, 성공률, 안전)는 무엇인가?',
      '환각/근거 부적합을 탐지하기 위한 실무적 방법(샘플링, 라벨링, 자동 평가)은 무엇인가?',
      '경보(알림)와 대응(롤백/차단)을 연결하는 운영 프로세스는 어떻게 설계하는가?',
    ],
  },
  {
    id: 'fine-tuning-data-licensing',
    title: '파인튜닝 데이터 라이선스/저작권 체크',
    description: '모델을 학습시키는 순간, 데이터는 법·계약·컴플라이언스 문제가 된다. 실무에서 놓치기 쉬운 체크리스트를 정리한다.',
    keyInsights: [
      '데이터는 기술 자산이자 법적 리스크다. 출처/권한/보관을 같이 관리해야 한다.',
      '외부 데이터는 “썼다”가 아니라 “증명했다”가 중요하다.',
    ],
    researchQuestions: [
      '파인튜닝 데이터 수집/사용에서 일반적으로 요구되는 라이선스/동의/계약 요소는 무엇인가?',
      '저작권/개인정보가 섞일 수 있는 데이터셋을 안전하게 관리하는 방법은 무엇인가?',
      '기업에서 데이터 출처 증빙과 감사 가능성을 확보하기 위한 운영 방법은 무엇인가?',
    ],
  },
  {
    id: 'korea-policy-monitoring',
    title: '국내 AI 정책/규제 모니터링 방법',
    description: '정책은 느리게 움직이지만, 한번 바뀌면 제품이 멈춘다. 국내 AI 규제/가이드 변화 감지 방법을 정리한다.',
    keyInsights: [
      '정책 변경은 기술보다 “배포/운영”에 먼저 영향을 준다.',
      '공식 문서·보도자료·가이드라인을 한곳에서 추적하는 루틴이 필요하다.',
    ],
    researchQuestions: [
      '국내에서 AI 관련 정책/가이드라인을 발표하는 주요 기관과 공식 채널은 무엇인가?',
      '제품팀이 정책 변경을 놓치지 않기 위한 모니터링/알림 운영 방법은 무엇인가?',
      '정책 대응을 위한 최소 문서(데이터 흐름도, 위험 평가, 로그 정책)는 무엇인가?',
    ],
  },
  {
    id: 'korean-llm-landscape',
    title: '한국어 LLM 생태계 지도(국내·글로벌)',
    description: '한국어 사용자를 위한 LLM 선택을 “모델명”이 아니라 데이터·안전·비용·배포 조건으로 정리한다.',
    keyInsights: [
      '한국어 품질은 평균 점수보다 “업무 도메인(법/금융/CS) 적합성”에서 갈린다.',
      '국내/글로벌 모델은 정책·데이터 이전·지원 체계까지 포함해 비교해야 한다.',
    ],
    researchQuestions: [
      '한국어 LLM 비교에서 자주 쓰이는 평가 항목(문장 자연스러움, 사실성, 지시 따르기)은 무엇인가?',
      '기업/개인이 모델을 선택할 때 확인해야 할 정책/데이터 보관/지역(리전) 조건은 무엇인가?',
      '한국어 최적화(토크나이저/프롬프트/평가셋)에서 가장 흔한 함정은 무엇인가?',
    ],
  },
  {
    id: 'korean-ai-law-basics',
    title: '국내 AI 규제/가이드 “핵심만” 읽는 법',
    description: '법·가이드 문서를 전부 읽지 않아도 된다. 제품팀이 놓치면 안 되는 체크포인트를 정리한다.',
    keyInsights: [
      '정책 문서는 “의도”보다 “요구되는 증빙”이 핵심이다.',
      '데이터 흐름/목적/보관 기간/권한을 문서화하면 대응 속도가 올라간다.',
    ],
    researchQuestions: [
      '국내에서 AI 관련 가이드/정책이 요구하는 공통 요소(투명성, 안전성, 개인정보)는 무엇인가?',
      'AI 제품에서 특히 문제가 되는 영역(민감정보, 자동 의사결정, 미성년자)에는 어떤 요구가 붙는가?',
      '정책 준수를 위해 제품팀이 준비해야 하는 최소 산출물(로그, 위험평가, 공지)은 무엇인가?',
    ],
  },
  {
    id: 'korean-llm-evals',
    title: '한국어 LLM 평가(Evals) 설계 가이드',
    description: '영문 벤치마크를 가져와도 한국어 서비스에서는 빗나간다. 한국어 평가를 “업무 질문”으로 구성하는 방법을 정리한다.',
    keyInsights: [
      '한국어 평가는 번역 품질 문제가 아니라 “의도/존댓말/문맥” 문제로 무너진다.',
      '사내 데이터에서 평가셋을 뽑으면 회귀 테스트로 오래 간다.',
    ],
    researchQuestions: [
      '한국어 LLM 평가에서 자주 실패하는 유형(존댓말, 다의어, 문맥 유지)은 무엇인가?',
      '사내 로그/CS에서 평가셋을 만들 때 개인정보를 안전하게 제거하는 방법은 무엇인가?',
      '한국어 평가에서 자동 채점(LLM-as-judge)의 신뢰도를 높이는 방법은 무엇인가?',
    ],
  },
  {
    id: 'multimodal-rag',
    title: '멀티모달 RAG(이미지/문서) 설계 가이드',
    description: 'PDF/이미지/스캔 문서를 다루는 순간, OCR→구조화→검색→근거 생성의 파이프라인으로 바뀐다.',
    keyInsights: [
      '멀티모달 RAG의 병목은 모델이 아니라 문서 전처리(OCR/레이아웃)에서 생긴다.',
      '근거 링크를 “페이지/좌표”로 남겨야 검증과 CS가 가능해진다.',
    ],
    researchQuestions: [
      'PDF/이미지 문서에서 RAG를 구축할 때 필요한 전처리 단계(OCR, 레이아웃 분석)는 무엇인가?',
      '멀티모달 검색에서 메타데이터(페이지, 섹션, 표/그림)를 어떻게 설계하는가?',
      '근거 제시를 위해 “출처 단위”(페이지/문단)로 회수하는 대표 패턴은 무엇인가?',
    ],
  },
  {
    id: 'agent-sandboxing',
    title: '에이전트 샌드박싱(권한 격리) 실전',
    description: '툴을 붙인 순간, LLM은 “프로그램”이 된다. 파일/네트워크/DB 권한을 격리하는 방법을 정리한다.',
    keyInsights: [
      '샌드박싱은 프롬프트가 아니라 실행 환경(OS/컨테이너/네트워크)에서 결정된다.',
      '권한을 “최소화”하지 않으면 인젝션은 기능 버그가 아니라 보안 사고가 된다.',
    ],
    researchQuestions: [
      '에이전트 실행 환경에서 권한을 격리하기 위한 대표 기법(컨테이너, 네트워크 정책)은 무엇인가?',
      '툴 호출 allowlist/denylist와 실행 전 승인(HITL)을 언제 적용해야 하는가?',
      '감사 로그와 리플레이를 위한 최소 기록 단위는 무엇인가?',
    ],
  },
  {
    id: 'llm-caching-strategy',
    title: 'LLM 비용 절감: 캐시/메모이제이션 전략',
    description: '토큰 최적화보다 먼저 볼 것은 캐시다. “어떤 응답을 캐시할 수 있는가”부터 정리한다.',
    keyInsights: [
      '캐시는 비용만 줄이는 게 아니라 지연시간과 안정성도 같이 올린다.',
      '캐시 키 설계가 틀리면, 절감이 아니라 오류(오답 재사용)가 된다.',
    ],
    researchQuestions: [
      'LLM 응답 캐시를 안전하게 적용하기 위한 조건(입력 정규화, 사용자 분리)은 무엇인가?',
      'RAG/툴콜 환경에서 캐시를 어디에 두는 것이 유리한가(검색 결과 vs 최종 응답)?',
      '캐시 오염/누수를 방지하기 위한 설계(테넌시, 권한, TTL)는 무엇인가?',
    ],
  },
  {
    id: 'prompt-regression-testing',
    title: '프롬프트 회귀 테스트(프롬프트도 CI가 필요하다)',
    description: '프롬프트 변경은 배포다. 테스트 없이는 “좋아졌다”가 아니라 “운 좋았다”가 된다.',
    keyInsights: [
      '프롬프트는 작은 변경에도 행동이 바뀐다. 회귀 테스트로만 통제할 수 있다.',
      '평가셋은 크기보다 “대표 질문”의 품질이 중요하다.',
    ],
    researchQuestions: [
      '프롬프트 회귀 테스트를 설계할 때 필요한 구성(평가셋, 채점, 임계값)은 무엇인가?',
      '자동 채점(LLM-as-judge)과 규칙 기반 체크를 어떻게 조합하는가?',
      '프롬프트 변경 실패 시 롤백/완화 전략은 어떻게 설계하는가?',
    ],
  },
  {
    id: 'pii-redaction-llm',
    title: 'LLM에 넣기 전 개인정보(PII) 마스킹 전략',
    description: '대화/로그/문서를 모델에 넣기 전, PII를 제거하지 않으면 어떤 안전장치도 취약해진다.',
    keyInsights: [
      'PII 마스킹은 모델이 아니라 데이터 파이프라인의 문제다.',
      '완벽한 마스킹은 어렵다. 탐지/삭제/감사 가능성을 같이 설계해야 한다.',
    ],
    researchQuestions: [
      'PII의 범위(식별자, 준식별자)와 서비스에서 자주 누출되는 경로는 무엇인가?',
      '정규식/NER/룰 기반 + LLM 보조 마스킹의 트레이드오프는 무엇인가?',
      '마스킹 품질을 평가하고 운영(감사/재처리)하는 방법은 무엇인가?',
    ],
  },
  {
    id: 'llm-incident-response',
    title: 'LLM 서비스 사고 대응(Incident Response) 플레이북',
    description: '환각/권한 누수/툴 오작동은 “버그”가 아니라 사고다. 탐지→차단→회복의 표준 절차를 정리한다.',
    keyInsights: [
      '사고 대응은 사후가 아니라 사전 설계다. 차단 스위치(kill switch)가 필요하다.',
      '증상(오답)보다 원인(검색/툴콜/프롬프트)을 분리해 기록해야 한다.',
    ],
    researchQuestions: [
      'LLM 서비스에서 실제로 발생하는 사고 유형(권한 누수, 데이터 유출, 툴 오동작)은 무엇인가?',
      '사고 탐지를 위해 필요한 지표/로그/샘플링 설계는 무엇인가?',
      '사고 시 즉시 적용할 수 있는 완화(차단, 롤백, 정책 강화)는 무엇인가?',
    ],
  },
  {
    id: 'llm-api-proxy-gateway',
    title: 'LLM API 프록시/게이트웨이 설계',
    description: '모델이 바뀌는 시대엔 “API를 통일”해야 운영이 산다. 라우팅/키 관리/정책을 한 곳에서 처리하는 방법을 정리한다.',
    keyInsights: [
      '프록시는 비용/키/정책/로그를 한 곳에서 통제하게 해준다.',
      '단, 프록시가 SPOF가 되지 않도록 관측성과 롤백을 같이 설계해야 한다.',
    ],
    researchQuestions: [
      'LLM API 프록시에서 제공해야 할 핵심 기능(라우팅, rate limit, 키 회전)은 무엇인가?',
      'OpenAI-compatible 인터페이스로 표준화할 때의 장단점은 무엇인가?',
      '프록시 레벨에서 정책 집행(툴 사용 제한, PII 차단)을 구현하는 방법은 무엇인가?',
    ],
  },
  {
    id: 'korea-company-tech-sources',
    title: '국내 테크/AI 소스(기술 블로그) 모아보기',
    description: '국내 기업의 기술 블로그/문서에서 AI 관련 시그널을 빠르게 포착하는 루틴을 만든다.',
    keyInsights: [
      '국내 소스는 “한국형 운영 이슈(보안/규제/결제/CS)” 힌트를 준다.',
      'RSS/메일링/보도자료를 한 큐로 묶으면 “놓침”이 줄어든다.',
    ],
    researchQuestions: [
      '국내 주요 테크 기업/연구조직에서 AI 관련 글이 자주 나오는 공식 채널은 무엇인가?',
      '기술 블로그를 RSS 기반으로 모아볼 때의 운영 팁(중복 제거, 키워드 필터)은 무엇인가?',
      '국내 이슈를 글로벌 아티클과 연결해 해석하는 대표 패턴은 무엇인가?',
    ],
  },
  {
    id: 'on-device-llm-quantization',
    title: '온디바이스 LLM: 양자화(Quantization) 체크리스트',
    description: '모바일/엣지에서 LLM을 돌리려면 모델이 아니라 메모리/지연/배터리 제약을 먼저 이해해야 한다.',
    keyInsights: [
      '양자화는 품질 손실과 속도 이득의 교환이다. 목표 KPI부터 정해야 한다.',
      '온디바이스는 업데이트/보안/모델 유출 대응까지 포함한 제품 문제다.',
    ],
    researchQuestions: [
      '온디바이스 LLM에서 자주 쓰이는 양자화 방식(INT8/INT4 등)의 차이는 무엇인가?',
      '지연/메모리/품질을 동시에 관리하기 위한 측정 방법은 무엇인가?',
      '온디바이스 배포에서 모델 유출/프롬프트 인젝션 등 보안 이슈는 어떻게 다루는가?',
    ],
  },
  {
    id: 'llm-latency-playbook',
    title: 'LLM 지연시간(Latency) 최적화 플레이북',
    description: '체감 품질은 정확도보다 “반응 속도”에서 무너진다. 지연을 쪼개 측정하고 줄이는 방법을 정리한다.',
    keyInsights: [
      '지연은 모델이 아니라 “검색/툴/스트리밍/클라이언트” 합산이다.',
      'p95/p99를 보지 않으면, 사용자 경험은 계속 흔들린다.',
    ],
    researchQuestions: [
      'LLM 요청의 지연을 구성 요소별(추론, 검색, 툴, 네트워크)로 분해하는 방법은 무엇인가?',
      '스트리밍/프리페치/캐시로 체감 지연을 줄이는 대표 패턴은 무엇인가?',
      '지연 최적화가 정확도/비용에 주는 트레이드오프는 무엇인가?',
    ],
  },
  {
    id: 'data-freshness-rag',
    title: 'RAG의 “최신성” 문제(데이터 신선도) 다루기',
    description: 'RAG가 맞는데도 틀리게 보이는 순간이 있다. 문서 업데이트·버전·삭제를 설계하는 방법을 정리한다.',
    keyInsights: [
      '데이터 신선도는 검색 품질이 아니라 “인덱싱 운영” 문제다.',
      '버전/유효기간 메타데이터가 없으면 최신 문서가 오래된 답을 이긴다.',
    ],
    researchQuestions: [
      '문서 업데이트/삭제가 잦은 환경에서 RAG 인덱스를 운영하는 대표 패턴은 무엇인가?',
      '문서 버전/유효기간을 검색 결과에 반영하는 방법은 무엇인가?',
      '최신성 회귀를 잡기 위한 테스트(샘플링, 회귀셋) 구성은 어떻게 하는가?',
    ],
  },
  {
    id: 'retrieval-debugging',
    title: '검색 단계 디버깅: “왜 이 문서가 안 잡히나?”',
    description: 'RAG에서 가장 답답한 문제는 검색이 비는 것이다. 원인을 빠르게 분리하는 디버깅 절차를 만든다.',
    keyInsights: [
      '검색 실패는 모델이 아니라 문서 분할/메타데이터/쿼리 변환 문제인 경우가 많다.',
      '디버깅은 샘플링이 아니라 “재현 가능한 로그”에서 시작해야 한다.',
    ],
    researchQuestions: [
      'RAG 검색 실패의 대표 원인(청크, 임베딩, 필터, 쿼리)과 점검 순서는 무엇인가?',
      '검색 로그에 남겨야 하는 필수 메타데이터(쿼리, 필터, 후보, 점수)는 무엇인가?',
      'reranking/하이브리드 검색을 도입해야 하는 신호는 무엇인가?',
    ],
  },
  {
    id: 'tool-validation',
    title: '툴 실행 전 검증(Output Validation) 패턴',
    description: '에이전트가 “말”을 “행동”으로 바꾸는 순간 위험해진다. 실행 전 검증을 어떻게 넣는지 정리한다.',
    keyInsights: [
      '검증은 형식(JSON)만이 아니라 “의도/권한/안전”을 포함해야 한다.',
      '실행 실패를 복구하는 리트라이/보상 트랜잭션이 없으면 자동화는 불안정해진다.',
    ],
    researchQuestions: [
      '툴 실행 전 검증에서 확인해야 할 요소(스키마, 권한, 금지 명령)는 무엇인가?',
      '실행 실패 시 재시도/대체 플랜을 설계하는 방법은 무엇인가?',
      '감사 로그/리플레이를 위한 최소 기록 항목은 무엇인가?',
    ],
  },
  {
    id: 'agent-memory-privacy',
    title: '에이전트 메모리와 개인정보: 저장·삭제·회수',
    description: '메모리를 넣는 순간, 개인정보는 “언제든 튀어나올 수 있는” 상태가 된다. 안전한 운영 원칙을 정리한다.',
    keyInsights: [
      '메모리는 편의 기능이 아니라 규정/보안 기능이다.',
      '저장보다 중요한 건 삭제/만료/감사다.',
    ],
    researchQuestions: [
      '에이전트 메모리에서 개인정보/민감정보를 다룰 때 필요한 정책(동의, 목적 제한)은 무엇인가?',
      '저장/회수 단계에서 권한 누수를 막기 위한 설계 패턴은 무엇인가?',
      '만료/삭제/감사를 자동화하기 위한 운영 방법은 무엇인가?',
    ],
  },
  {
    id: 'open-source-agent-frameworks',
    title: '오픈소스 에이전트 프레임워크 비교 기준',
    description: '프레임워크 선택은 기능 목록이 아니라 “운영 가능성”으로 결정된다. 평가 기준과 질문을 정리한다.',
    keyInsights: [
      '에이전트 프레임워크는 결국 실행기(runner)다. 관측성과 디버깅이 없으면 쓸 수 없다.',
      '툴/메모리/정책이 분리되어야 안전하게 확장된다.',
    ],
    researchQuestions: [
      '에이전트 프레임워크를 평가할 때 중요한 요소(툴콜, 메모리, 정책, 관측성)는 무엇인가?',
      '프로덕션 적용 시 자주 생기는 장애/운영 포인트는 무엇인가?',
      '프레임워크를 교체해도 유지되는 “표준화 포인트”(인터페이스)는 무엇인가?',
    ],
  },
  {
    id: 'korean-ai-hardware-npu',
    title: '국내 AI 하드웨어/NPU 트렌드 읽는 법',
    description: '한국 시장에서는 “서버 GPU”만이 아니라 NPU/엣지/단말이 중요하다. 스펙을 읽는 기준을 정리한다.',
    keyInsights: [
      'NPU는 TOPS보다 “지원 연산/메모리/툴체인”에서 체감이 갈린다.',
      '온디바이스는 배포·업데이트·안전까지 포함한 제품 경쟁이다.',
    ],
    researchQuestions: [
      'NPU/엣지 가속기 평가에서 봐야 할 핵심 항목(정밀도, 메모리, 커널 지원)은 무엇인가?',
      '온디바이스 추론에서 성능을 좌우하는 병목(메모리 대역폭, 양자화)은 무엇인가?',
      '국내 시장에서 온디바이스 AI가 필요한 대표 제품 시나리오는 무엇인가?',
    ],
  },
  {
    id: 'enterprise-rag-security',
    title: '기업용 RAG 보안 체크리스트(권한·감사·누수)',
    description: '기업 RAG는 “정확도”보다 “누수 방지”가 1순위다. 설계·테스트·운영 체크리스트를 정리한다.',
    keyInsights: [
      '권한은 검색 단계에서 적용해야 한다. 생성 단계 마스킹은 늦다.',
      '감사 로그가 없으면 사고 원인을 증명할 수 없다.',
    ],
    researchQuestions: [
      '기업 RAG에서 흔한 누수 경로(검색, 캐시, 로그)는 무엇인가?',
      '권한 기반 검색을 구현하는 대표 패턴(ABAC/RBAC, 테넌시)은 무엇인가?',
      '누수 탐지/테스트를 자동화하기 위한 방법(리플레이, 샘플링)은 무엇인가?',
    ],
  },
  {
    id: 'ai-benchmark-misuse',
    title: '벤치마크가 실무를 망치는 순간(오용 사례)',
    description: '점수는 마케팅이 되기 쉽다. 실무에서 벤치마크가 오히려 실패를 부르는 패턴을 정리한다.',
    keyInsights: [
      '벤치마크는 목표가 아니라 신호다. 업무 질문으로 검증해야 한다.',
      '프롬프트/버전/데이터가 바뀌면 점수 의미도 바뀐다.',
    ],
    researchQuestions: [
      'LLM 벤치마크의 한계(과적합, 데이터 누수, 재현성 문제)는 무엇인가?',
      '실무 도입에서 벤치마크를 “의사결정 입력”으로 바꾸는 방법은 무엇인가?',
      '모델 비교를 할 때 반드시 포함해야 하는 운영 항목(비용, 지연, 정책)은 무엇인가?',
    ],
  },
  {
    id: 'model-context-window',
    title: '컨텍스트 윈도우 운영: 길이보다 중요한 것',
    description: '컨텍스트가 길어질수록 좋다는 착각이 있다. 요약/검색/정책을 섞어 운영하는 방법을 정리한다.',
    keyInsights: [
      '긴 컨텍스트는 비용과 누수 위험을 같이 올린다.',
      '요약/검색/메모리 전략을 분리하면 품질이 더 안정적이다.',
    ],
    researchQuestions: [
      '컨텍스트 윈도우 운영에서 자주 발생하는 문제(오염, 누수, 비용 폭증)는 무엇인가?',
      '요약 기반 메모리 vs RAG 기반 회수의 트레이드오프는 무엇인가?',
      '시스템 프롬프트/정책과 사용자 컨텍스트를 안전하게 분리하는 방법은 무엇인가?',
    ],
  },
];
