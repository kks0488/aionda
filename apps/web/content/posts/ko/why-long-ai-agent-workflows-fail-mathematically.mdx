---
title: 'AI 에이전트, 긴 단계일수록 실패가 누적된다'
slug: why-long-ai-agent-workflows-fail-mathematically
date: '2026-02-12'
lastReviewedAt: '2026-02-12'
locale: ko
description: 단계별 1% 오류가 100단계에서 37% 성공률로 누적될 수 있다. 검증·HITL·킬스위치를 설계하라.
tags:
  - llm
  - deep-dive
  - governance
author: AI온다
sourceId: wired-ai-1skvn3s
sourceUrl: 'https://www.wired.com/story/ai-agents-math-doesnt-add-up/'
verificationScore: 0.9666666666666667
alternateLocale: /en/posts/why-long-ai-agent-workflows-fail-mathematically
coverImage: /images/posts/why-long-ai-agent-workflows-fail-mathematically.png
---

## 세 줄 요약
- **무슨 변화/핵심이슈인가?** 다단계 AI 에이전트에서 단계별 작은 오류가 누적되는 **복합 오류(오류 전파)** 때문에 전체 성공률이 빠르게 떨어질 수 있다는 “수학적 한계” 논쟁이 커졌다.  
- **왜 중요한가?** 단계당 오류 확률이 **1%**라면 **100**단계를 모두 맞힐 확률이 **(0.99)^100 ≈ 37%**로 계산돼, 긴 워크플로 자동화가 제품/운영 리스크로 이어질 수 있다.  
- **독자는 뭘 하면 되나?** 에이전트를 “끝까지 맡긴다”가 아니라, **골든 셋 성공률 80% 이상** 같은 기준선을 두고 단계별 검증(액터-크리틱), 인간 개입, 킬 스위치를 포함한 운영 규칙부터 설계하라.

복잡한 작업을 길게 맡길수록, 에이전트는 겉보기와 달리 실패 쪽으로 기울 수 있다. 한 단계의 작은 실수는 다음 단계의 전제가 되고, 이후 판단을 오염시킨다. 그 결과 로그와 실행 내역이 그럴듯해도, 최종 결과가 틀릴 수 있다. WIRED는 이런 직관을 수학적으로 다룬 연구를 소개하며 “AI 에이전트는 수학적으로 실패할 수 있다”는 주장과 업계의 반론이 맞선다고 전했다.

## 도입부
**(0.99)^100 ≈ 37%**라는 계산은 “단계당 1% 오류”가 **100**단계 작업에서는 무시하기 어렵다는 점을 보여준다.

예: 한 에이전트가 여러 도구를 오가며 문서를 읽고, 계획을 세우고, 실행을 걸고, 결과를 요약한다. 중간에 작은 오해가 생겼지만 겉으로는 자연스러운 문장과 그럴듯한 작업 순서가 이어진다. 마지막 산출물만 보면 성공처럼 보이는데, 정작 핵심 조건을 하나 놓쳤다.

WIRED는 한 연구가 “AI 에이전트는 수학적으로 실패할 수 있다”고 경고하며, 업계가 그 결론에 동의하지 않는다고 전했다. 쟁점은 단순히 성능이 낮다는 말이 아니다. **업무 단계가 길어질수록 실패가 구조적으로 누적될 수 있느냐**가 핵심이다.

## 현황
다단계 작업에서 오류가 누적되면, 체인 길이가 늘어날수록 전체 성공률이 빠르게 내려갈 수 있다. 이 문제는 **복합 오류(Compound Error)** 또는 **오류 전파(Error Propagation)**로 설명된다. 에이전트가 “계획 → 실행 → 검증 → 수정”처럼 여러 단계를 거치면, 각 단계의 작은 실수가 다음 단계 입력이 되어 누적될 수 있다.

정량 예시는 본문에 이미 제시돼 있다. 해당 논문은 LLM이 각 단계에서 **1%**의 오류 확률만 가져도, **100**단계를 완벽하게 통과할 확률이 **(0.99)^100 ≈ 37%**까지 낮아질 수 있다고 썼다. 이 계산은 단일 모델을 지목하기보다, “긴 워크플로”라는 형식이 갖는 위험을 강조한다.

멀티 에이전트는 보완책이 될 수 있지만, 설계에 따라 결과가 달라질 수 있다. 한 연구는 투표 같은 검증 구조를 넣으면 단일 에이전트 대비 성공률을 **최대 90%**까지 높일 수 있다고 말한다. 동시에 나이브한 토론 구조에서는 에이전트들이 서로의 오류를 증폭시키거나 아첨(sycophancy)으로 잘못된 결론에 동조할 수 있다고 지적한다. 즉 “여럿이 논의한다”는 사실만으로 개선이 보장되지는 않으며, 어느 지점에서 상쇄가 증폭으로 바뀌는지는 작업/도메인에 따라 **추가 확인이 필요**하다.

프로덕션 관점에서는 “얼마나 똑똑한가”보다 “얼마나 관리 가능한가”가 먼저가 될 수 있다. 조사 결과는 최소한의 거버넌스 프레임으로 **NIST AI RMF(2023년 1월 발행)**를 언급한다. 또한 실무적 기준선으로 “골든 셋(Golden Set) 평가에서 **80% 이상** 성공률을 권장”하며, 다단계 오류를 줄이기 위한 **액터-크리틱(Actor-Critic) 구조의 단계별 자동 검증 루프**, 고위험 작업의 **인간 개입(Human-in-the-Loop)**, **글로벌 킬 스위치** 같은 운영 장치를 제시한다.

## 분석
이 논쟁이 의미 있는 이유는, 실패 원인이 “모델이 부족해서”만이 아니라 “프로세스가 길어서”일 수 있기 때문이다. 기업이 에이전트를 붙이는 지점은 문서 읽기, 여러 시스템 호출, 정책 조건 충족, 예외 처리, 결과 보고처럼 단계가 길어지기 쉽다. 여기서 **(0.99)^100 ≈ 37%** 같은 계산이 주는 메시지는 분명하다. **성공적으로 보이는 데모가, 긴 업무에서의 신뢰성으로 그대로 이어진다고 말하기는 어렵다.**

한편 산업계의 반론도 완전히 배제하기 어렵다. 실무에서는 단계별 검증, 자기 수정(self-correction), 도구 사용(tool use)으로 에이전트를 제품에 넣는 사례가 있다. 특히 멀티 에이전트에서 투표/검증 구조를 잘 설계하면 성공률을 **최대 90%**까지 올릴 수 있다는 결과는, “수학적 한계”가 곧 “실무적 불가능”을 뜻한다고 단정하기 어렵다는 점을 시사한다. 다만 조건이 따른다. 순차 실행 단계만 늘리면 실패는 누적될 수 있고, 토론을 추가했다고 자동으로 좋아지는 것도 아니다. 결국 **검증 장치가 제품 기능의 일부**가 된다.

## 실전 적용
의사결정 기준을 바꾸면 혼란을 줄일 수 있다. 에이전트 도입을 “자동화”만으로 보지 말고, “검증 가능한 워크플로”로 본다. 우선 체인을 짧게 설계하고, 각 단계에 검증을 두며(액터-크리틱), 위험 구간은 사람의 승인과 중단 장치로 끊을 수 있게 만든다. 멀티 에이전트를 쓰더라도 “여럿이 말한다”가 아니라 “서로를 검증한다”에 초점을 둔다.

예: 에이전트가 외부 도구로 변경 작업을 수행한다면, 실행 전에는 계획을 구조화해 검토하고, 실행 후에는 결과를 독립 규칙으로 재검증한 뒤, 실패 시 자동 롤백 또는 중단을 걸어야 한다. 이 흐름 없이 단계만 늘리면, 작업량은 늘어도 성공률은 체인 길이에 끌려 내려갈 수 있다.

**오늘 바로 할 일:**
- 골든 셋을 만들고, 현재 에이전트가 그 셋에서 **80% 이상**을 안정적으로 넘는지 측정하라.  
- 각 단계에 검증자를 붙여 액터-크리틱 루프로 실패를 조기에 끊고, 실패 로그를 원인별로 분류하라.  
- 고위험 작업에는 인간 승인 지점과 글로벌 킬 스위치를 넣고, 트리거 조건을 문서로 남겨라.  

## FAQ
**Q1. “단계당 1% 오류면 100단계에서 37%” 같은 계산은 너무 단순한 가정 아닌가?**  
A1. 단순화한 모델이다. 다만 체인이 길어질수록 작은 오류가 누적될 수 있다는 구조적 위험을 정량적으로 보여준다. 실제 환경에서는 단계별 오류가 독립이 아닐 수 있어 더 나빠질 수도 있고, 강한 검증 루프가 있으면 완화될 수도 있다.

**Q2. 멀티 에이전트면 해결되는가, 아니면 더 위험한가?**  
A2. 둘 다 가능하다. 투표 같은 검증 구조로 성공률을 **최대 90%**까지 높일 수 있다는 보고가 있는 반면, 단순한 토론은 오류를 증폭하거나 아첨으로 성능을 떨어뜨릴 수 있다는 지적도 있다. 변수는 “몇 명을 붙였는가”가 아니라 “어떻게 검증하게 했는가”다.

**Q3. 프로덕션에서 ‘최소한’으로 갖춰야 할 안전장치는 무엇인가?**  
A3. 조사 결과 기준으로는 **NIST AI RMF(2023년 1월)** 같은 프레임을 바탕으로 거버넌스를 세우고, 기술적으로는 골든 셋에서 **80% 이상**을 기준선으로 두는 접근이 언급된다. 또한 단계별 자동 검증(액터-크리틱), 고위험 작업의 인간 개입, 킬 스위치 같은 “중단 가능한 설계”가 핵심 축으로 제시된다.

## 결론
에이전트 논쟁의 초점은 “똑똑함” 자체에서 “길이”와 “검증”으로 이동하고 있다. 체인이 길어질수록 **(0.99)^100 ≈ 37%** 같은 계산이 시사하는 위험이 커질 수 있다. 관전 포인트는 업계가 에이전트를 그럴듯하게 보이게 만드는 데 그치지 않고, **실패를 전제로도 운영 가능한 검증 구조**를 표준에 가깝게 정착시킬 수 있느냐다.

## 다음으로 읽기
- [AI 자료 모음 (24h) - 2026-02-12](/ko/posts/ai-resources-roundup-2026-02-12)
- [AI 자료 모음 (6h) - 2026-02-11](/ko/posts/ai-resources-roundup-2026-02-11)
- [AI 자료 모음 (24h) - 2026-02-10](/ko/posts/ai-resources-roundup-2026-02-10)
- [AI를 활용한 고도화된 리팩토링과 코드 무결성 확보](/ko/posts/ai-code-refactoring-and-functional-integrity)
- [AI 코딩 비서와 보안: 1인 개발 리스크 관리](/ko/posts/ai-coding-security-for-solo-developers)
---

## 참고 자료

- 🛡️ [AI Risk Management Framework | NIST](https://nist.gov)
- 🛡️ [wired.com](https://www.wired.com/story/ai-agents-math-doesnt-add-up/)
- 🏛️ [Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models](https://arxiv.org/html/2505.24187v1)
- 🏛️ [Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate](https://arxiv.org/html/2509.05396v1)
