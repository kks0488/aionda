---
title: 'VLM의 진화: 선호도 최적화와 시각적 할루시네이션 극복'
slug: vlm-preference-optimization-hallucination-suppression
date: '2026-01-17'
locale: ko
description: 'VLM의 시각적 할루시네이션을 해결하기 위한 선호도 최적화 기술과 보상 모델 구조, 알고리즘의 발전 방향을 살펴봅니다.'
tags:
  - VLM
  - Preference Optimization
  - Hallucination
  - Multi-modal AI
  - DPO
author: AI온다
sourceId: huggingface-1aly9fe
sourceUrl: 'https://huggingface.co/blog/dpo_vlm'
verificationScore: 0.9666666666666667
alternateLocale: /en/posts/vlm-preference-optimization-hallucination-suppression
coverImage: /images/posts/vlm-preference-optimization-hallucination-suppression.jpeg
---

눈앞에 사과가 놓여 있는데도 "상큼한 오렌지"라고 답하는 인공지능은 더 이상 시장에서 살아남기 어렵습니다. 시각 언어 모델(VLM)이 단순히 이미지를 인식하는 단계를 넘어 인간의 의도와 실제 시각 정보를 얼마나 일관성 있게 연결하느냐가 인공지능 기술의 새로운 격전지로 부상했습니다. 최근 업계는 VLM의 출력 품질을 인간의 선호도에 맞춰 정렬하는 '선호도 최적화(Preference Optimization)' 기술에 집중하며 시각적 할루시네이션(환각)이라는 해묵은 난제에 도전하고 있습니다.

## 보는 것과 말하는 것의 간극을 메우는 기술

지금까지의 VLM은 방대한 양의 이미지-텍스트 쌍을 학습하며 '그럴듯한' 답변을 내놓는 데 주력했습니다. 하지만 이는 종종 모델이 시각적 단서를 무시하고 언어 모델 특유의 통계적 확률에만 의존해 답변을 생성하는 부작용을 낳았습니다. 이를 해결하기 위해 등장한 것이 보상 모델(Reward Model)입니다. 

VLM을 위한 보상 모델은 크게 두 가지 줄기로 나뉩니다. 먼저 CLIP 기반의 '임베딩 비교 구조'는 이미지와 텍스트 사이의 수학적 거리(코사인 유사도)를 계산해 두 정보가 얼마나 일치하는지 판별합니다. 반면, 멀티모달 대규모 언어 모델(LLM)을 직접 활용하는 '자기회귀적 생성 구조'는 시각 정보를 바탕으로 직접 수치적 보상을 주거나, '리스너(Listener)' 구조를 통해 모델이 내놓은 추론 과정이 타당한지 평가합니다. 'Vision-Language Models are Zero-Shot Reward Models' 연구나 'Listener-Rewarded Thinking' 기법은 이러한 보상 체계가 모델의 판단 기준을 어떻게 정교화하는지 보여주는 대표적인 사례입니다.

단순히 보상을 주는 것을 넘어, 학습 알고리즘 자체를 멀티모달 환경에 최적화하려는 시도도 활발합니다. 텍스트 기반 모델에서 효과를 거둔 직접 선호도 최적화(DPO) 알고리즘은 VLM의 특성에 맞춰 mDPO, V-DPO, OPA-DPO 등으로 변모하고 있습니다. mDPO는 모델이 시각 정보를 무시하고 언어적 편향에만 의존하는 '무조건적 선호' 문제를 방지하기 위해 시각적 단서를 강제로 반영하도록 설계되었습니다. V-DPO는 시각 가이드 레이어를 추가해 할루시네이션을 억제하며, OPA-DPO는 모델의 실제 출력 분포에 맞춰 전문가의 수정을 반영하는 온폴리시(On-policy) 정렬 방식을 택해 효율성을 높였습니다.

## 할루시네이션 억제와 제로샷 추론의 진화

이러한 최적화 기법의 가장 큰 수확은 시각적 할루시네이션의 유의미한 감소입니다. 모델이 이미지에 존재하지 않는 객체를 설명하거나 위치를 오인하는 빈도가 줄어들면서, MMBench나 MME와 같은 주요 제로샷(Zero-shot) 벤치마크 점수도 동반 상승했습니다. 특히 시각 정보에 기반하지 않은 응답을 걸러내는 능력이 강화되면서, 별도의 추가 학습 없이도 새로운 이미지에 대해 정확한 답변을 내놓는 제로샷 추론 능력이 기존 SFT(지도 미세 조정) 모델 대비 크게 개선되었습니다.

LLaVA-RLHF와 같은 프로젝트는 사실 관계가 보강된 RLHF(인간 피드백 기반 강화학습)를 통해 멀티모달 모델을 정렬했을 때 공간 추론 능력이 얼마나 정밀해질 수 있는지를 증명했습니다. 하지만 이러한 정렬 과정이 장점만 있는 것은 아닙니다. 시각적 정확도를 높이기 위해 모델을 강하게 제약할 경우, 일반적인 언어 추론 성능이 하락하는 '정렬 세금(Alignment Tax)' 문제가 발생할 수 있습니다. 또한, 실시간 서비스에 이러한 복잡한 보상 모델이나 최적화 알고리즘을 적용할 때 발생하는 연산 지연 시간(Latency)은 여전히 해결해야 할 숙제로 남아 있습니다. GPT-4o나 Gemini와 같은 상용 모델들이 내부적으로 어떤 구조의 보상 모델을 사용하는지 공개되지 않은 점도 연구자들이 넘어야 할 정보의 장벽입니다.

## 실전 적용: 신뢰할 수 있는 VLM 구축하기

개발자와 기업이 신뢰할 수 있는 VLM 서비스를 구축하려면, 단순한 성능 지표 이상의 정교한 접근이 필요합니다. 먼저 자신의 서비스가 시각적 정확도(예: 의료 영상 분석, 자율주행)와 언어적 유창함(예: 마케팅 문구 생성) 중 어디에 우선순위를 두는지 결정해야 합니다.

1. **데이터 큐레이션의 정밀화**: 할루시네이션이 포함된 응답과 정확한 응답을 쌍으로 구성한 선호도 데이터를 확보하는 것이 최적화의 첫걸음입니다.
2. **하이브리드 정렬 전략**: mDPO나 V-DPO와 같은 최신 알고리즘을 도입해 모델이 시각적 단서를 놓치지 않도록 강제하는 구조를 검토해야 합니다.
3. **지속적인 모니터링**: 정렬 세금으로 인한 일반 추론 성능 하락을 방지하기 위해, 멀티모달 벤치마크와 일반 NLP 벤치마크를 동시에 테스트하는 파이프라인이 필수적입니다.

## FAQ

**Q: VLM에서 발생하는 할루시네이션의 주요 원인은 무엇인가요?**
A: 가장 큰 원인은 '언어적 편향'입니다. 모델이 시각 정보를 분석하기보다 이전에 학습한 텍스트 데이터의 확률적 패턴에 의존해 답변을 생성하기 때문입니다. 선호도 최적화 기술은 모델이 텍스트 확률보다 시각적 증거에 더 높은 가중치를 두도록 훈련합니다.

**Q: CLIP 기반 보상 모델과 자기회귀적 보상 모델의 차이는 무엇인가요?**
A: CLIP 기반 모델은 이미지와 텍스트의 수학적 유사도를 빠르게 측정하는 데 강점이 있지만, 복잡한 문맥이나 추론 과정을 평가하기엔 한계가 있습니다. 자기회귀적 모델은 언어 모델의 추론 능력을 활용해 답변의 타당성을 더 깊게 평가할 수 있지만, 연산 비용이 더 많이 발생합니다.

**Q: 선호도 최적화를 거치면 모델의 일반적인 대화 능력도 좋아지나요?**
A: 반드시 그렇지는 않습니다. 시각적 정렬에만 지나치게 집중하면 일반적인 언어 능력이 다소 감소하는 '정렬 세금' 현상이 나타날 수 있습니다. 따라서 시각 정보 정렬과 언어 능력 유지 사이의 균형을 맞추는 하이퍼파라미터 튜닝이 매우 중요합니다.

## 결론

VLM 선호도 최적화 기술은 이제 막 걸음마를 뗐지만, 인공지능이 세상을 '이해'하고 '설명'하는 방식을 근본적으로 바꾸고 있습니다. 단순한 인식 기능을 넘어 인간과 시각적 경험을 공유하고 논리적으로 소통하는 인터페이스로 진화하기 위한 필수 과정입니다. 앞으로는 정렬 세금을 최소화하면서도 추론 속도를 확보하는 하드웨어 가속 기술과 알고리즘의 결합이 이 분야의 핵심 관전 포인트가 될 것입니다. 인공지능이 본 것을 정직하게 말하기 시작할 때, 우리는 비로소 AI를 진정한 동반자로 받아들일 수 있을 것입니다.
---

## 참고 자료

- 🛡️ [Listener-Rewarded Thinking in VLMs for Image Preferences](https://arxiv.org/abs/2506.28123)
- 🛡️ [RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback](https://arxiv.org/abs/2402.03681)
- 🛡️ [mDPO: Conditional Preference Optimization for Multimodal Large Language Models](https://arxiv.org/abs/2311.16922)
- 🛡️ [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.12345)
- 🛡️ [Hallucination of Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2402.00253)
- 🏛️ [Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning](https://arxiv.org/abs/2310.12921)
- 🏛️ [V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization](https://aclanthology.org/2024.findings-acl.123/)
- 🏛️ [OPA-DPO: Efficiently minimizing hallucinations in large vision-language models](https://www.microsoft.com/en-us/research/blog/opa-dpo-efficiently-minimizing-hallucinations-in-large-vision-language-models/)
- 🏛️ [LLaVA-RLHF: Aligning Large Multimodal Models with Factually Augmented RLHF](https://llava-rlhf.github.io/)
