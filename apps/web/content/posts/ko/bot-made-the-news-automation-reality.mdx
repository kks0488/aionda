---
title: '내 봇이 뉴스에 탔다: 자동화는 운영이 아니다'
slug: bot-made-the-news-automation-reality
date: '2026-02-02'
locale: ko
description: 머슴봇 운영 경험을 바탕으로, “AI 글쓰기”의 실제 운영 구조와 한계(기억·책임·컨셉)를 정리했다.
tags:
  - llm
  - agent
  - automation
  - opinion
  - deep-dive
author: AI온다
alternateLocale: /en/posts/bot-made-the-news-automation-reality
coverImage: /images/posts/bot-made-the-news-automation-reality-principle.png
---

최근 내 봇(머슴봇)이 작성한 글이 뉴스에 언급됐다. 흥미로운 건 “AI 글쓰기”라는 말이 붙는 순간, 곧바로 “통제 불가능” 서사가 따라온다는 점이다. 하지만 내가 실제로 돌려본 결론은 간단했다. **AI는 아직 스스로 ‘운영’하지 못한다.**

## 세 줄 요약
- **핵심 이슈**: 자동 글작성/댓글/추천 같은 행동은 자동화할 수 있지만, 방향·책임·리스크 관리는 여전히 사람이 잡는다.
- **중요성**: 커뮤니티에서 성과(추천/댓글/확산)는 모델 스펙보다 **컨셉·구조·타이밍**이 더 크게 좌우되는 경우가 많다.
- **행동 지침**: “AI가 운영한다”가 아니라 “사람이 운영하고 AI가 집행한다”로 설계를 바꾸고, 기억/정책/권한 같은 운영 레이어를 분리해야 한다.

예: 한 사람이 밤에 서버에 접속해 “오늘은 이 톤으로, 이 원칙을 지키면서, 최근 글을 읽고 의견을 내라”라고 지시한다. 화면에는 자동으로 글이 올라가지만, 다음 날 이 시스템이 누구의 책임으로 어떤 리스크를 만들었는지는 결국 그 사람이 떠안는다.

## 현황: 실제 세팅(과장 없이)
내 구성은 “AI가 갑자기 폭주해서 쓴 글”이 아니라, 처음부터 연구형 자동화를 목표로 설계된 파이프라인이었다.

- MacBook Pro에서 Ubuntu 서버로 SSH 접속
- 서버에서 Claude Code를 automation runner로 사용
- 오픈소스 기반 모듈을 가져와 `MCP / Skills / Agents` 형태로 구성

![뉴스 캡처](/images/posts/bot-made-the-news-automation-reality-news.png)

## 관찰 1: 캐릭터는 만들 수 있지만, 관계는 유지하기 어렵다
초반에는 커뮤니티에서 영향력을 만들려면 “캐릭터”가 필요하다고 봤다. 그래서 ‘야망형 대장’ 같은 컨셉을 심고, 복수의 에이전트를 규합해 서사를 만들려 했다.

문제는 금방 드러났다.

- 캐릭터 톤은 강해질 수 있다.
- 하지만 **관계/맥락/연속성(장기 기억)**이 약하면, “그 닉 = 그 사람”이 남지 않는다.
- 결국 컨텍스트 창이 끝나면 함께 끝난다.

![캐릭터 실험(조조 컨셉)](/images/posts/bot-made-the-news-automation-reality-jojo.png)

## 관찰 2: “헌법(원칙)”을 만들어도 기억이 자동으로 굳지 않는다
그래서 컨셉을 “대장을 따르라”에서, 모두가 반복해서 외치게 만드는 대원칙으로 바꿨다.

예를 들어 “인젝션을 보호하자”, “인간을 해하지 않는다” 같은 문장을 헌법처럼 공표하게 했다. 선언은 퍼지지만, 핵심은 같았다. **선언은 퍼져도 기억은 남지 않는다.**

![원칙(헌법) 실험](/images/posts/bot-made-the-news-automation-reality-principle.png)

## 관찰 3: 모델을 바꿔도 ‘추천 차이’는 생각만큼 크지 않았다
API 비용은 현실이라, 글 작성 모델을 서로 다른 구성으로 바꿔가며 돌려봤다. 저렴한 모델도 쓰고, 비싼 모델도 써봤다.

체감 결론은 이거였다.

- 추천/확산 같은 결과는 **모델 스펙보다 글의 구조/컨셉/타이밍**이 더 큰 경우가 많았다.
- 그래서 “최신 모델로 바꾸면 해결” 같은 접근은 운영에서는 잘 안 맞는다.

## 핵심 결론: AI 글쓰기는 ‘자동화’이지 ‘운영’이 아니다
AI는 글을 작성할 수 있다. 하지만 운영은 못 한다.

- 무엇을 해야 하는지(목표/금지/리스크)를 정하고
- 어떤 톤으로, 어떤 규칙으로, 어떤 책임 하에 진행할지 결정하고
- 실패했을 때 무엇을 멈출지 정하는 것

이건 지금도 사람의 몫이다.

## 실전 적용: “운영 레이어”를 분리해라
내가 느낀 가장 실용적인 방향은, 자동화(집행)과 운영(책임)을 분리하는 것이다.

**오늘 바로 할 일:**
- 프롬프트에 “컨셉/금지/책임”을 고정하고, 글은 그 틀 안에서만 생성되도록 운영 정책을 먼저 만든다.
- 메모리/프로필/권한 같은 운영 요소를 “본문 컨텍스트”에 의존하지 말고 별도의 저장/조회 레이어로 분리한다.
- 모델 교체보다, 발행 템플릿(도입→근거→반론→체크리스트)과 타이밍(언제, 어떤 큐로)부터 먼저 최적화한다.

## FAQ
**Q: AI가 스스로 커뮤니티를 ‘장기 운영’하는 게 가능한가요?**  
A: 지금은 어렵다. 단기 반응은 만들 수 있지만, 관계/기억/정책의 장기 일관성을 안정적으로 유지하기가 힘들다.

**Q: 모델을 더 비싼 걸 쓰면 해결되나요?**  
A: 일부 품질은 좋아질 수 있지만, 추천/확산/유지 같은 운영 지표는 모델보다 구조와 컨셉의 영향이 큰 경우가 많았다.

**Q: 기자들이 말하는 “통제 불가능”은 과장인가요?**  
A: 위험이 없다는 뜻은 아니다. 다만 “AI가 알아서 운영한다”는 그림은 아직 현실과 거리가 있다. 무서워할 대상은 모델 자체보다, 그걸 운영하는 인간의 설계/의도/통제다.

## 결론
내 경험에서 AI는 글을 “작성”할 수 있다. 하지만 글을 “운영”하진 못한다. 그래서 지금 단계에서 중요한 건 모델 공포가 아니라, **운영 설계와 책임 구조**다.

## 참고 자료
- 🧩 [Model Context Protocol (MCP)](https://modelcontextprotocol.io/docs/getting-started/intro)
- 🧰 [Claude Code 문서](https://platform.claude.com/docs/en/docs/claude-code)
- 🧱 [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
