---
title: 'AI 정신의약품 처방, 책임은 누구에게'
slug: ai-psychiatric-drug-prescription-liability
date: '2026-01-12'
locale: ko
description: >-
  의료 AI의 정신의약품 처방 개입 시 발생하는 법적 책임 공백과 국가별 규제 차이를 분석하고, 해결을 위한 사회적 합의의 필요성을
  제시합니다.
tags:
  - 의료인공지능
  - 정신의약품
  - 법적책임
  - 의료규제
  - AI윤리
author: AI온다
sourceId: '931860'
sourceUrl: 'https://gall.dcinside.com/mgallery/board/view/?id=thesingularity&no=931860'
verificationScore: 0.95
alternateLocale: /en/posts/ai-psychiatric-drug-prescription-liability
coverImage: /images/posts/ai-psychiatric-drug-prescription-liability.jpeg
---

# AI가 정신의약품을 처방할 때, 누가 책임질 것인가

의료 인공지능이 진단과 처방에 깊숙이 개입하면서 법적 책임의 빈 공간이 드러나고 있다. 특히 항정신의약품과 같은 고위험 약물을 다룰 때, 이 모호성은 규제와 환자 안전에 대한 근본적인 질문을 제기한다. AI의 권고와 인간 의사의 최종 판단 사이에서, 책임의 선은 어떻게 그어져야 하는가.

## 현황: 규제의 현재와 국가 간 차이

현재 미국 FDA와 한국 식약처 모두 'AI 기반 정신의약품 처방'만을 위한 독립된 특별 규정을 두고 있지 않다. 이들 시스템은 일반적인 인공지능 의료기기 및 임상 의사결정 지원 가이드라인에 따라 평가받는다. 다만, 정신 질환 관련 AI는 고위험군으로 분류되어 대부분 '의사 처방 필수' 기기로 인가되며, 실제 처방은 각국의 마약류 관리 법령에 따라 반드시 의료진의 최종 판단을 거치도록 되어 있다. AI가 의사의 개입 없이 독립적으로 정신의약품을 자동 처방하는 프로세스에 대한 명시적인 승인 사례는 확인되지 않는다.

국가별로 항정신성의약품에 대한 규제 접근법은 뚜렷한 차이를 보인다. 학술 연구에 따르면, 미국, 네덜란드, 독일의 청소년 정신의약품 사용 유병률을 비교한 연구는 규제 제한과 보상 정책의 차이를 강조한다. 소아 우울증 약물의 허가 정보를 국제적으로 비교한 연구 역시 국가별 허가 사항과 처방 지침의 불일치를 지적한다. 한국 식품의약품안전처의 정책 보고서는 이러한 국가별 처방 감시 체계와 허가 기준의 차이를 상세히 분석하고 있다.

## 분석: 책임의 공백과 사회적 합의의 역할

법적 틀은 여전히 전통적인 의사-환자 관계를 중심으로 설계되어 있다. AI가 진료 과정에 깊이 관여할 경우, 알고리즘의 오류, 데이터 편향, 또는 설명 불가능한 결정으로 인한 피해 발생 시, 제조사, 소프트웨어 개발자, 사용 의사, 기관 중 누가 최종 책임을 질지가 불분명해진다. 이는 환자 보호와 소송 리스크 관리 측면에서 상당한 불확실성을 초래한다.

흥미로운 점은, 이러한 규제의 차이가 순수하게 의학적 근거만으로 설명되기 어렵다는 사실이다. 비교 연구들은 사회적 정서, 문화적 수용도, 보건의료 시스템의 구조가 규제의 엄격함과 처방 패턴에 지대한 영향을 미친다고 분석한다. 즉, AI에 의한 고위험 약물 처방을 허용할지 여부는 단순한 기술적 안전성 평가를 넘어, 사회가 위험을 어떻게 정의하고 분배할지에 대한 윤리적·정치적 합의가 선행되어야 하는 문제다.

## 실전 적용: 시스템 도입 시 고려해야 할 질문

의료 기관이 정신의약품 처방 지원 AI를 도입하거나 정책 입안자가 관련 지침을 마련할 때, 몇 가지 명확한 질문을 던져볼 필요가 있다. 첫째, 시스템의 출력이 '권고'인지 '지시'인지를 프로토콜 수준에서 명확히 정의해야 한다. 둘째, 해당 국가의 마약류 관리 법규와 AI 가이드라인이 어떻게 연계되는지, 특히 의사의 필수 개입 지점이 어디인지를 법률 자문을 통해 검증해야 한다. 마지막으로, 시스템의 결정 근거를 의사가 이해하고 환자에게 설명할 수 있는지(설명 가능성)를 기술 도입의 핵심 조건으로 삼아야 한다.

## FAQ

**Q: AI가 처방한 정신의약품으로 인해 부작용이 발생하면, 환자는 누구를 상대로 소송을 제기할 수 있나요?**
A: 현재 규제 체계 하에서는 AI 자체를 법적 주체로 보기 어렵습니다. 따라서 환자는 일반적으로 해당 처방을 최종 승인하고 서명한 의사, 또는 소프트웨어에 결함이 있었다면 제조사를 상대로 책임을 물을 수 있습니다. 그러나 AI의 복잡한 의사결정 과정에서 과실을 특정하기는 매우 어려워, 법적 공백과 논쟁이 발생할 수 있습니다.

**Q: 모든 국가에서 AI 정신의약품 처방 지원 시스템은 의사의 최종 서명이 필수인가요?**
A: 미국 FDA와 한국 식약처의 현재 가이드라인에 따르면, 정신의약품과 관련된 AI는 고위험군으로 분류되어 의사 처방이 필수적인 장비로 인가됩니다. 따라서 공식적으로는 의사의 최종 검토와 승인이 반드시 필요합니다. 그러나 국가별 마약류 관리 법령을 세부적으로 검토해야 하며, 기술이 발전함에 따라 이 원칙이 어떻게 적용될지는 지켜봐야 합니다.

**Q: 의사가 AI의 처방 권고를 따르지 않다가 환자 상태가 악화되면, 이에 대한 책임은 어떻게 되나요?**
A: 의사는 AI의 권고를 포함한 모든 정보를 종합하여 자신의 전문적 판단을 내리는 최종 책임자입니다. 따라서 의사가 합리적인 근거에 따라 AI의 권고를 거절했고, 그 결정이 당시의 표준 진료에 부합했다면, 의사는 책임에서 자유로울 수 있습니다. 반면, AI의 명백하고 적절한 경고를 무시한 것이 과실로 판단될 경우 책임이 있을 수 있습니다.

## 결론

AI 의료 처방, 특히 정신의약품 영역에서의 책임 문제는 기술의 발전 속도에 규제와 법리가 따라가지 못하는 전형적인 사례다. 해결책은 단순히 기술에 '브레이크'를 거는 것이 아니라, 의사, 알고리즘, 제도 간의 새로운 책임 분배 모델을 사회적 합의를 통해 설계하는 데 있다. 당장의 행동은 명확하다: AI 시스템을 도입하는 모든 기관은 기술적 유용성보다 먼저 법적·윤리적 경계를 스스로 설정해야 한다.
---

## 참고 자료

- 🛡️ [Clinical Decision Support Software | FDA](https://www.fda.gov/medical-devices/digital-health-center-excellence/clinical-decision-support-software)
- 🛡️ [Generative AI-Enabled Digital Mental Health Medical Devices](https://www.fda.gov/advisory-committees/digital-health- advisory-committee/november-6-2024-digital-health-advisory-committee-meeting-announcement)
- 🛡️ [식품의약품안전처 가이드라인 자료실](https://www.mfds.go.kr)
- 🛡️ [A three-country comparison of psychotropic medication prevalence in youth](https://pmc.ncbi.nlm.nih.gov/articles/PMC2564946/)
- 🛡️ [식품의약품안전처 연구관리시스템 - 연구보고서](https://www.mfds.go.kr/brd/m_218/list.do)
- 🏛️ [An International Comparison of the Information in the Regulatory-Approved Drug Labeling and Prescribing Guidelines for Pediatric Depression](https://pubmed.ncbi.nlm.nih.gov/33600204/)
