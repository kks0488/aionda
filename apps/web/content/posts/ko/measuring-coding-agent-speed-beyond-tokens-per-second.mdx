---
title: '코딩 에이전트 속도, duration으로 쪼개 보기'
slug: measuring-coding-agent-speed-beyond-tokens-per-second
date: '2026-02-14'
lastReviewedAt: '2026-02-14'
locale: ko
description: 코딩 에이전트 속도를 토큰/초 대신 duration(출력·프리필·도구·네트워크)로 분해해 병목을 찾는다.
tags:
  - hardware
  - llm
  - explainer
  - benchmark
author: AI온다
sourceId: arstechnica-515b2198b04c8ed6
sourceUrl: >-
  https://arstechnica.com/ai/2026/02/openai-sidesteps-nvidia-with-unusually-fast-coding-model-on-plate-sized-chips/
verificationScore: 0.7666666666666666
alternateLocale: /en/posts/measuring-coding-agent-speed-beyond-tokens-per-second
coverImage: /images/posts/measuring-coding-agent-speed-beyond-tokens-per-second.png
---

## 세 줄 요약

- **무슨 변화/핵심이슈인가?** Ars Technica는 OpenAI의 GPT‑5.3‑Codex‑Spark가 코딩에서 **“이전 대비 15배 빠르다”**고 요약했으며, OpenAI는 이를 **Cerebras WSE‑3** 기반의 “지연시간 우선(latency-first)” 서빙 티어로 제공한다고 설명했다.  
- **왜 중요한가?** OpenAI는 작업 총 소요시간(duration)을 **4개 구성요소(출력 생성, 프리필, 도구 실행, 네트워크 오버헤드)**로 쪼개 제시했고, 개선 수치로 **라운드트립 오버헤드 80%**, **토큰당 오버헤드 30%**, **TTFT 50%** 감소를 언급했다.  
- **독자는 뭘 하면 되나?** “토큰/초”만 보지 말고, 에이전트 작업을 **duration 기준으로 계측**한 뒤(출력·프리필·도구·네트워크), 모델·서빙·연결 방식의 변경이 **총 작업 완료 시간**에 미치는 영향을 비교 측정하라.

---

코드 에이전트가 멈칫하는 구간은, 모델이 “생각을 못 해서”라기보다 **prefill/샘플링**, **도구 호출**, **네트워크 왕복**에서 생기는 경우가 많다. Ars Technica는 OpenAI의 코딩 모델 **GPT‑5.3‑Codex‑Spark**를 “이전 대비 **15배 빠름**”이라고 요약했다. OpenAI는 **Cerebras의 WSE‑3** 위에서 “지연시간 우선(latency-first) 서빙 티어”를 제공한다고도 밝혔다.

예: 한 팀이 코드 에이전트를 붙였지만, 모델 출력은 빨라도 도구 실행과 통신 대기 때문에 작업이 늦게 끝난다. 이때 모델 변경보다 연결 유지나 요청 구성 변경이 더 먼저 효과를 낼 수 있다.

다만 ‘15배’가 무엇을 기준으로 한 비교인지, 그 속도가 품질·비용·전력과 어떤 관계를 갖는지는 공개 정보만으로 확정하기 어렵다. 그래서 필요한 건 감상이 아니라 분해다. “모델이 빨라진 것”과 “서빙이 빨라진 것”, “하드웨어가 바뀐 것”을 분리해서 봐야 한다.

---

## 현황

코딩 에이전트의 체감 속도는 **모델 출력 속도**뿐 아니라 **프리필**, **도구 실행**, **네트워크 오버헤드**까지 합친 **총 소요시간(duration)**에서 갈린다. Ars Technica에 따르면 GPT‑5.3‑Codex‑Spark는 전작 대비 코딩이 **15배 빠르다**고 소개됐다. 같은 기사에서는 이 모델이 **초당 1,000 토큰을 넘는 속도**로 코드를 낸다고도 전했다. 다만 이 수치가 **동일 프롬프트**, **동일 컨텍스트 길이**, **동일 성공률(품질) 조건**에서의 비교인지까지는 기사 내용만으로 단정하기 어렵고, 독립 재현 자료도 충분히 공유되지 않았다는 점을 기사에서 짚는다.

OpenAI가 제시한 설명은 “속도”를 단일 지표로 보지 않는 쪽에 가깝다. OpenAI는 코딩 에이전트 작업의 총 소요시간(duration)을 다음 합으로 정의했다.

1. 출력 생성 시간(output tokens ÷ sampling speed)  
2. 프리필 시간(prefill tokens ÷ prefill speed)  
3. 도구 실행 시간  
4. 네트워크 오버헤드  

그리고 OpenAI는 네트워크 측면에서 **지속 WebSocket 연결**을 도입해 **클라이언트/서버 라운드트립 오버헤드 80% 감소**, **토큰당 오버헤드 30% 감소**, **TTFT(time-to-first-token) 50% 감소**를 언급했다. 이 수치들이 어떤 워크로드에서 측정됐는지까지는 본문만으로 확정하기 어렵지만, OpenAI가 어디를 병목으로 보고 있는지는 비교적 분명해진다.

하드웨어 변화도 포함된다. OpenAI는 Codex‑Spark가 **Cerebras Wafer Scale Engine 3(WSE‑3)**에서 돌아가며, 이를 “지연시간 우선 서빙 티어”로 제공한다고 썼다. IEEE Spectrum이 정리한 WSE‑3의 특징으로는 **온칩 메모리 44GB**, **메모리 대역폭 21PB/s**, **패브릭(온칩 네트워크) 대역폭 214Pb/s**가 언급된다. “접시 크기 칩” 같은 표현은 공식 용어로 확인되지 않을 수 있으나, 웨이퍼 스케일 접근을 설명하려는 관용적 표현으로는 이해 가능하다.

---

## 분석

이 이슈의 핵심은 “코딩 모델이 빨라졌다”라는 결론보다, OpenAI가 **추론 지연을 어떤 항목으로 정의하고 관리하려 하는지**에 있다. OpenAI의 duration 분해는 **토큰/초** 같은 단일 지표가 에이전트 환경을 충분히 설명하지 못한다는 점을 드러낸다. 코딩 에이전트는 테스트 실행, 파일 수정, 검색 같은 도구 호출이 잦고, 그 과정에서 왕복 통신이 반복된다. 이런 상황에서는 OpenAI가 언급한 **라운드트립 오버헤드 80% 감소** 같은 네트워크/서빙 최적화가 체감 속도에 큰 영향을 줄 수 있다.

다만 “GPU를 우회한다” 같은 해석은 조심할 필요가 있다. OpenAI는 WSE‑3 사용을 언급하면서도 GPU가 여전히 중요하다는 취지의 설명을 함께 두었다고 알려져 있다. 즉, 전면 대체로 단정할 근거는 부족하다. 또한 OpenAI는 **GPT‑5.3‑Codex‑Spark**가 **GPT‑5.3‑Codex의 smaller version**이라고 밝혔다. 속도는 종종 (a) 모델을 작게 만드는 선택, (b) 서빙 최적화, (c) 하드웨어 선택이 합쳐져 나온 결과다.

남는 질문은 최소 두 가지다.

1) 그 속도가 코드 품질(정확도, 테스트 통과율, 환각)과 어떤 관계가 있는가?  
2) 그 속도가 비용(달러/토큰)과 전력(와트/토큰)을 실제로 낮추는가?  

현재 본문에 포함된 정보만으로는 Spark의 품질 지표나 비용/전력 수치를 정량으로 연결하기 어렵다. 따라서 “빠름”만으로 제품 적합성을 결론내리기보다는, 워크로드 기준의 재현 측정이 필요하다.

---

## 실전 적용

필요한 일은 “15배 빠르다”는 문장을 그대로 채택하는 것이 아니라, 내 워크로드에서 ‘빠름’을 **duration(작업 단위 총 소요시간)**으로 정의하는 것이다. OpenAI가 제시한 프레임(출력 생성, 프리필, 도구 실행, 네트워크 오버헤드)을 그대로 가져오면, 어느 구간이 병목인지 비교적 빠르게 드러난다. 코딩 에이전트는 컨텍스트가 길어져 prefill 부담이 커지기 쉽고, 도구 호출이 늘면 네트워크·도구 시간이 비중을 차지한다.

또한 하드웨어를 GPU vs 비GPU로만 나누기보다는, 내 시스템에서 지연을 만드는 항목이 무엇인지부터 확인하는 편이 안전하다. WSE‑3 같은 웨이퍼 스케일은 온칩 메모리와 대역폭을 강조하고, GPU는 생태계와 범용성이 강점으로 자주 언급된다. 그러나 실제 최적화 대상은 칩 브랜드가 아니라 **지연의 구성요소**다.

**오늘 바로 할 일:**
- 에이전트 실행 1회에 대해 prefill/샘플링/도구/네트워크 시간을 분리해 로그로 남겨라.  
- 동일 작업을 2~3개 조건에서 반복 실행하고, 토큰/초가 아니라 작업 완료까지의 duration을 KPI로 비교해라.  
- 연결 유지(WebSocket 등), 요청 배치, 캐시 적용으로 라운드트립과 재전송을 줄일 수 있는지 먼저 실험해라.  

---

## FAQ

**Q1. ‘15배 빠름’은 정확히 뭘 기준으로 한 말인가?**  
A. 본문에 포함된 설명만 보면 단일 지표라기보다 코딩 에이전트 작업의 **총 소요시간(duration)** 맥락에 가깝다. 다만 ‘15배’가 어떤 프롬프트/컨텍스트 길이/품질 조건에서 측정됐는지는 여기 인용된 범위의 자료만으로 확정하기 어렵고, 재현 가능한 원시 로그도 확인되지 않았다는 문제가 남는다.

**Q2. 왜 WSE‑3 같은 웨이퍼 스케일 칩이 “지연시간”에 유리할 수 있나?**  
A. IEEE Spectrum 기준 WSE‑3는 **온칩 메모리 44GB**, **메모리 대역폭 21PB/s**, **패브릭 대역폭 214Pb/s**를 내세운다. 큰 온칩 메모리와 높은 대역폭은 데이터 이동을 줄여 지연을 낮출 여지를 만든다. 다만 OpenAI가 실제 서빙에서 어떤 메모리 계층을 어떻게 구성했는지까지는 공개 정보만으로 단정하기 어렵다.

**Q3. 속도가 빨라지면 코드 품질도 좋아지나, 비용도 내려가나?**  
A. 자동으로 그렇다고 보기 어렵다. OpenAI는 Spark가 speed에 맞춰 튜닝되었고 **smaller version**이라고 밝혔다. 이는 품질-속도 트레이드오프 가능성을 남긴다. 또한 비용(달러/토큰)과 전력(와트/토큰)은 본문에 포함된 자료만으로 확정할 수 없으므로, 성공률(예: 테스트 통과)과 duration, 비용을 함께 측정하는 편이 안전하다.

---

## 결론

Codex‑Spark 관련 소식은 “모델이 더 빠르다”로만 요약하기 어렵다. OpenAI는 지연을 **출력 생성·프리필·도구 실행·네트워크**로 쪼개 정의하고, 네트워크/서빙 최적화로 **80%**, **30%**, **50%** 같은 개선 수치를 함께 제시했다. 다음 관전 포인트는 두 가지다. 첫째, Ars Technica가 요약한 **15배**가 어떤 조건에서 재현되는지다. 둘째, “지연시간 우선” 티어가 품질·비용·전력과 어떤 교환관계를 갖는지다.

## 다음으로 읽기
- [AI 자료 모음 (24h) - 2026-02-14](/ko/posts/ai-resources-roundup-2026-02-14)
- [레이트리밋을 넘는 지속 접근 설계](/ko/posts/beyond-rate-limits-continuous-access-policy-engine-design)
- [버전 앵커링 줄이는 프롬프트 설계](/ko/posts/designing-prompts-to-reduce-version-anchoring-risks)
- [GABRIEL로 정성자료를 정량화](/ko/posts/gabriel-toolkit-turns-qualitative-data-into-quantitative-metrics)
- [LLM 변경 감지·대응 운영 루프](/ko/posts/operational-loop-monitoring-llm-provider-change-logs)
---

## 참고 자료

- [Introducing GPT‑5.3‑Codex‑Spark | OpenAI - openai.com](https://openai.com/index/introducing-gpt-5-3-codex-spark/)
- [arstechnica.com - arstechnica.com](https://arstechnica.com/ai/2026/02/openai-sidesteps-nvidia-with-unusually-fast-coding-model-on-plate-sized-chips/)
- [Cerebras WSE-3: Third Generation Superchip for AI - IEEE Spectrum - spectrum.ieee.org](https://spectrum.ieee.org/cerebras-chip-cs3)
- [Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute - arxiv.org](https://arxiv.org/abs/2509.20241)
