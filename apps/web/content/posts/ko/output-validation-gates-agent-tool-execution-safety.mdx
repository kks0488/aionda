---
title: '툴 호출 에이전트, 실행 전 검증이 핵심'
slug: output-validation-gates-agent-tool-execution-safety
date: '2026-02-17'
lastReviewedAt: '2026-02-17'
locale: ko
description: >-
  툴 호출은 실행이다. JSON 유효성만으론 부족하니 strict 스키마, allowed_tools, refusal·상태 검증을 게이트로
  둔다.
tags:
  - agi
  - hardware
  - llm
  - explainer
  - tool-calling
author: AI온다
sourceId: evergreen-tool-validation
sourceUrl: ''
verificationScore: 0.7866666666666666
alternateLocale: /en/posts/output-validation-gates-agent-tool-execution-safety
coverImage: /images/posts/output-validation-gates-agent-tool-execution-safety.png
---

## 세 줄 요약

- **무슨 변화/핵심이슈인가?** 에이전트가 **툴 호출 한 번**으로 “행동”하는 순간부터는, 출력 검증(Output Validation)이 선택 사항이 아니라 설계의 중심 과제가 된다.  
- **왜 중요한가?** 문서 기준으로 JSON mode는 **유효한 JSON**만 보장하고 **스키마 일치**는 보장하지 않는다. 따라서 스키마·권한·정책·상태가 어긋난 호출이 실행되면 사고로 이어질 수 있다.  
- **독자는 뭘 하면 되나?** 툴 호출마다 `allowed_tools`로 범위를 줄이고, Structured Outputs의 `strict: true`(또는 동등한 스키마 검증)를 적용한 뒤, `refusal` 감지와 상태 기반 검증을 **실행 직전 게이트**로 고정한다.

---

툴을 **한 번 호출**하는 순간, 결과는 “텍스트”가 아니라 “실행”이 된다. 예를 들어 메일 발송, 파일 삭제, 권한이 있는 시스템 변경은 되돌리기 어렵다. 그래서 에이전트 설계의 초점은 모델을 더 “그럴듯하게” 만드는 것보다, **실행 전에 검증 게이트를 두는 것**으로 옮겨가고 있다.

예: 사용자가 정리를 부탁했는데 에이전트가 삭제나 외부 전송 같은 동작을 시도한다. 겉보기에는 자연스러운 흐름일 수 있다. 하지만 실행은 결과를 바꾼다. 그래서 실행 직전에 멈춰서 확인해야 한다.

핵심은 단순하다. 모델이 만든 툴 호출이 (1) **유효한 형태(JSON)**인지, (2) **요구 스키마**를 만족하는지, (3) **허용된 권한/도구**인지, (4) **금지된 동작**이 아닌지를 **실행 전에** 확인해야 한다. 문서에서도 JSON mode는 “유효한 JSON”만 보장하며 **특정 스키마 일치**를 보장하지 않는다고 설명한다. 스키마 일치가 필요하면 Structured Outputs의 `strict: true` 같은 방식(또는 별도 검증/리트라이)을 권한다. 또한 연구 ‘AgenTRIM’은 도구 권한이 과도할수록 위험이 커질 수 있다고 보고, 단계별 최소권한과 상태 기반 검증을 강조한다.

---

## 현황

툴 실행 전 검증에서 먼저 흔들리는 지점은 **호출 인자(arguments)**다. 모델이 만든 JSON이 “파싱 가능”하다는 사실만으로는 안전을 보장하기 어렵다. 문서 기준으로 JSON mode는 **유효한 JSON 출력**은 보장하지만, **어떤 스키마와도 일치한다고 보장하지 않는다**. 반대로 Structured Outputs는 함수 호출 인자가 **제공한 JSON Schema와 정확히 일치**하도록 하는 방향을 설명하며, 이를 위해 `strict` 같은 설정을 사용하라고 안내한다.

두 번째 축은 **툴 선택 자체의 검증**이다. 모델이 여러 툴 중 무엇을 호출할지 완전히 자유롭게 두면, 설계자가 의도하지 않은 경로로 우회할 수 있다. 함수 호출 가이드는 `allowed_tools`로 모델이 호출할 수 있는 도구를 **부분집합으로 제한**하라고 권한다. 즉, “맞는 인자”뿐 아니라 “맞는 도구”를 함께 묶어야 한다.

세 번째는 **권한과 정책(금지 동작) 검증**이다. ‘AgenTRIM’은 부적절한 툴 권한이 보안 리스크를 만들 수 있다고 지적하고, 단계별 최소권한과 상태 인지(status-aware) 검증으로 완화하는 접근을 제안한다. 상태 인지는 “이 호출이 지금 시점에 가능한가?”를 묻는다. 예를 들어 **이미 취소된 작업을 다시 취소**하거나, **승인 없이 실행 단계로 점프**하는 호출은 실행 전에 걸러야 한다.

---

## 분석

이 패턴이 중요한 이유는, 에이전트의 실패가 “환각 텍스트”에서 “현실 세계의 변경”으로 옮겨가기 때문이다. 텍스트 오류는 보통 수정하면 끝난다. 하지만 툴 호출은 로그가 남고, 데이터가 바뀌며, 외부 시스템을 움직일 수 있다. 그래서 관심이 모델 성능만이 아니라, **제한(`allowed_tools`)**, **형식 준수(`strict` 스키마)**, **거절 감지(`refusal`)** 같은 “검증 가능한 인터페이스”로 이동한다.

한계도 있다. 첫째, 스키마 검증은 강력하지만 “정책적으로 안전한 의도”까지 자동으로 보장하지는 않는다. 스키마에 맞는 값으로도 금지된 조합이 만들어질 수 있다. 둘째, `allowed_tools`는 경로를 줄이지만 목록 구성이 부정확하면 정상 작업이 막히거나(과잉 차단), 위험 툴이 남을 수 있다(과소 차단). 셋째, 문서에서 말하듯 `refusal` 같은 신호는 감지를 돕지만, 조직별 금지 명령 목록과 예외 규칙은 환경마다 달라 추가 정의가 필요하다. 이 지점부터는 구현만의 문제가 아니라 운영·거버넌스 문제로 넘어간다.

---

## 실전 적용

실행 전 검증을 “한 번의 함수”로 끝내려 하기보다, 게이트를 **3겹**으로 나눈다: (1) 스키마 검증, (2) 도구/권한 검증, (3) 정책·상태 검증. 스키마는 Structured Outputs의 `strict: true` 같은 방식으로 강제하거나, 별도 검증기로 실패 시 거절/재질문/리트라이 경로를 분리한다. 그다음 요청 맥락에서 허용한 도구만 `allowed_tools`로 제한한다. 마지막으로 금지 동작(조직 정책), 사용자 승인 필요 조건, 상태 전이 규칙을 검사한 뒤에만 실행한다.

예: 사용자가 “정리해줘”라고 말했는데 에이전트가 파일을 옮기는 툴을 호출하려 한다고 하자. 스키마는 맞을 수 있다. 하지만 허용된 도구 목록에 그 툴이 없으면 호출을 막아야 한다. 허용된 도구가 맞더라도 정책상 “삭제/외부 전송” 같은 동작이 포함되거나, 현재 상태에서 실행하면 안 되는 단계라면 중단하고 확인 질문으로 되돌려야 한다.

**오늘 바로 할 일:**
- 모든 툴 호출 지점에 스키마 검증(Structured Outputs의 `strict: true` 또는 동등 검증)을 넣고, 실패 시 재질문/리트라이 경로를 분리하라.  
- 요청 단위로 `allowed_tools`를 최소화해, 필요 없는 툴이 호출되지 않게 하라.  
- 응답의 `refusal` 같은 거절 신호를 프로그램적으로 감지하고, 감지 시 “실행 중단 + 사용자 확인”으로 처리 흐름을 고정하라.  

---

## FAQ

**Q1. JSON mode만 켜면 툴 호출이 안전해지나?**  
A. 그렇지 않다. 문서 기준으로 JSON mode는 “유효한 JSON”을 보장하지만, **특정 스키마 일치**는 보장하지 않는다. 스키마 일치가 필요하면 Structured Outputs(예: `strict`)나 별도 검증이 필요하다.

**Q2. `allowed_tools`를 쓰면 충분한가?**  
A. 위험을 줄이는 데는 도움이 되지만 충분조건은 아니다. `allowed_tools`는 “어떤 도구를 호출할 수 있는지”만 제한한다. 해당 도구의 인자가 정책적으로 안전한지, 현재 상태에서 실행 가능한지는 별도로 검증해야 한다.

**Q3. `refusal`은 어디에 쓰나?**  
A. Structured Outputs 소개 글은 `refusal` 문자열 값이 API 응답에 포함될 수 있다고 설명한다. 이를 통해 개발자는 모델이 스키마 출력 대신 **거절을 생성했는지 프로그램적으로 감지**할 수 있다. 즉, 거절 신호가 보이면 실행을 멈추는 자동 안전장치를 만드는 데 쓰인다.

---

## 결론

툴 실행 전 검증은 “형식 맞추기”에 그치지 않는다. **스키마(`strict`)·도구 제한(`allowed_tools`)·정책·상태**를 실행 직전에 확인하는 방어선이다. 앞으로 확인할 지점은 이 검증이 단일 체크를 넘어 단계별 최소권한과 상태 기반 검증처럼 “흐름 전체”로 확장되는지, 그리고 `refusal` 같은 신호가 제품 운영에 일관되게 반영되는지다.

---

### 참고 자료
- [Function Calling in the OpenAI API | OpenAI Help Center - OpenAI Help Center](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api%23.eps)
- [Function calling - OpenAI API - OpenAI](https://platform.openai.com/docs/guides/function-calling/how-do-i-ensure-the-model-calls-the-correct-function)
- [AgenTRIM: Tool Risk Mitigation for Agentic AI - arXiv](https://arxiv.org/abs/2601.12449)
- [Introducing Structured Outputs in the API - OpenAI](https://openai.com/index/introducing-structured-outputs-in-the-api/)

## 다음으로 읽기
- [AI 자료 모음 (24h) - 2026-02-17](/ko/posts/ai-resources-roundup-2026-02-17)
- [멀티플랜 전환으로 한도 회피? 리스크 점검](/ko/posts/managing-message-caps-and-rate-limits-across-ai-plans)
- [AI로 IP 장편 애니 연속 제작, 핵심은 운영](/ko/posts/operating-continuity-for-ip-based-long-form-ai-animation-production)
- [대화형 AI의 동조 편향, sycophancy 리스크](/ko/posts/sycophancy-risks-when-conversational-ai-over-agrees-with-users)
- [디지털 지능, 인간 초월의 조건들](/ko/posts/when-digital-intelligence-truly-exceeds-human-capabilities)
---

## 참고 자료

- [Function Calling in the OpenAI API | OpenAI Help Center - help.openai.com](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api%23.eps)
- [Function calling - OpenAI API - platform.openai.com](https://platform.openai.com/docs/guides/function-calling/how-do-i-ensure-the-model-calls-the-correct-function)
- [Introducing Structured Outputs in the API | OpenAI - openai.com](https://openai.com/index/introducing-structured-outputs-in-the-api/)
- [AgenTRIM: Tool Risk Mitigation for Agentic AI - arxiv.org](https://arxiv.org/abs/2601.12449)
