---
title: '구글 FACTS: AI 환각과 진실성을 측정하는 정교한 기준'
slug: google-facts-benchmark-ai-hallucination
date: '2026-01-14'
locale: ko
description: >-
  구글 딥마인드의 FACTS는 AI 환각을 해결하기 위해 4가지 차원에서 진실성을 측정하며, 모델 최적화를 위한 정밀 진단서 역할을
  수행합니다.
tags:
  - FACTS 벤치마크
  - 인공지능 환각
  - 구글 딥마인드
  - AI 진실성
  - LLM 평가
author: AI온다
sourceId: deepmind-vlbhpe
sourceUrl: >-
  https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/
verificationScore: 0.97
alternateLocale: /en/posts/google-facts-benchmark-ai-hallucination
coverImage: /images/posts/google-facts-benchmark-ai-hallucination.jpeg
---

인공지능이 뱉어내는 그럴싸한 거짓말, 이른바 '환각(Hallucination)'은 AI 산업의 성장을 가로막는 가장 거대한 벽이다. 챗GPT가 시를 쓰고 코드를 짜는 능력은 이제 놀랍지도 않지만, 정작 중요한 비즈니스 의사결정에서 엉뚱한 수치를 제시하는 순간 신뢰는 모래성처럼 무너진다. 구글 딥마인드가 최근 공개한 'FACTS 벤치마크 스위트(FACTS Benchmark Suite)'는 이 고질적인 문제를 해결하기 위해 AI의 '진실성'을 측정하는 가장 정교하고 가혹한 잣대를 제시한다.

## 거짓말의 시대를 끝내기 위한 4개의 기둥

기존의 AI 평가 방식은 마치 사지선다형 시험과 같았다. 모델이 정답을 맞히면 점수를 주고, 틀리면 깎는 식이다. 하지만 실제 업무 환경은 그렇게 단순하지 않다. FACTS는 모델의 지능을 네 가지 차원으로 쪼개서 분석한다. 모델이 내부에 쌓아둔 지식(Parametric Knowledge), 실시간 웹 검색 활용 능력(Web Search), 이미지와 텍스트를 결합해 이해하는 능력(Multimodal), 그리고 주어진 문서 안에서만 답을 찾는 능력(Grounding)을 각각 측정한다.

특히 주목할 지점은 '근거 기반(Grounding)' 평가다. 이는 기업용 AI에서 가장 핵심적인 요소로, 모델이 외부 문서를 참고해 답변할 때 얼마나 정확하게 출처를 따르는지 확인한다. FACTS는 단순히 "맞다, 틀리다"를 판별하는 이진법을 넘어, 멀티스텝 검색을 통해 정보를 합성하는 복잡한 시나리오를 강요한다. 엉성한 추론으로는 통과할 수 없는 장벽을 세운 셈이다.

구글은 이 과정에서 객관성을 확보하기 위해 '심판진 앙상블'이라는 카드를 꺼냈다. 제미나이 3 프로와 GPT 5.2 같은 최상위 모델들을 평가자로 고용해 합의를 도출하는 방식이다.
 인간 평가자와의 상관관계를 분석한 결과, 의료와 법률 등 12개 이상의 전문 도메인에서 FACTS는 인간 전문가에 육박하는 신뢰도를 증명했다.

## 숫자 뒤에 숨은 냉혹한 현실

FACTS 벤치마크는 단순히 순위표를 만드는 도구가 아니다. 모델의 취약점을 수술대 위에 올리는 정밀 진단서에 가깝다. 예를 들어 특정 모델이 '파라미터 지식' 점수는 높지만 '근거 기반' 점수가 낮다면, 그 모델은 아는 체는 잘하지만 정작 사용자가 준 보고서 내용은 무시한다는 뜻이다. 이 경우 개발자는 모델을 처음부터 다시 가르치는 대신, RAG(검색 증강 생성) 파이프라인을 뜯어고치는 쪽으로 방향을 틀 수 있다.

그런데 FACTS가 사용하는 평가 모델(LLM-as-a-judge) 방식 자체가 본질적인 한계를 안고 있다. 평가 모델 자체가 가진 편향성이나 특정 도메인의 전문 용어를 오해할 가능성을 배제하기 어렵기 때문이다. 실제로 의료나 법률 도메인의 세부적인 난이도 변동에 따른 신뢰도 수치는 아직 명확하게 공개되지 않았다. "심판도 결국 AI인데, 심판이 틀리면 누가 잡느냐"는 근본적인 의문이 남는다.

또한, 이 벤치마크는 모델이 "모른다"고 답해야 할 때를 정확히 아는지에 대해서도 엄격한 잣대를 들이댄다. 무리하게 답변을 생성하다가 환각을 일으키는 모델에게는 가차 없는 감점이 주어진다. 이는 성능 경쟁에만 매몰되어 '정확도'보다 '화려함'을 좇던 업계 관행에 경종을 울린다.

## 기업과 개발자가 지금 당장 주목해야 할 지점

이제 개발자들은 "우리 모델이 MMLU에서 몇 점을 받았다"는 공허한 홍보 문구 대신, FACTS의 하위 지표를 파고들어야 한다. 만약 금융권 챗봇을 만든다면 '근거 기반' 점수에 사활을 걸어야 하고, 실시간 뉴스 요약 서비스를 만든다면 '웹 검색' 지표를 최우선으로 관리해야 한다.

실제 활용 시나리오에서 FACTS는 일종의 '성능 튜닝 지도' 역할을 한다. 파라미터 점수가 낮은 도메인 지식은 미세 조정(Fine-tuning)을 통해 보강하고, 질문에 대한 거절 능력이 부족하다면 답변 거부(Abstention) 학습을 진행하는 식이다. 막연한 추측이 아닌 데이터에 기반한 모델 최적화 시대가 열린 것이다.

## FAQ: FACTS에 대해 궁금한 3가지

**Q: 기존의 TruthfulQA나 다른 사실성 벤치마크와 무엇이 다른가?**
A: 기존 지표들이 단답형이나 단순 지식 확인에 그쳤다면, FACTS는 웹 검색과 멀티모달 정보를 결합한 '복합 추론' 과정을 평가한다. 또한 비공개 테스트 세트를 활용해 모델이 벤치마크 문제를 암기해서 점수를 올리는 '오염 문제'를 원천적으로 차단했다.

**Q: 평가 모델로 GPT 5.2나 제미나이 3를 쓰면 비용이 너무 많이 들지 않나?**
A: 그렇다. 하지만 수만 명의 인간 전문가를 고용해 수천 개의 답변을 검증하는 비용보다는 훨씬 저렴하다.
 FACTS는 합의 기반 시스템을 통해 인간 수준의 신뢰도를 확보하면서도 대규모 평가를 자동화할 수 있는 현실적인 대안을 제시한다.

**Q: 이 벤치마크 점수가 높으면 환각 현상이 완전히 사라졌다고 봐도 되는가?**
A: 아니다. FACTS는 모델의 '사실 관계 정확성'을 측정하는 도구일 뿐, 환각을 완전히 제거하는 치료제는 아니다. 다만 어떤 상황에서 환각이 발생하는지 정밀하게 추적할 수 있게 해주므로, 이를 줄이기 위한 기술적 토대를 마련해준다.

## 데이터가 보증하는 진실의 무게

FACTS 벤치마크의 등장은 AI 기술의 초점이 '창의성'에서 '신뢰성'으로 이동하고 있음을 시사한다. 이제 모델의 성능은 얼마나 화려한 문장을 구사하느냐가 아니라, 얼마나 책임질 수 있는 정보를 제공하느냐로 결정될 것이다. 구글이 제시한 이 엄격한 가이드라인은 AI가 단순한 장난감을 넘어 산업의 중추적인 인프라로 자리 잡기 위한 필수적인 관문이다. 앞으로 우리가 주목해야 할 것은 단순한 모델 출시 소식이 아니라, 그 모델이 FACTS의 현미경 아래에서 얼마나 버텨낼 수 있느냐는 점이다.
---

## 참고 자료

- 🛡️ [FACTS Benchmark Suite Introduced to Evaluate Factual Accuracy of Large Language Models](https://www.infoq.com/news/2026/01/facts-benchmark-suite-llm/)
- 🛡️ [The FACTS Leaderboard: A Comprehensive Benchmark for LLM Factuality](https://www.kaggle.com/benchmarks/google/facts)
- 🏛️ [FACTS Benchmark Suite: Systematically evaluating the factuality of large language models](https://deepmind.google/discover/blog/facts-benchmark-suite/)
- 🏛️ [The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality](https://storage.googleapis.com/deepmind-media/FACTS/FACTS_benchmark_suite_paper.pdf)
- 🏛️ [The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input](https://arxiv.org/abs/2501.03200)
- 🏛️ [FACTS Benchmark Suite: a new way to systematically evaluate LLMs factuality](https://deepmind.google/discover/blog/facts-benchmark-suite-a-new-way-to-systematically-evaluate-llms-factuality/)
