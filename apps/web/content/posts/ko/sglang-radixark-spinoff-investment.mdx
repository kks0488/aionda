---
title: 'SGLang, RadixArk로 독립 및 4억 달러 투자 유치'
slug: sglang-radixark-spinoff-investment
date: '2026-01-22'
locale: ko
description: SGLang 팀이 RadixArk로 독립하며 4억 달러 투자를 유치했습니다. vLLM 대비 3.1배 높은 처리량으로 추론 효율을 혁신합니다.
tags:
  - llm
  - radixark
  - sglang
  - radixattention
  - hardware
author: AI온다
sourceId: techcrunch-ai-4xqwp3m
sourceUrl: >-
  https://techcrunch.com/2026/01/21/sources-project-sglang-spins-out-as-radixark-with-400m-valuation-as-inference-market-explodes/
verificationScore: 0.8166666666666668
alternateLocale: /en/posts/sglang-radixark-spinoff-investment
coverImage: /images/posts/sglang-radixark-spinoff-investment.png
---

## TL;DR
- UC 버클리 연구진이 설립한 SGLang 팀이 'RadixArk'로 독립하며 Accel로부터 4억 달러 가치의 투자를 유치했습니다.
- RadixAttention 기술을 활용해 RAG 및 구조화된 생성 작업에서 기존 엔진 대비 최대 6.4배의 처리량을 달성했습니다.
- 70B 모델 기준 vLLM보다 3.1배 높은 처리량과 3.7배 빠른 첫 토큰 생성 시간(TTFT)을 기록하며 성능을 입증했습니다.

## 연구실에서 시장으로: RadixArk의 등장
AI 업계에서 추론 효율성은 비용 절감과 직접 연결됩니다. SGLang은 복잡한 프롬프트를 빠르고 정확하게 처리하는 프레임워크로 오픈소스 커뮤니티에서 인지도를 쌓아왔습니다. 이 프로젝트는 이제 RadixArk라는 사명으로 전환하며 상용화를 본격화하고 있습니다.

RadixArk의 주요 기술은 'RadixAttention'이라 불리는 공유 접두사(Shared Prefix) 캐싱입니다. 대규모 언어 모델이 긴 문맥이나 반복되는 지시사항을 처리할 때, 매번 처음부터 다시 계산하지 않고 이전 데이터를 재사용합니다. 이는 도서관에서 자주 찾는 페이지에 표시를 해두고 바로 찾아보는 것과 유사한 방식입니다.

조사 결과에 따르면 이 기술을 적용한 SGLang은 RAG(검색 증강 생성) 및 구조화된 생성 작업에서 기존 엔진보다 최대 6.4배 높은 처리량(Throughput)을 보였습니다. 특히 CUDA 그래프 기반의 정적 그래프 컴파일 최적화는 2.8배의 성능 향상을 추가로 이끌어냈습니다. 이는 AI 에이전트나 복잡한 워크플로우를 운영하는 기업에 서버 비용 절감 기회를 제공할 수 있습니다.

## vLLM과의 비교를 통한 성능 증명
현재 LLM 추론 엔진의 주요 선택지인 vLLM과의 성능 차이도 확인되었습니다. 벤치마크 데이터에 따르면 70B 규모 모델을 기준으로 SGLang은 vLLM 대비 3.1배 높은 처리량을 기록했습니다. 사용자 경험에 영향을 미치는 '첫 토큰 생성 시간(TTFT)'은 저지연 환경에서 약 3.7배 빠른 속도를 나타냈습니다.

이러한 수치는 RadixArk가 4억 달러의 기업 가치를 인정받은 근거가 됩니다. 대규모 트래픽이 발생하는 엔터프라이즈 환경에서 실질적인 경쟁 우위를 보였기 때문입니다. Accel의 이번 투자는 SGLang의 기술이 실제 산업 현장의 병목 현상을 해결할 주요 도구라고 판단한 결과로 보입니다.

## 분석: 효율성이 중심이 되는 전략
RadixArk의 독립은 AI 업계의 두 가지 흐름을 보여줍니다.

첫째는 '구조화된 생성'의 경제적 가치입니다. 현대 AI 서비스는 단순 대화를 넘어 JSON 형태의 데이터 추출이나 특정 규격에 맞춘 결과물을 요구합니다. SGLang은 이러한 구조화된 명령 처리에 특화되어 있으며, 이는 기업용 AI 시장에서 요구하는 기능입니다.

둘째는 오픈소스 기반의 상용화 속도입니다. 학계 프로젝트가 발표 직후 대규모 투자를 유치하며 독립 법인화되는 사례가 늘고 있습니다. 이는 기술의 성숙도만큼이나 해당 기술이 해결하려는 문제의 규모에 자본이 민감하게 반응하고 있음을 의미합니다.

다만 vLLM 역시 강력한 커뮤니티를 보유하고 있으며 성능 개선을 위한 업데이트를 지속하고 있습니다. RadixArk가 오픈소스 프로젝트의 정체성을 유지하면서 유료 고객을 위한 독자적인 가치를 어떻게 구축할지가 향후 과제입니다.

## 실전 적용: 도입 시 고려 사항
LLM 인프라를 구축 중인 개발자나 기업은 SGLang(RadixArk)을 운영 환경의 후보로 검토할 수 있습니다.

1. **RAG 워크플로우 최적화**: 긴 문서 기반의 질의응답 시스템을 운영한다면 RadixAttention의 캐싱 효율을 점검해야 합니다. 캐시 적중률이 높을수록 운영 비용을 낮출 수 있습니다.
2. **에이전트 시스템 구축**: 고정된 시스템 프롬프트가 반복 사용되는 환경에서 SGLang의 정적 그래프 컴파일은 응답 시간을 대폭 단축할 수 있습니다.
3. **벤치마크 수행**: 70B 이상의 대형 모델을 서비스한다면 vLLM과 SGLang의 처리량 대비 비용(Cost per Token)을 직접 비교하는 과정이 필요합니다.

## FAQ
**Q: RadixAttention은 일반적인 KV 캐시와 무엇이 다른가요?**
A: 일반적인 캐시는 단일 세션 내 재사용에 집중하지만, RadixAttention은 여러 사용자나 요청 간에 겹치는 공통 문구(Prefix)를 시스템 레벨에서 자동으로 인식하고 공유합니다. 이를 통해 메모리 사용을 줄이고 속도를 높입니다.

**Q: 4억 달러 가치 산정의 배경은 무엇인가요?**
A: 추론 비용은 AI 기업의 주요 지출 항목입니다. SGLang이 입증한 성능 향상이 실제 서버 인프라 비용 절감으로 이어진다면, 대규모 서비스 운영사들에 상당한 경제적 가치를 제공할 수 있습니다.

**Q: 기존 vLLM 사용자가 SGLang으로 전환하기 어렵나요?**
A: SGLang은 호환성을 고려하여 설계되었습니다. 다만 성능을 최대한 끌어내기 위해서는 SGLang 전용 인터페이스와 정적 그래프 컴파일 설정을 최적화하는 추가 작업이 필요할 수 있습니다.

## 결론
SGLang의 RadixArk 독립은 LLM 추론 시장의 경쟁이 효율화 단계로 진입했음을 보여줍니다. 6.4배의 처리량 향상은 기술적 성과를 넘어 AI 비즈니스의 수익 구조에 영향을 줄 수 있는 지표입니다. RadixArk가 오픈소스 생태계와 협력하며 엔터프라이즈 시장의 기준이 될 수 있을지 주목됩니다.
---

## 참고 자료

- 🛡️ [Source](https://techcrunch.com/2026/01/21/sources-project-sglang-spins-out-as-radixark-with-400m-valuation-as-inference-market-explodes/)
- 🏛️ [SGLang: Efficient Execution of Structured Language Model Programs](https://arxiv.org/abs/2406.06148)
