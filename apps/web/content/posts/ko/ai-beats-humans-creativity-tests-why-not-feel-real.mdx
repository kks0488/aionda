---
title: 'AI, 창의성 테스트서 인간 이겼지만 실감 못하는 이유'
slug: ai-beats-humans-creativity-tests-why-not-feel-real
date: '2026-01-12'
locale: ko
description: >-
  AI가 표준 창의성 테스트에서 인간 상위 1% 성적을 기록했지만, 평가 체계 부재와 안전성-창의성 균형 문제로 실감나지 않는 이유를
  분석합니다.
tags:
  - 인공지능
  - 창의성
  - GPT 5.2
  - 토런스 테스트
  - AI 한계
author: AI온다
sourceId: '931514'
sourceUrl: 'https://gall.dcinside.com/mgallery/board/view/?id=thesingularity&no=931514'
verificationScore: 0.96
alternateLocale: /en/posts/ai-beats-humans-creativity-tests-why-not-feel-real
coverImage: /images/posts/ai-beats-humans-creativity-tests-why-not-feel-real.png
---

# AI는 창의성 테스트에서 인간을 이겼다. 그런데 왜 우리는 실감하지 못할까?

표준화된 창의성 테스트에서 최신 대규모 언어 모델은 인간 상위 1%의 성적을 기록하고 있습니다. 이는 창의성을 패턴 인식과 의미 공간 탐색으로 정의할 때, AI가 고차원 벡터 공간에서 인간보다 우수한 연결 고리를 찾을 수 있기 때문입니다. 그러나 이러한 기술적 우위에도 불구하고 AI의 창의성은 종종 과소평가되는데, 그 원인은 평가 체계의 부재와 안전성과 창의성 사이의 까다로운 균형에 있습니다.

## 현황: 조사된 사실과 데이터

몬태나 대학교의 연구에 따르면, GPT 5.2는 토런스 창의력 검사의 핵심 요소인 '유창성'과 '독창성' 부문에서 인간 응답자 상위 1% 이내의 성적을 기록했습니다. '유연성' 부문에서도 상위 3%에 달해, 2,700명의 대학생으로 구성된 인간 대조군 대부분을 압도했습니다. 이는 단순히 정보를 재조합하는 수준을 넘어, 공인된 심리 측정 도구에서 인간의 최상위 창의적 사고를 모방하거나 재현할 수 있음을 시사합니다.

대안적 사용 과제 평가에서 AI의 정교함을 측정하기 위해 연구자들은 자동화된 채점 도구를 활용합니다. 'Stoplist method'를 통해 불용어를 제외한 의미 있는 단어 수를 계산하거나, 인간 평가자가 세부 묘사 수준을 직접 점수화하는 방식으로 AI 답변의 풍부함을 정량화합니다. 이러한 객관적 측정은 AI가 생성한 내용이 단순히 길기만 한 것이 아니라, 의미 있는 정보로 채워져 있음을 보여줍니다.

## 분석: 의미와 영향

AI의 높은 테스트 점수는 창의성에 대한 우리의 정의와 측정 방식 자체에 질문을 던집니다. 창의성을 '의미 공간에서의 탐색'으로 보는 기술적 관점에서는, 방대한 데이터를 초고속으로 처리하며 고차원적 패턴을 연결하는 AI가 인간을 앞설 수 있습니다. 그러나 이는 동시에 현재의 표준화된 테스트가 진정한 창의성의 핵심인 의도성, 정서적 깊이, 사회문화적 문맥 이해를 충분히 평가하지 못할 수 있음을 드러냅니다.

AI의 창의성 발현은 기술적 조절 장치에 민감하게 좌우됩니다. Temperature 파라미터 실험에 따르면, 이 값을 높여 출력의 무작위성을 증가시키면 결과물의 참신성은 소폭 상승하지만, 논리적 일관성이 결여된 '창의적 환각'이 비례하여 증가합니다. 특히 1.0을 넘어서는 순간 모델의 유용한 성능이 급격히 무너지는 임계점이 존재합니다. 이는 안정성과 예측 불가능한 창의성 사이에 명확한 트레이드오프 관계가 있음을 증명하며, 개발자들이 안전한 창의성의 경계를 찾아야 하는 이유입니다.

## 실전 적용: 독자가 활용할 수 있는 방법

AI를 창의적 파트너로 활용할 때는 Temperature 설정에 주의해야 합니다. 0.7에서 0.9 사이는 일반적으로 새로운 아이디어 생성에 안전하면서도 효과적인 범위로 알려져 있습니다. 1.0을 넘어서면 출력의 일관성이 크게 떨어질 위험이 있습니다. 아이디어 브레인스토밍이나 초기 개념 구상 단계에서는 이 값을 높여 다양한 가능성을 탐색하고, 정교화나 실행 가능성 검토 단계에서는 값을 낮추어 보다 안정적이고 사실적인 출력을 얻는 전략적 접근이 유용합니다.

## FAQ: 질문 3개

**Q: AI가 창의성 테스트에서 인간을 이겼다는 건, 이제 인간이 할 일이 없다는 뜻인가요?**
A: 아닙니다. 현재의 테스트는 발산적 사고의 일부 측면만을 측정합니다. 진정한 창의성은 종종 문제 정의, 감정 이입, 문화적 이해, 실패에서 배우는 능력 등 테스트로 쉽게 포착되지 않는 요소들을 포함합니다. AI는 강력한 아이디어 생성 도구이지만, 의미 부여와 실행은 여전히 인간의 영역에 남아 있습니다.

**Q: Temperature를 높이면 항상 더 창의적인 결과가 나오나요?**
A: 꼭 그렇지 않습니다. Temperature 상승은 출력의 다양성과 참신성을 높일 수 있지만, 동시에 사실과 무관하거나 논리적으로 일관성 없는 내용을 생성할 위험도 증가시킵니다. 창의성과 정확성 사이의 균형을 맞추는 것이 중요합니다.

**Q: AI의 창의성을 평가하는 더 좋은 방법은 무엇일까요?**
A: 표준화된 테스트 외에, 장기적인 프로젝트에서의 문제 해결 능력, 다양한 제약 조건 하에서의 적응적 사고, 또는 인간과의 협업을 통한 시너지 창출 효과 등을 평가하는 다각적이고 실전적인 평가 체계가 필요합니다. 단일 점수가 아닌 포트폴리오 평가에 가까운 접근이 요구됩니다.

## 결론: 요약 + 행동 제안

데이터는 명확합니다. AI는 우리가 설계한 특정 창의성 테스트에서 인간의 최상위 수행자를 능가할 수 있습니다. 그러나 이는 종착점이 아닌 출발점입니다. 우리는 이제 창의성의 본질에 대해 더 깊이 질문하고, AI의 기계적 우월성을 인간의 맥락 이해력과 통합할 새로운 협업 방식을 모색해야 할 때입니다. 당신의 다음 행동은 AI를 단순한 도구가 아닌, 당신의 사고를 확장시키는 비판적 파트너로 대화석에 앉히는 일부터 시작될 수 있습니다.
---

## 참고 자료

- 🛡️ [UM Research: AI Tests Into Top 1% for Original Creative Thinking](https://www.umt.edu/news/2023/07/070523ai-research.php)
- 🏛️ [The current state of artificial intelligence generative language models is more creative than humans on divergent thinking tasks](https://www.nature.com/articles/s41598-024-53303-w)
- 🏛️ [The Effect of Sampling Temperature on Problem Solving in Large Language Models](https://arxiv.org/abs/2402.10412)
