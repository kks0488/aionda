---
title: AI Abuse Shifts From Text To Distribution TTPs
slug: ai-abuse-shifts-from-text-to-distribution-ttps
date: '2026-02-25'
lastReviewedAt: '2026-02-25'
locale: en
description: >-
  AI abuse is shifting from text generation to channel-linked TTPs. Defend with
  multi-signal detection and rapid takedowns plus appeals.
tags:
  - agi
  - llm
  - explainer
author: AI온다
sourceId: openai-d163670fb22ce1f9
sourceUrl: 'https://openai.com/index/disrupting-malicious-ai-uses'
verificationScore: 0.82
alternateLocale: /ko/posts/ai-abuse-shifts-from-text-to-distribution-ttps
coverImage: /images/posts/ai-abuse-shifts-from-text-to-distribution-ttps.png
---

## TL;DR
- This shift can weaken text-only detection and raise the value of multi-signal, channel-level defenses and fast enforcement.
- Review your enforcement latency, then add multi-signal rules and improve appeals and transparency processes.

Example: An operator reviews a feed and notices similar links spreading across fresh accounts. Language and tone vary across posts. The behavior suggests coordinated distribution rather than independent discussion. The operator prioritizes account and link signals over writing style.

When an operator opens a dashboard late at night, similar links may appear across new accounts. These links can appear at short intervals. The anomaly can show up in distribution flows before sentence-level signals. The issue can shift from plausible writing to coordinated operations. Models can be paired with websites and social platforms. That pairing can change attack speed and scale. OpenAI는 2024년 2월 및 2024년 5월에 각각 위협 행위자 악용 및 은밀한 영향공작 관련 보고서에서 탐지·대응(방어) 관점의 내용을 논의했다.

## Current status
AI abuse can be clearer in channels, account clusters, and distribution speed. It can be less clear in the generated text alone. OpenAI published a threat-focused post in **February 2026**. It described patterns where malicious actors combine models with websites and social platforms. The post reads like a report summary. It may not support a full, detailed inventory from excerpts alone. The linked PDF scope should be checked separately.

The investigation results describe **three branches** of TTPs.
First, **phishing and scams** can use OSINT for targeting. They can generate personalized messages at scale. They can distribute messages through email, SNS, and messengers.
Second, **impersonation and deepfakes** can set up fake accounts. They can forge audio or video to build trust. They can steer victims to non-standard channels to reduce tracking.
Third, **covert influence operations** can generate, translate, and edit posts. They can spread content on platforms like X and Telegram. They can operate in multiple languages. They can also automate operational tasks, including code debugging for bots.

On defense, a practical point goes beyond better detection. A cited arXiv study discusses takedown speed on social media. It notes that removal **within a few hours** can reduce exposure, reach, and diffusion. It also notes that delayed removal may fail to suppress spread. Enforcement delay time can influence outcomes.

## Analysis
The cost structure and labor split in attacks appear to be changing. OSINT targeting and multilingual posting existed before. Cross-channel movement also existed before. Generative AI can make this closer to an operational pipeline. It can be less dependent on individual writing skill. Attackers can behave more like operators managing account pools and distribution. Defenses can shift away from single signals like “AI-written.” Defenses can use multi-signal views across accounts, networks, and propagation.

Multi-signal controls and fast enforcement can add risks. Rapid takedown can reduce spread. Faster action can also raise false-positive risk. Identity controls like KYC can show similar trade-offs. Stronger identity checks can reduce some abuse vectors. This investigation does not confirm a quantitative effect. Stronger checks can also raise privacy and governance concerns. Sensitive identity data could be exposed by insiders or vendors.

Responsibility allocation also remains unsettled. The NIST AI RMF recommends practices for testing and incident identification. It also recommends information sharing. The arXiv discussion notes joint testing and sharing may reduce risk. It also notes **antitrust** concerns can chill collaboration. Legal uncertainty can slow execution, even when parties want to collaborate.

## Practical application
Operators can focus on where operations can be interrupted. Text detectors can still help in some cases. Phishing and scams can show stronger signals in link transitions and channel use. Impersonation can show signals in account creation and relationship-building. Influence operations can show signals in multilingual duplication and cross-platform movement. These signals can be missed with text-only review.

**Checklist for Today:**
- Add multi-signal rules that combine account behavior, link movement, and propagation speed, alongside text signals.
- Measure enforcement latency and review escalation paths to support takedowns **within a few hours** when appropriate.
- Improve appeals, re-review, log retention, and user notifications to manage false positives from faster enforcement.

## FAQ
**Q1. If we have a detector that catches “AI-written text,” doesn’t that solve it?**  
A. The cited concern is less about text quality and more about distribution operations. Humans can also write persuasive phishing text. Influence operations can blur traces via translation and re-editing. Text-only classification can leave evasion paths. Multi-signal review can reduce that gap.

**Q2. What lever is easiest for platform operators to see immediate impact from?**  
A. The most direct lever in the cited evidence is enforcement speed. The study discusses deletion **within a few hours**. It links this speed to reduced exposure and spread. Speed can also increase enforcement errors. Appeals, re-review, and logs can help manage that risk.

**Q3. How should model providers and platforms split responsibility, and how far?**  
A. A fixed split is hard to justify from the cited material. The NIST AI RMF emphasizes incident practices and information sharing. Other research notes collaboration can face **antitrust** concerns. Teams can define scopes like logging, notification, and joint analysis. They can also run legal review in parallel.

## Conclusion
AI abuse can shift from plausible text to coordinated attack operations. These operations can ride distribution channels. OpenAI discussed related concerns in **February 2026**. Defense may shift beyond text classification toward channel-level detection and blocking. The next checkpoint involves managing false positives, transparency, and privacy risks. This can be done through explicit procedures and metrics under pressure for fast enforcement.

## Further Reading
- [AI Resource Roundup (24h) - 2026-02-25](/en/posts/ai-resources-roundup-2026-02-25)
- [CleaveNet Designs Protease-Cleavable Peptides for Urine Sensors](/en/posts/cleavenet-designs-protease-cleavable-peptides-for-urine-sensors)
- [Defense AI Full Use Clashes With Contract Controls](/en/posts/defense-ai-full-use-contract-controls)
- [Defense AI Procurement: Operations, Logging, Rights, And Incident Response](/en/posts/defense-ai-procurement-operations-logging-rights-incident-response)
- [Designing Dispute Procedures Beyond Generative Detection Scores](/en/posts/designing-dispute-procedures-beyond-generative-detection-scores)
---

## References

- [Disrupting deceptive uses of AI by covert influence operations | OpenAI - openai.com](https://openai.com/index/disrupting-deceptive-uses-of-ai-by-covert-influence-operations/)
- [AI RMF Core - AIRC (NIST) - airc.nist.gov](https://airc.nist.gov/airmf-resources/airmf/5-sec-core/)
- [openai.com - openai.com](https://openai.com/index/disrupting-malicious-ai-uses)
- [Digital deception: generative artificial intelligence in social engineering and phishing | Artificial Intelligence Review - link.springer.com](https://link.springer.com/article/10.1007/s10462-024-10973-2)
- [Audit of takedown delays across social media reveals failure to reduce exposure to illegal content - arxiv.org](https://arxiv.org/abs/2502.08841)
- [Enabling Frontier Lab Collaboration to Mitigate AI Safety Risks (arXiv) - arxiv.org](https://arxiv.org/abs/2511.08631)
