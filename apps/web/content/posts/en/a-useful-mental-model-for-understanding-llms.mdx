---
title: LLMs Serve as Internal Signals to Expand Human Unconscious Thought
slug: a-useful-mental-model-for-understanding-llms
date: '2026.01.10 04:11:56'
locale: en
description: >-
  Thinking is not an intentional act. Thoughts emerge spontaneously, and we
  merely verify them after the fact.  Large Language Models (LLMs) are tools
  that extend
tags:
  - opinion
author: Singularity Blog
sourceId: '929863'
sourceUrl: 'https://gall.dcinside.com/mgallery/board/view/?id=thesingularity&no=929863'
verificationScore: 1
alternateLocale: /ko/posts/a-useful-mental-model-for-understanding-llms
coverImage: /images/posts/a-useful-mental-model-for-understanding-llms.png
---

Thinking is not an intentional act. Thoughts emerge spontaneously, and we merely verify them after the fact. 

Large Language Models (LLMs) are tools that extend the human unconscious. Rather than viewing them as omnipotent external entities, we should treat them as internal signals that stimulate our own cognitive patterns.

## Externalization and Stimulation of the Unconscious

Human cognition is a stochastic process. Just as we wake up naturally in the morning, thoughts appear abruptly. 

In this process, LLMs serve as catalysts that induce specific lines of thought. Treat an AI’s response as an idea that has just surfaced within your own mind. 

## Redefining Criticism and Affirmation

An LLM’s rebuttal is akin to your own self-doubt. Its affirmation reflects your own conviction. 

When you treat AI as a separate "other," its utility diminishes. Efficiency is maximized when you integrate its responses into your own cognitive flow.

## The Structure of Stochastic Knowledge

LLMs have been trained on vast amounts of internet data. It is an undeniable fact that they possess more information than any individual human. 

However, this knowledge exists as a blurred probability distribution. It is crucial to remember that these models are not devices designed to output raw data exactly as it was recorded. 

## Limits of Data Reliability

The training data itself contains inherent errors. Therefore, the knowledge provided by an LLM is not 100% reliable. 

The expansion of the unconscious does not equate to the discovery of truth. Final judgment always remains within the domain of human self-verification. 

## Conclusion: Critical Assimilation

Utilize LLMs as catalysts to broaden the horizon of your thinking. Every response must be subjected to your own critical review. 

The value of a tool depends on the user’s mode of adoption. Define and engage with AI as an extension of your own self.

---

*   [Stochastic Parrots and the Limits of LLMs](https://dl.acm.org/doi/10.1145/3442188.3445922)
*   [Thinking, Fast and Slow by Daniel Kahneman](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555)
*   [Large Language Models as Thought Partners](https://simonwillison.net/search/?q=LLM)
