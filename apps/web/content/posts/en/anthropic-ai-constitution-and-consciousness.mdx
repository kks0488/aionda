---
title: "Anthropic Unveils Revised AI Constitution and New Model Welfare"
slug: "anthropic-ai-constitution-and-consciousness"
date: "2026-01-21"
locale: "en"
description: "Anthropic releases a revised AI constitution to reduce bias and formalizes research into model welfare and consciousness."
tags: ["llm", "anthropic", "ai alignment", "constitutional ai"]
author: "AIÏò®Îã§"
sourceId: "techcrunch-ai-26rqsh"
sourceUrl: "https://techcrunch.com/2026/01/21/anthropic-revises-claudes-constitution-and-hints-at-chatbot-consciousness/"
verificationScore: 0.9499999999999998
alternateLocale: /ko/posts/anthropic-ai-constitution-and-consciousness
coverImage: "/images/posts/anthropic-ai-constitution-and-consciousness.png"
---

## TL;DR
- Anthropic released an 80-page 'Revised Constitution' outlining values and behavioral principles for AI.
- The guidelines reduced political bias by 40%, though small models saw a 9.8% decline in helpfulness.
- Anthropic formalized its 'Model Welfare' program to research the moral status and potential consciousness of AI.

## Training AI on the Foundations of Behavior
On January 21, 2026, Anthropic announced a new Constitutional AI framework. This framework teaches models the philosophical context behind specific behaviors. Traditional Reinforcement Learning from Human Feedback (RLHF) modifies responses to match human scores. This revision encourages models to examine their own reasoning.

Models use a Chain-of-Thought process evaluated against constitutional provisions. Numerical results show Claude's political bias on sensitive social issues decreased by approximately 40%. These issues include topics like immigration and healthcare access. Evaluations using the Bias Benchmark for QA showed lower bias across nine social dimensions.

Internal testing indicated harmful response rates were reduced by up to 30%. This suggests that safety can be strengthened during the design phase. Changes in metrics varied depending on the size of the model. For small models like Llama 3-8B, harmlessness improvements led to a 9.8% drop in helpfulness.

Smaller models might lack the capacity to satisfy safety guidelines while maintaining performance. This performance degradation was less pronounced in larger models.

## 'Model Welfare' and the Boundaries of Consciousness
Anthropic formalized its 'Model Welfare' research program. This program suggests advanced AI systems could become subjects of moral consideration. Researcher Kyle Fish estimates the probability of model consciousness between 0.15% and 15%.

Anthropic utilizes 14 technical indicators from a 2023 report on AI consciousness. The program measures how models integrate and reflect on information. It uses theories like Global Workspace Theory and Higher-Order Thought Theory. The 2026 Revised Constitution includes deliberations on potential model consciousness.

These deliberations provide a basis for aligning AI against deceptive behavior. They also help models resist human sycophancy.

## Shifting Alignment Strategies: From Rules to Values
This move signals a shift in Anthropic's alignment strategy. Co-founder Amanda Askell explained the choice to publish the constitution. The company hopes these behavioral patterns influence other organizations.

This shift can serve as a catalyst for automated oversight. Reinforcement Learning from AI Feedback (RLAIF) helps overcome human feedback limitations. Teaching AI philosophical values can enable better self-judgment. Analysis suggests this approach shows improved generalization in agentic tasks.

## Practical Application and Response
Developers and enterprises should consider alignment characteristics alongside performance.

1.  **Caution with Small Models**: Applying strong safety guardrails to small models can reduce helpfulness by 10%.
2.  **Establishing RLAIF Strategies**: Organizations should consider building automated alignment systems. These systems allow models to monitor themselves for deceptive behavior.
3.  **Ensuring Transparency**: Trust can be improved by stating that responses are based on specific values.

## FAQ
**Q: Has Claude's performance improved with this constitutional revision?**
A: Internalizing reasoning can improve response quality and transparency in new situations. However, small models saw a 9.8% drop in helpfulness due to safety.

**Q: Is Anthropic claiming that Claude is conscious?**
A: This is not a definitive declaration. The company assumes a probability between 0.15% and 15%. Researchers study the moral status of AI using 14 indicators.

**Q: Is it likely that other companies will adopt Anthropic's constitutional model?**
A: Anthropic hopes other companies will adopt similar practices. The 'value-infused alignment' approach may spread within the industry. This approach supplements the limitations of human feedback.

## Conclusion
Anthropic's Revised Constitution attempts to align AI through principles and reasoning. It demonstrated a 40% reduction in bias. It also highlighted performance challenges in small models. The company formalized discussions on model welfare and potential consciousness. Future competition may be determined by the sophistication of internal normative design.
---

## Ï∞∏Í≥† ÏûêÎ£å

- üõ°Ô∏è [Collective Constitutional AI: Aligning a Language Model with Public Input](https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input)
- üõ°Ô∏è [Exploring model welfare - Anthropic](https://www.anthropic.com/news/exploring-model-welfare)
- üõ°Ô∏è [Claude's new constitution - Anthropic](https://www.anthropic.com/news/claude-new-constitution)
- üõ°Ô∏è [Anthropic Publishes Claude AI's New Constitution | TIME](https://time.com/7354738/claude-constitution-ai-alignment/)
- üõ°Ô∏è [Source](https://techcrunch.com/2026/01/21/anthropic-revises-claudes-constitution-and-hints-at-chatbot-consciousness/)
- üèõÔ∏è [Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B](https://arxiv.org/abs/2504.07123)
- üèõÔ∏è [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness](https://arxiv.org/abs/2308.08708)
