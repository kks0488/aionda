---
title: "AI Image Generators and the App Store's Responsibility and Regulation"
slug: "us-senators-demand-removal-of-grok-from-app-store-and-play-s"
date: "2026-01-12"
locale: "en"
description: "An analysis of platform responsibility and legal regulations, examining the controversy over harmful content from AI image generators and its implications for app stores."
tags: ["AI ì´ë¯¸ì§€ ìƒì„±ê¸°", "ì•± ìŠ¤í† ì–´", "í”Œë«í¼ ì±…ì„", "AI ê·œì œ", "ìœ í•´ ì½˜í…ì¸ "]
author: "AIì˜¨ë‹¤"
sourceId: "930623"
sourceUrl: "https://gall.dcinside.com/mgallery/board/view/?id=thesingularity&no=930623"
verificationScore: 0.97
alternateLocale: /ko/posts/us-senators-demand-removal-of-grok-from-app-store-and-play-s
coverImage: "/images/posts/us-senators-demand-removal-of-grok-from-app-store-and-play-s.jpeg"
---

# The Boundaries of AI-Generated Content: Why App Stores Are Scrutinizing AI Image Generators

The controversy over harmful content produced by AI image generators has become a measure testing platform responsibility and regulatory effectiveness, extending beyond mere technical flaws. A senator's letter reveals that app stores' claim of providing a 'safe environment' is a core argument for maintaining market dominance. Now, even strategies that differentiate features between paid and free users will face strict scrutiny over whether they are part of risk management or a means to evade regulation.

## Current Status: Investigated Facts and Data

The approaches of major AI image generators are clearly divided. OpenAI's DALL-E implements the strictest and most proactive control by applying prompt rewriting through ChatGPT and multi-stage filtering at input and output. Midjourney has evolved from initial simple keyword blocking to an AI moderation system that understands context, focusing on compliance with community guidelines. In contrast, due to its open-source nature, Stable Diffusion includes a safety mechanism called Safety Checker, but users can easily disable or modify it, showing the highest autonomy and lowest level of control among the three models.

The legal framework is also rapidly evolving. The European Union, through the Digital Services Act (DSA), has imposed obligations on large platforms to assess and mitigate systemic risks, such as AI-generated deepfakes. The AI Act explicitly mandates machine-readable labeling and transparency for generative AI content. In the United States, Executive Order 14110 directs the establishment of standards for watermarking and identifying AI-generated content, while the Senate is discussing limiting the immunity under Section 230 of the Communications Decency Act to clarify platforms' legal liability for AI-generated content.

## Analysis: Implications and Impact

These technical and legal differences raise a fundamental question: Where are the boundaries of a platform's 'responsibility'? App store operators require apps to comply with content policies based on their own terms of service. However, the level of control service providers exert varies widely, from DALL-E's closed control to Stable Diffusion's open structure. This means app store policies require a test of practical feasibility, not just a simple pledge. The strategy of relaxing harmful content filtering only in paid versions can be interpreted as an attempt by platforms to manage risk in tiers, but it may also draw criticism for creating regulatory blind spots.

The letter sent by senators to Apple and Google's app stores hits the core of this debate. They point out that platforms use the claim of providing a 'safe environment' as a shield to maintain market dominance while shifting responsibility for the actual harmfulness of AI services distributed within them. This suggests that beyond mere technical regulation, political scrutiny of the platforms' own governance models and market power is intensifying.

## Practical Application: Methods Readers Can Utilize

Risk managers or policy designers within companies should recognize and evaluate the varying control levels of different generation tools when establishing internal guidelines, rather than applying a single 'AI-generated content' policy. For example, if employees use open models like Stable Diffusion, stronger user education and post-monitoring protocols may be needed compared to using DALL-E. Additionally, it is essential to monitor legislative trends in the EU and the US and prepare for technical compliance requirements such as machine-readable labeling or watermarking.

## FAQ: 3 Questions

**Q: If harm occurs due to harmful content created by an AI image generator, who can be held legally liable?**
A: The legal landscape is changing rapidly. In the US, legislation is being discussed to limit the application of Section 230 of the Communications Decency Act, which grants immunity to platforms. The EU's DSA imposes risk assessment and mitigation obligations on large platforms. The ultimate locus of liability will vary depending on the specific case and applicable laws, among the user, the AI service provider, and the distribution platform.

**Q: For open-source AI models, is there no way to prevent the generation of harmful content?**
A: That's not true. Even Stable Diffusion includes safety mechanisms like the Safety Checker by default. However, due to the nature of open source, users can modify or remove them. Therefore, introducing additional safety measures at the distribution channel (e.g., app stores) or the end-user environment (e.g., corporate internal systems) becomes a more critical consideration than control at the service provider stage.

**Q: What exactly does 'AI moderation that understands context' mean?**
A: It refers to a system that evaluates the overall context of the prompt and the generated image, going beyond simply blocking a keyword like 'violence.' For example, it signifies a more sophisticated filtering method capable of distinguishing between a prompt for a 'historical war painting' and a prompt generating 'a hateful image of a modern politician.' Midjourney is known to be evolving in this direction.

## Conclusion: Summary + Actionable Suggestions

The battlefield of AI-generated content regulation is expanding from technical filtering to discussions of legal liability and, ultimately, scrutiny of platform power. App stores' requirement for terms of service compliance is no longer an empty declaration but requires provable evidence of the service provider's actual control mechanisms. All stakeholdersâ€”companies, policymakers, developersâ€”must reassess their position in this complex chain of responsibility and respond by focusing on verifiable implementation and transparency, not just simple policy pledges.
---

## ì°¸ê³  ìë£Œ

- ğŸ›¡ï¸ [CompVis/stable-diffusion-safety-checker - Hugging Face](https://huggingface.co/CompVis/stable-diffusion-safety-checker)
- ğŸ›¡ï¸ [Executive Order 14110 on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence](https://www.congress.gov/118/meeting/hzm/116492/HHRG-118-IF16-20231030-SD002.pdf)
- ğŸ›¡ï¸ [ìœ ëŸ½ì—°í•© ì¸ê³µì§€ëŠ¥ë²•(AI Act) - ê¸€ë¡œë²Œ ì…ë²•ë°ì´í„° í”Œë«í¼](https://world.klri.re.kr/glp/law/L8202400030?lang=ko)
